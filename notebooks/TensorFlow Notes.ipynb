{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard Usage -- Updated\n",
    "\n",
    "Looks like I need to relearn Tensorboard since my old code is all deprecated :/\n",
    "Tutorial link: [link](https://www.tensorflow.org/get_started/summaries_and_tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the TensorFlow graph that you'd like to collect summary data from, and decide which nodes you would like to annotate with summary operations.\n",
    "```python\n",
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "```\n",
    "\n",
    "2. To generate summaries, we need to run all of these summary nodes. Combine them into a single op that generates all the summary data.\n",
    "```python\n",
    "merged = tf.summary.merge_all()\n",
    "```\n",
    "\n",
    "3. Run the merged summary op, which will generate a serialized Summary protobuf object with all of your summary data at a given step. Finally, to write this summary data to disk, pass the summary protobuf to a tf.summary.FileWriter.\n",
    "```python\n",
    "train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',\n",
    "                                      sess.graph)\n",
    "test_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/test')\n",
    "tf.global_variables_initializer().run()\n",
    "```\n",
    "\n",
    "4. During training . . . \n",
    "```python\n",
    "summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "    test_writer.add_summary(summary, i)\n",
    "```\n",
    "\n",
    "5. Launching tensorboard after training. \n",
    "```\n",
    "tensorboard --logdir=path/to/log-directory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of below code (docstring):\n",
    "\n",
    "*A simple MNIST classifier which displays summaries in TensorBoard. This is an unimpressive MNIST model, but it is a good example of using tf.name\\_scope to make a graph legible in the TensorBoard graph explorer, and of naming summary tags so that they are grouped meaningfully in TensorBoard. It demonstrates the functionality of every TensorBoard dashboard.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    # Create a multilayer model.\n",
    "\n",
    "    # Input placeholders\n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
    "\n",
    "    with tf.name_scope('input_reshape'):\n",
    "        image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('input', image_shaped_input, 10)\n",
    "\n",
    "    # We can't initialize these variables to 0 - the network will get stuck.\n",
    "    def weight_variable(shape):\n",
    "        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def variable_summaries(var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "\n",
    "    def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "        \"\"\"Reusable code for making a simple neural net layer.\n",
    "\n",
    "        It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "        It also sets up name scoping so that the resultant graph is easy to read,\n",
    "        and adds a number of summary ops.\n",
    "        \"\"\"\n",
    "        # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "        with tf.name_scope(layer_name):\n",
    "            # This Variable will hold the state of the weights for the layer\n",
    "            with tf.name_scope('weights'):\n",
    "                weights = weight_variable([input_dim, output_dim])\n",
    "                variable_summaries(weights)\n",
    "            with tf.name_scope('biases'):\n",
    "                biases = bias_variable([output_dim])\n",
    "                variable_summaries(biases)\n",
    "            with tf.name_scope('Wx_plus_b'):\n",
    "                preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "                tf.summary.histogram('pre_activations', preactivate)\n",
    "            activations = act(preactivate, name='activation')\n",
    "            tf.summary.histogram('activations', activations)\n",
    "            return activations\n",
    "\n",
    "    hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "\n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "        dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "\n",
    "    # Do not apply softmax activation yet, see below.\n",
    "    y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)\n",
    "\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        # The raw formulation of cross-entropy,\n",
    "        #\n",
    "        # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n",
    "        #                               reduction_indices=[1]))\n",
    "        #\n",
    "        # can be numerically unstable.\n",
    "        #\n",
    "        # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n",
    "        # raw outputs of the nn_layer above, and then average across\n",
    "        # the batch.\n",
    "        diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "        with tf.name_scope('total'):\n",
    "            cross_entropy = tf.reduce_mean(diff)\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Merge all the summaries and write them out to /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test')\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # Train the model, and also write summaries.\n",
    "    # Every 10th step, measure test-set accuracy, and write test summaries\n",
    "    # All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "    def feed_dict(train):\n",
    "        \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "        if train or FLAGS.fake_data:\n",
    "            xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n",
    "            k = FLAGS.dropout\n",
    "        else:\n",
    "            xs, ys = mnist.test.images, mnist.test.labels\n",
    "            k = 1.0\n",
    "        return {x: xs, y_: ys, keep_prob: k}\n",
    "\n",
    "    for i in range(FLAGS.max_steps):\n",
    "        if i % 10 == 0:  # Record summaries and test-set accuracy\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('Accuracy at step %s: %s' % (i, acc))\n",
    "        else:  # Record train set summaries, and train\n",
    "            if i % 100 == 99:  # Record execution stats\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run([merged, train_step],\n",
    "                                      feed_dict=feed_dict(True),\n",
    "                                      options=run_options,\n",
    "                                      run_metadata=run_metadata)\n",
    "                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "                train_writer.add_summary(summary, i)\n",
    "                print('Adding run metadata for', i)\n",
    "            else:  # Record a summary\n",
    "                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "                train_writer.add_summary(summary, i)\n",
    "    train_writer.close()\n",
    "    test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    if tf.gfile.Exists(FLAGS.log_dir):\n",
    "        tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "    train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--fake_data', nargs='?', const=True, type=bool,\n",
    "              default=False,\n",
    "              help='If true, uses fake data for unit testing.')\n",
    "    parser.add_argument('--max_steps', type=int, default=1000,\n",
    "              help='Number of steps to run trainer.')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "              help='Initial learning rate')\n",
    "    parser.add_argument('--dropout', type=float, default=0.9,\n",
    "              help='Keep probability for training dropout.')\n",
    "    parser.add_argument('--data_dir', type=str, default='/tmp/tensorflow/mnist/input_data',\n",
    "                      help='Directory for storing input data')\n",
    "    parser.add_argument('--log_dir', type=str, default='/tmp/tensorflow/mnist/logs/mnist_with_summaries',help='Summaries log directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to save the Graph Again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tutorial Link](https://www.tensorflow.org/programmers_guide/meta_graph)\n",
    "\n",
    "In order for a Python object to be serialized to and from MetaGraphDef, the Python class must implement to_proto() and from_proto() methods, and register them with the system using register_proto_function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Variable:\n",
    "\n",
    "    def to_proto(self, export_scope=None):\n",
    "        \"\"\"Converts a `Variable` to a `VariableDef` protocol buffer.\n",
    "\n",
    "        Args:\n",
    "        export_scope: Optional `string`. Name scope to remove.\n",
    "\n",
    "        Returns:\n",
    "        A `VariableDef` protocol buffer, or `None` if the `Variable` is not\n",
    "        in the specified name scope.\n",
    "        \"\"\"\n",
    "        if (export_scope is None or self._variable.name.startswith(export_scope)):\n",
    "            var_def                  = variable_pb2.VariableDef()\n",
    "            var_def.variable_name    = ops.strip_name_scope(self._variable.name, export_scope)\n",
    "            var_def.initializer_name = ops.strip_name_scope(self.initializer.name, export_scope)\n",
    "            var_def.snapshot_name    = ops.strip_name_scope(self._snapshot.name, export_scope)\n",
    "            if self._save_slice_info:\n",
    "                var_def.save_slice_info_def.MergeFrom(self._save_slice_info.to_proto(export_scope=export_scope))\n",
    "            return var_def\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def from_proto(variable_def, import_scope=None):\n",
    "        \"\"\"Returns a `Variable` object created from `variable_def`.\"\"\"\n",
    "        return Variable(variable_def=variable_def, import_scope=import_scope)\n",
    "\n",
    "\n",
    "ops.register_proto_function(ops.GraphKeys.GLOBAL_VARIABLES,\n",
    "                            proto_type=variable_pb2.VariableDef,\n",
    "                            to_proto=Variable.to_proto,\n",
    "                            from_proto=Variable.from_proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a MetaGraph\n",
    "\n",
    "The function signature, for reference:\n",
    "```python\n",
    "def export_meta_graph(filename=None, collection_list=None, as_text=False):\n",
    "  \"\"\"Writes `MetaGraphDef` to save_path/filename.\n",
    "\n",
    "  Args:\n",
    "    filename: Optional meta_graph filename including the path.\n",
    "    collection_list: List of string keys to collect.\n",
    "    as_text: If `True`, writes the meta_graph as an ASCII proto.\n",
    "\n",
    "  Returns:\n",
    "    A `MetaGraphDef` proto.\n",
    "  \"\"\"\n",
    "```\n",
    "\n",
    "You should store all variables you wanna get letter in ```collection```. Or, if you don't specify one, it will save everything (yeah do that). \n",
    "\n",
    "How to export the default running graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.matmul(hidden2, weights) + biases\n",
    "tf.add_to_collection(\"logits\", logits)\n",
    "\n",
    "init_all_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initializes all the variables.\n",
    "    sess.run(init_all_op)\n",
    "    # Runs to logit.\n",
    "    sess.run(logits)\n",
    "    chatbot.saver.save(sess, 'my-save-dir/my-model-10000')\n",
    "    # Generates MetaGraphDef.\n",
    "    chatbot.saver.export_meta_graph('my-save-dir/my-model-10000.meta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the MetaGraph and Resuming Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case: Import and Extend Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')\n",
    "    new_saver.restore(sess, 'my-save-dir/my-model-10000')\n",
    "    \n",
    "    # Adds loss and train.\n",
    "    labels = tf.constant(0, tf.int32, shape=[100], name=\"labels\")\n",
    "    batch_size = tf.size(labels)\n",
    "    labels = tf.expand_dims(labels, 1)\n",
    "    indices = tf.expand_dims(tf.range(0, batch_size), 1)\n",
    "    concated = tf.concat([indices, labels], 1)\n",
    "    onehot_labels = tf.sparse_to_dense(concated, tf.stack([batch_size, 10]), 1.0, 0.0)\n",
    "    \n",
    "    # Getting the logits variable back that we saved.\n",
    "    logits = tf.get_collection(\"logits\")[0]\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=onehot_labels, logits=logits, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(cross_entropy, name=\"xentropy_mean\")\n",
    "\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    # Creates the gradient descent optimizer with the given learning rate.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "    # Runs train_op.\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    sess.run(train_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case: Retrieve Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \".\".join([tf.latest_checkpoint(train_dir), \"meta\"])\n",
    "tf.train.import_meta_graph(filename)\n",
    "hparams = tf.get_collection(\"hparams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FileWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Management\n",
    "\n",
    "[Very useful link](https://www.tensorflow.org/versions/r0.11/api_docs/python/client/session_management). Main think to know is that the following are *equivalent*:\n",
    "\n",
    "* Equivalent usage 1:\n",
    "```python\n",
    "# Using the `close()` method.\n",
    "sess = tf.Session()\n",
    "sess.run(...)\n",
    "sess.close()\n",
    "```\n",
    "\n",
    "* Equivalent usage 2:\n",
    "```python\n",
    "# Using the context manager.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing Variables\n",
    "\n",
    "My version of TensorFlow's tutorial by the same name, but in English. The (sub)section names are identical; I compress the content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "**Reusing the same model on multiple inputs**: Let's say we make a function ```my_model(inputs)``` that creates a bunch of tf.Variable() objects and returns the final output tensor variable. Each time we call my_model(next_inputs), it is going to create a whole new model with new variables. This bad. \n",
    "\n",
    "**How to fix?**: Rather than requiring classes to create a model and manage the variables, TensorFlow provides a *Variable scope* mechanism for sharing named variables while constructing a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Scope\n",
    "\n",
    "The mechanism consists of 2 main functions:\n",
    "1. ```tf.get_variable(name, shape, initializer)``` creates or returns a variable with a given name.\n",
    "2. ```tf.variable_scope(scope_name)``` manages namespaces **for names passed to tf.get_variable()**. \n",
    "\n",
    "\n",
    "__Difference between name scope and variable scope__: [Here is a link](http://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow) to the best explanation I could find. The following is the main part:\n",
    "\n",
    "\"However, name scope is ignored by tf.get_variable. We can see that in the following example:\n",
    "\n",
    "```python\n",
    "with tf.name_scope(\"my_scope\"):\n",
    "    v1 = tf.get_variable(\"var1\", [1], dtype=tf.float32)\n",
    "    v2 = tf.Variable(1, name=\"var2\", dtype=tf.float32)\n",
    "    a = tf.add(v1, v2)\n",
    "\n",
    "print(v1.name)  # var1:0\n",
    "print(v2.name)  # my_scope/var2:0\n",
    "print(a.name)   # my_scope/Add:0\n",
    "```\n",
    "\n",
    "The only way to place a variable accessed using tf.get_variable in a scope is to use variable scope, as in the following example:\n",
    "\n",
    "```python\n",
    "with tf.variable_scope(\"my_scope\"):\n",
    "    v1 = tf.get_variable(\"var1\", [1], dtype=tf.float32)\n",
    "    v2 = tf.Variable(1, name=\"var2\", dtype=tf.float32)\n",
    "    a = tf.add(v1, v2)\n",
    "\n",
    "print(v1.name)  # my_scope/var1:0\n",
    "print(v2.name)  # my_scope/var2:0\n",
    "print(a.name)   # my_scope/Add:0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNCell\n",
    "\n",
    "Any subclass of RNNCell must implement a __call__ method, with signature:\n",
    "```python\n",
    "def __call__(self, inputs, state, scope=None):\n",
    "```\n",
    "\n",
    "For example here is the source code for a few (core_rnn_cell_impl.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BasicRNNCell(RNNCell):\n",
    "    def __init__(self, num_units, activation=tanh):\n",
    "        self._num_units = num_units\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"output = new_state = act(W * input + U * state + B).\"\"\"\n",
    "        with vs.variable_scope(scope or \"basic_rnn_cell\"):\n",
    "            # _linear(x, y) == sum_i (x[i] * y[i))\n",
    "            # inputs and state are both 2D tensors (i.e. matrices). of shape batch by n. \n",
    "            output = self._activation(_linear([inputs, state], self._num_units, True, scope=scope))\n",
    "        return output, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Class -- Currently undocumented . . . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm going to give myself an overview of this strange new set of files in tf.contrib.seq2seq (master branch) that is not on the website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: logits terminology\n",
    "\n",
    "1.  Logistic function: \n",
    "$$\\begin{align}\n",
    "p(X) &= \\frac{1}{1 + e^{-\\beta^TX}}  \\\\\n",
    "&= \\frac{e^{ \\beta^TX}}{1 + e^{ \\beta^TX}}\n",
    "\\end{align}$$\n",
    "\n",
    "2. Odds: \n",
    "$$\n",
    "\\frac{p(X)}{1 - p(X)} = e^{\\beta^T X}\n",
    "$$\n",
    "\n",
    "3. log-odds, \"logit\":\n",
    "$$\n",
    "\\log\\left(\\frac{p(X)}{1 - p(X)} \\right) = \\beta^T X\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __tf.Variable vs tf.placeholder__: \"The difference is that with tf.Variable you have to provide an initial value when you declare it. With tf.placeholder you don't have to provide an initial value and you can specify it at run time with the feed_dict argument inside Session.run\" [Source](http://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Embedding Lookup\n",
    "\n",
    "Goal: Figure out how embedding lookup works. Ideally i'd like to show, as a toy example, an integer being mapped to its binary string. For example, with embed size of 5\n",
    "```python\n",
    "embed([2, 19]) --> [[0, 0, 0, 1, 0], [1, 0, 0, 1, 1]] \n",
    "```\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "input_size = (1, 2) # arbitrary\n",
    "embed_size = 5\n",
    "vocab_size = 2**embed_size\n",
    "input_array = np.random.randint(vocab_size, size=input_size)\n",
    "expected_embedding = ['{0:b}'.format(inp) for inp in input_array.flatten()]\n",
    "print(\"Input array:\\n\", input_array)\n",
    "print(\"Expected embedding:\\n\", expected_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.30149388 -0.099668    0.14693063 -0.39335564  0.34846574]\n",
      "  [-0.0505532  -0.21680753 -0.3851763  -0.1110653   0.0726169 ]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = None\n",
    "with tf.Session() as sess:\n",
    "    params   = tf.get_variable('embed_tensor', \n",
    "                               shape=[vocab_size, embed_size], \n",
    "                               initializer=tf.contrib.layers.xavier_initializer())\n",
    "    inputs    = tf.convert_to_tensor(input_array)\n",
    "    embedding = tf.nn.embedding_lookup(params, inputs)\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    emb = sess.run(embedding)\n",
    "    print(emb)\n",
    "\n",
    "sess = None\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Representations of Words\n",
    "\n",
    "[Link](https://www.tensorflow.org/tutorials/word2vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
