{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs, LSTMs, and Reddit Comments\n",
    "\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "Here we work through implementations of a \"vanilla\" (as basic as it gets) __Recurrent Neural Network__ and a __Long Short Term Memory network__ for generating reddit comments. As a bonus, at the end we will also implement a more sophisticated network using TFlearn to see the real power of such networks. \n",
    "\n",
    "### Sneak Peek: Example Results for Each Implementation\n",
    "\n",
    "__VanillaRNN__:\n",
    "\n",
    "* Many should still especially as me in n't nothing as the fight combat are favorite of your lot of with great towards using the time away for the # in most ( now \n",
    "* The blood is people idea they be make a 3 than law to be like were not who n't have them , i 've like like in my population you have has as they have an tree '' is a wants to eating 3a in the decent healthy does n't internet about excited date as it.\n",
    "* It should know arguing really pounds and much.\n",
    "* If you can both used as the fact [expletive] is better or kind and quickly with the [expletive] maybe i did already know.\n",
    "\n",
    "__LSTM__:\n",
    "\n",
    "* People cash everything and christian the to dust or the glorious.\n",
    "* Education time is is empty ps4.\n",
    "* Downvote problems asking marriage defender or to into about 'm you more your go way do abilities.\n",
    "* Do riot vocal lol.\n",
    "* Generally saying was interesting on wait.\n",
    "\n",
    "__TFLearn Implementation__:\n",
    "\n",
    "* Provide some flexibity and a good. \n",
    "* It sounds the sure and stread and of the fart to a some of the part. \n",
    "* In games journalism that do the press and the lan meaning.\n",
    "* http://nflstrear.com/tikestementom/compotec/2010500 \n",
    "    \n",
    "\n",
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 23335 sentences.\n",
      "Found 31209 unique words tokens.\n",
      "Using vocabulary size 4000.\n",
      "Least frequent word in vocab: 'knee', which appeared 8 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'UNKNOWN_TOKEN', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv as csv\n",
    "import itertools\n",
    "import nltk\n",
    "\n",
    "vocabulary_size = 4000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    " \n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print( \"Reading CSV file...\")\n",
    "with open('python/med_reddit.csv', 'r') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.__next__()\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print( \"Parsed %d sentences.\" % (len(sentences)))\n",
    "     \n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    " \n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    " \n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    " \n",
    "print( \"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print( \"Least frequent word in vocab: '%s', which appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    " \n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    " \n",
    "print(\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print( \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])\n",
    " \n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "print(type(tokenized_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN\n",
    "\n",
    "Working through [this series of tutorials](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "\n",
    "<img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg\" width=\"400\"/>\n",
    "\n",
    "* Inputs: $x_t$, the input at time step $t$. \n",
    "* Hidden: $s_t$ is the \"memory\" of the network. $$s_t = f(U x_t + W s_{t - 1})$$\n",
    "\n",
    "* Output: $o_t$ is calculated solely based on memory at time t, given by $s_t$, e.g.\n",
    "    $$ o_t = \\mathrm{softmax}(V s_t) $$\n",
    "\n",
    "\n",
    "### Backpropagation Through Time (BPTT)\n",
    "\n",
    "Code is based on the ideas from [this tutorial](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n",
    "\n",
    "![wut](http://s0.wp.com/latex.php?zoom=1.100000023841858&latex=%5Cbegin%7Baligned%7D++s_t+%26%3D+%5Ctanh%28Ux_t+%2B+Ws_%7Bt-1%7D%29+%5C%5C++%5Chat%7By%7D_t+%26%3D+%5Cmathrm%7Bsoftmax%7D%28Vs_t%29++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0)\n",
    "![](http://s0.wp.com/latex.php?zoom=1.100000023841858&latex=%5Cbegin%7Baligned%7D++E_t%28y_t%2C+%5Chat%7By%7D_t%29+%26%3D+-+y_%7Bt%7D+%5Clog+%5Chat%7By%7D_%7Bt%7D+%5C%5C++E%28y%2C+%5Chat%7By%7D%29+%26%3D%5Csum%5Climits_%7Bt%7D+E_t%28y_t%2C%5Chat%7By%7D_t%29+%5C%5C++%26+%3D+-%5Csum%5Climits_%7Bt%7D+y_%7Bt%7D+%5Clog+%5Chat%7By%7D_%7Bt%7D++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0)\n",
    "\n",
    "<img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/rnn-bptt1.png\" width=400/>\n",
    "\n",
    "#### Here I go taking derivatives again\n",
    "\n",
    "I'm denoting inputs, hidden, and outputs at time t, respectively, as $x_t^{(0)}$, $x_t^{(1)}$,$x_t^{(2)}$, all of which are vectors. TODO: Write more explanations here/finish. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{L}(y_t, \\hat y_t) &= -\\sum_{i = 1}^{n_{vocab}} (y_t)_i \\log((\\hat y_t)_i) \\\\\n",
    "\\frac{\\partial L_t}{\\partial V_{oh}} &= - \\left((x_t^{(2)})_o -  (y_t)_o  \\right) (y_t)_h\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "import operator\n",
    "\n",
    "class VanillaRNN(object):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        U: Connections between Inputs -> Hidden. Shape = (hidden_size, vocab_size)\n",
    "        V: Connections between Hidden -> Output. Shape = (vocab_size, hidden_size)\n",
    "        W: Connections between Hidden -> Hidden. Shape = (hidden_size, hidden_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=4, hidden_size=3, bptt_truncate=4, dicts=[], init_weights=True):\n",
    "        self.n_vocab = vocab_size\n",
    "        self.n_hid = hidden_size\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.char_to_ix, self.ix_to_char = dicts\n",
    "            \n",
    "        # _____________ Model Parameters. ______________\n",
    "        # Index convention: Array[i, j] is from neuron j to neuron i. \n",
    "        # Init values based on number of incoming connections from the *previous* layer.\n",
    "        if init_weights:\n",
    "            init = {'in': np.sqrt(1./vocab_size), 'hid': np.sqrt(1./hidden_size)}\n",
    "            self.U = np.random.uniform(- init['in'], init['in'], size=(hidden_size, vocab_size))\n",
    "            self.V = np.random.uniform(- init['hid'], init['hid'], size=(vocab_size, hidden_size))\n",
    "            self.W = np.random.uniform(- init['hid'], init['hid'], size=(hidden_size, hidden_size))\n",
    "        \n",
    "    def _step(self, x, o, s, t):\n",
    "        # Indexing U by x[t] is the same as multiplying U with a one-hot vector.\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W @ s[t-1])\n",
    "        o[t] = self.V @ s[t]\n",
    "        o[t] = np.exp(o[t]) / np.exp(o[t]).sum()\n",
    "        return o[t], s[t]\n",
    "\n",
    "    def forward_pass(self, x, step=VanillaRNN._step, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:  a list of word indices. We keep it this way to avoid converting to a \n",
    "                ridiculously large one-hot encoded matrix. \n",
    "            step: function(x, s, t)\n",
    "        Returns:\n",
    "            o: output probabilities over all inputs in x. shape: (len(x), n_vocab)\n",
    "            s: hidden states at each time step. shape: (len(x) + 1, n_hid)\n",
    "        \"\"\"\n",
    "        n_steps = len(x)\n",
    "        # Save hidden states in s because need them later. (extra element for initial hidden state)\n",
    "        s = np.zeros((n_steps + 1, self.n_hid))\n",
    "        s[-1] = np.zeros(self.n_hid)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((n_steps, self.n_vocab))\n",
    "        # Feed in each word of x sequentially. \n",
    "        for t in np.arange(n_steps):\n",
    "            o[t], s[t] = self._step(x, o, s, t)\n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: training sample sentence.\n",
    "        Returns:\n",
    "            max_out_ind: [indices of] most likely words, given the input sentence. \n",
    "        \"\"\"\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_pass(x)\n",
    "        max_out_ind = np.argmax(o, axis=1)\n",
    "        pred_words = [self.ix_to_char[i] for i in max_out_ind]\n",
    "        print('Preds at each time step:\\n',  ' '.join(pred_words))\n",
    "        return max_out_ind\n",
    "    \n",
    "    def loss(self, x, y, norm=True):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = np.sum((len(y_i) for y_i in y)) if norm else 1\n",
    "        L = 0\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_pass(x[i])\n",
    "            # Extract our predicted probabilities for the actual labels y. \n",
    "            predicted_label_prob = o[np.arange(len(y[i])), y[i]]\n",
    "            # Increment loss. Multiply by 1. to remind of interp y_n = 1 for truth else 0. \n",
    "            L += - 1. * np.sum(np.log(predicted_label_prob))\n",
    "        return L / N\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        \"\"\"\n",
    "        Backpropagation Through Time.\n",
    "        \"\"\"\n",
    "        n_words = len(y) # in the single sentence of y. \n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_pass(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # Countdown backwards from T. \n",
    "        for t in np.arange(n_words)[::-1]: \n",
    "            # Difference in outputs * hidden at timestep t. \n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # First part of delta_t computation before bptt. \n",
    "            delta_t = (self.V.T @ delta_o[t]) * (1. - s[t]**2) \n",
    "            # Step backwards in time for either btt_truncate steps or hit 0, whichever comes first.\n",
    "            for bptt_step in np.arange(max(0, t - self.bptt_truncate), t + 1)[::-1]:\n",
    "                dLdW                  += np.outer(delta_t, s[bptt_step-1])  \n",
    "                dLdU[:, x[bptt_step]] += delta_t\n",
    "                delta_t                = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "                \n",
    "        return [dLdU, dLdV, dLdW]\n",
    "\n",
    "            \n",
    "    # Performs one step of SGD.\n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training/Generating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print( \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5 \n",
    "                print( \"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "            \n",
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs, _ = model.forward_pass(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "def generate(model, n_sentences=1, min_length=5):\n",
    "    for i in range(n_sentences):\n",
    "        i_try = 0\n",
    "        sent = []\n",
    "        while len(sent) < min_length or i_try > 5:\n",
    "            i_try += 1\n",
    "            sent = generate_sentence(model)\n",
    "        print(\" \".join(sent))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: VanillaRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds at each time step:\n",
      " upset replied key doubts committed reactions factory bernie choice instances ^^^or isis status sales keyboard arguments openly long speaker increasing factor //www.reddit.com/r/askreddit/wiki/index heavy creep dollars acknowledge 're sports reactions mad acknowledge factors refuse hmm office crown suggestions acknowledge pride surface share risk min past mainly\n",
      "Expected Loss for random predictions: 8.294050\n",
      "Actual loss: 8.294656\n"
     ]
    }
   ],
   "source": [
    "rnn = VanillaRNN(vocab_size=vocabulary_size, \n",
    "                hidden_size=10, \n",
    "                bptt_truncate=4, \n",
    "                dicts=[word_to_index, index_to_word])\n",
    "\n",
    "preds = rnn.predict(X_train[10])\n",
    "\n",
    "# Limit to 1000 examples to save time\n",
    "print( \"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print( \"Actual loss: %f\" % rnn.loss(X_train[:100], y_train[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = VanillaRNN(vocab_size=vocabulary_size,\n",
    "                   hidden_size=10,\n",
    "                   bptt_truncate=512,\n",
    "                   dicts=[word_to_index, index_to_word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the timesteps during training below. In just a matter of minutes, the network is able to output somewhat reasonable looking results. If we were to run this on [not my laptop], we'd expect rather convincing comments to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = np.random.randint(0, X_train.shape[0], size=1000)\n",
    "losses = train_with_sgd(model, X_train[ind], y_train[ind], nepoch=10, evaluate_loss_after=1)\n",
    "\n",
    "generate(model, n_sentences=10, min_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "# Long Short Term Memory Networks (LSTMS)\n",
    "\n",
    "Now reading through [this overview](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "For comparison, here is how this tutorial illustrate the vanilla RNN:\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" width=\"350\"/>\n",
    "\n",
    "__Purpose__: Better deal with the problem of long-term dependencies, e.g. \"I grew up in France... I speak fluent _French_\" could be difficult for a vanilla RNN, depending on how far the gap between \"France\" and trying to predict the word \"French\". LSTMs solve this problem.\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" width=\"550\"/>\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png\"  width=\"400\"/>\n",
    "\n",
    "__Core Idea__: The cell state, which is the horizontal line running through the top of the diagram.\n",
    "* _Gates_, the areas composed of a $\\sigma$ layer and pointwise multiplication (x) operation, are a way to optionally let information through [to the cell state]. The output of the sigmoid, between 0 and 1, tells how much info to let through to the cell.\n",
    "\n",
    "__Three Main Gates__:\n",
    "1. The \"forget gate\" layer. Determines which information to throw away from the cell state. \n",
    "2. The \"input gate\" layer, comprised of the second $\\sigma$ and tanh, Determines what new info to store in the cell state. \n",
    "3. The output gate. First, we run a sigmoid layer which decides what parts of the cell state we're going to output. Then, we put the cell state through tanh (to push the values to be between -1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to. \n",
    "\n",
    "\n",
    "__Conceptual Explanation of Gates__:\n",
    "\n",
    "1. __Cell State__: Tells you which hidden states were important from past step and which are important from current step. \n",
    "2. __Middle Gates__: So the sigmoid allows us to tune which hidden values are most important for this time step, and the tanh our good ol' pal from the original RNN. \n",
    "3. __Rightmost Gates__: sigmoid for deciding the values to use from the cell state, and we send the cell state back through tanh to make its values in [-1, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(VanillaRNN):\n",
    "\n",
    "    def __init__(self, vocab_size=4, hidden_size=3, bptt_truncate=4, dicts=[]):\n",
    "        super().__init__(vocab_size, hidden_size, bptt_truncate, dicts, init_weights=False)\n",
    "        # _____________ Model Parameters. ______________\n",
    "        # Index convention: Array[i, j] is from neuron j to neuron i. \n",
    "        # Init values based on number of incoming connections from the *previous* layer.\n",
    "        init = {'in': np.sqrt(1./vocab_size), 'hid': np.sqrt(1./hidden_size)}\n",
    "\n",
    "        self.U, self.W = {}, {}\n",
    "        self.b = {}#; b['f'] = b['i'] = b['c'] = 0\n",
    "        for i_gate in ['i', 'f', 'o', 'c']:\n",
    "            self.U[i_gate] = np.random.uniform(- init['in'], init['in'], size=(hidden_size, vocab_size))\n",
    "            self.W[i_gate] = np.random.uniform(- init['hid'], init['hid'], size=(hidden_size, hidden_size))\n",
    "            self.b[i_gate] = np.zeros(self.n_hid)\n",
    "        self.V = np.random.uniform(- init['hid'], init['hid'], size=(vocab_size, hidden_size))\n",
    "\n",
    "    def forward_pass(self, x, verbose=False):\n",
    "        \"\"\"\n",
    "        Sequentially feed each element of self.inputs through network.\n",
    "        \"\"\"\n",
    "        # The 'Cell state' at all time steps. \n",
    "        self.C = np.zeros((len(x) + 1, self.n_hid))\n",
    "        self.C[-1] = np.zeros(self.n_hid)\n",
    "        return super().forward_pass(x, self._step, verbose)\n",
    "            \n",
    "    def _step(self, x, o, s, t):\n",
    "        from scipy.special import expit as sigmoid\n",
    "        gated_sums = self._gated_sums(x, s, t)\n",
    "        \n",
    "        # Compute individual gate functions.\n",
    "        forget_gate = sigmoid(gated_sums['f'])\n",
    "        input_gate  = sigmoid(gated_sums['i'])\n",
    "        cand_gate   = np.tanh(gated_sums['c'])\n",
    "        output_gate = sigmoid(gated_sums['o'])\n",
    "        \n",
    "        # Compute new cell outputs (cell state, hidden state, prediction probs). \n",
    "        self.C[t]     = forget_gate * self.C[t - 1] + input_gate * cand_gate\n",
    "        hidden        = output_gate * np.tanh(self.C[t])\n",
    "        softmax_probs = self.V @ hidden\n",
    "        softmax_probs = np.exp(softmax_probs) / np.exp(softmax_probs).sum()\n",
    "\n",
    "        return [softmax_probs, hidden]\n",
    "    \n",
    "    def _gated_sums(self, x, s, t):\n",
    "        return {g: self.U[g][:, x[t]] + self.W[g] @ s[t - 1] + self.b[g] for g in ['f', 'i', 'c', 'o']}\n",
    "    \n",
    "   \n",
    "    def bptt(self, x, y):\n",
    "        \"\"\"\n",
    "        Backpropagation Through Time.\n",
    "        \"\"\"\n",
    "        n_words = len(y) # in the single sentence of y. \n",
    "        \n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_pass(x)\n",
    "        \n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        \n",
    "        dLdU, dLdW = {}, {}\n",
    "        dLdb = {}\n",
    "        for i_gate in ['i', 'f', 'o', 'c']:\n",
    "            dLdU[i_gate] = np.zeros(self.U[i_gate].shape)\n",
    "            dLdW[i_gate] = np.zeros(self.W[i_gate].shape)\n",
    "            dLdb[i_gate] = np.zeros(self.b[i_gate].shape)\n",
    "            \n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        \n",
    "        # For each output backwards...\n",
    "        for t in np.arange(n_words)[::-1]:\n",
    "            \n",
    "            # Difference in outputs * hidden at timestep t. \n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # First part of delta_t calculation before bptt.\n",
    "            delta_t = self.V.T @ delta_o[t] * (1. - (s[t] ** 2))\n",
    "            \n",
    "            # Step backwards in time for either btt_truncate steps or hit 0, whichever comes first.\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                \n",
    "                for i_gate in ['i', 'f', 'o', 'c']:\n",
    "                    # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                    dLdW[i_gate] +=  np.outer(delta_t, s[bptt_step-1]) \n",
    "                    dLdU[i_gate][:, x[bptt_step]] += delta_t\n",
    "                    dLdb[i_gate] += delta_t.sum(axis=0)\n",
    "                \n",
    "                    # Update delta for next step\n",
    "                    delta_t = self.W[i_gate].T @ delta_t * (1 - s[bptt_step-1]**2)\n",
    "                \n",
    "        return dLdU, dLdV, dLdW, dLdb\n",
    "\n",
    "            \n",
    "        # Performs one step of SGD.\n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW, dLdb = self.bptt(x, y)\n",
    "        self.V -= learning_rate * dLdV\n",
    "        # Change parameters according to gradients and learning rate\n",
    "        for i_gate in ['i', 'f', 'o', 'c']:\n",
    "            self.U[i_gate] -= learning_rate * dLdU[i_gate]\n",
    "            self.W[i_gate] -= learning_rate * dLdW[i_gate]\n",
    "            self.b[i_gate] -= learning_rate * dLdb[i_gate]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds at each time step:\n",
      " wish dollar setting ignoring sources pushing 99 guilt skill difficulty difficulty disgusting king note cap product former out out out cooler product idiots annie charge entirety installed security cap ignore fees cast practice summer out physics # # ignoring kit wasting wish b fashion were\n",
      "Expected Loss for random predictions: 8.294050\n",
      "Actual loss: 8.294062\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(vocab_size=vocabulary_size,\n",
    "                   hidden_size=10,\n",
    "                   bptt_truncate=1000,\n",
    "                   dicts=[word_to_index, index_to_word])\n",
    "lstm.predict(X_train[10])\n",
    "\n",
    "# Limit to 1000 examples to save time\n",
    "print(\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print(\"Actual loss: %f\" % lstm.loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM and Generating Reddit Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "model = LSTM(vocab_size=vocabulary_size,\n",
    "                   hidden_size=20,\n",
    "                   bptt_truncate=1000,\n",
    "                   dicts=[word_to_index, index_to_word])\n",
    "\n",
    "ind = np.random.randint(0, X_train.shape[0], size=100)\n",
    "losses = train_with_sgd(model, X_train[ind], y_train[ind], nepoch=2, evaluate_loss_after=1)\n",
    "\n",
    "generate(model, n_sentences=2, min_length=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help Us O Great TFLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3958  | total loss: \u001b[1m\u001b[32m1.88942\u001b[0m\u001b[0m\n",
      "\u001b[2K\r",
      "| Adam | epoch: 003 | loss: 1.88942 -- iter: 0001024/1012260\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-74d5201337f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m           \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0mshow_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m           run_id='reddit')\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-- TESTING...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-- Test with temperature of 0.8 --\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tflearn/models/generator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, run_id)\u001b[0m\n\u001b[1;32m    172\u001b[0m                          \u001b[0mdaug_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaug_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                          run_id=run_id)\n\u001b[0m\u001b[1;32m    175\u001b[0m         self.predictor = Evaluator([self.net],\n\u001b[1;32m    176\u001b[0m                                    session=self.trainer.session)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[1;32m    302\u001b[0m                                                        \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                                                        show_metric)\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                             \u001b[0;31m# Update training state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[0;32m--> 759\u001b[0;31m                                              feed_batch)\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;31m# Retrieve loss value from summary string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from six.moves import urllib\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import *\n",
    "\n",
    "path = \"python/med_reddit.csv\"\n",
    "char_idx_file = 'char_idx.pickle'\n",
    "\n",
    "maxlen = 20\n",
    "char_idx = None\n",
    "\n",
    "X, Y, char_idx = textfile_to_semi_redundant_sequences(path, \n",
    "                                                      seq_maxlen=maxlen,\n",
    "                                                      redun_step=2)\n",
    "\n",
    "\n",
    "pickle.dump(char_idx, open(char_idx_file,'wb'))\n",
    "\n",
    "g = tflearn.input_data([None, maxlen, len(char_idx)])\n",
    "g = tflearn.lstm(g, 128, return_seq=True)\n",
    "g = tflearn.dropout(g, 0.6)\n",
    "g = tflearn.lstm(g, 512)\n",
    "g = tflearn.dropout(g, 0.6)\n",
    "g = tflearn.fully_connected(g, len(char_idx), activation='softmax')\n",
    "g = tflearn.regression(g, optimizer='adam', loss='categorical_crossentropy',\n",
    "                       learning_rate=0.001)\n",
    "\n",
    "m = tflearn.SequenceGenerator(g, dictionary=char_idx,\n",
    "                              seq_maxlen=maxlen, \n",
    "                              clip_gradients=5.0,\n",
    "                              checkpoint_path='model_reddit')\n",
    "\n",
    "for i in range(10):\n",
    "    seed = random_sequence_from_textfile(path, maxlen)\n",
    "    m.fit(X, Y, validation_set=0.1, batch_size=512,\n",
    "          n_epoch=1, \n",
    "          show_metric=False,\n",
    "          run_id='reddit')\n",
    "    print(\"-- TESTING...\")\n",
    "    print(\"-- Test with temperature of 0.8 --\")\n",
    "    print(m.generate(600, temperature=0.8, seq_seed=seed))\n",
    "    \n",
    "    print(\"\\n-- Test with temperature of 0.3 --\")\n",
    "    print(m.generate(600, temperature=0.3, seq_seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- TESTING...\n",
      "tner doesn't step on the mar a bation in the a warting that same around the came in the say in the mother as a ground in the the see shat to be a for privente the becture the sing that so the oper of the ration should be a streating the sice to be a start wat the ban to the subpers and he that shit in the get the peran singer work see the a sape to be in the was that the are the start and they are preation for the ferent. And do the wart and look to a stroptel and besouss what you was a really contries just do and strist in they a que the for the relacting in a place in the wast a bet and play and they was the seave and be and that contrate and be be and the start and all to the prectars to the sare and here did a deals and they was a ployshand the sure for be and was is on the something that the beat and enesting the have the prided that with that be a precting the contrent on the get at the on the that was be post of the mack and the sacter on the becension here far and sention at the car and streace of\n"
     ]
    }
   ],
   "source": [
    "seed = random_sequence_from_textfile(path, maxlen)\n",
    "print(\"-- TESTING...\")\n",
    "print(m.generate(1000, temperature=0.5, seq_seed=seed))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide some flexibity and a good. \n",
    "It sounds the sure and stread and of the fart to a some of the part. \n",
    "In games journalism that do the press and the lan meaning.\n",
    "http://nflstrear.com/tikestementom/compotec/2010500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
