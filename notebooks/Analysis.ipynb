{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis, Goals, and Predictions\n",
    "\n",
    "Here, I'd like to go over the loss functions being used to train the models and what we are aiming for, so that we can better understand the models' performance as training progresses. My approach will follow that of Ng (2015), as outlined in the textbook __\"Deep Learning\" by Goodfellow et al.__:\n",
    "\n",
    "* Determine your goals -- error metric(s) and target (re: desired) value(s). \n",
    "* Establish a working end-to-end pipeline. \n",
    "* Determine bottlenecks in performance, their sources, and whether they're due to overfitting/underfitting/software defect(s). \n",
    "* Repeatedly make incremental changes such as gathering new data, adjusting hyperparams, or changing algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual HyperParameter Tuning\n",
    "\n",
    "Here are I'll just bullet the main ideas:\n",
    "* The learning rate is perhaps the most important hyperparameter. The training error increases appx exponentially as the learning rate decreases below its optimal value. Above the optimal value, the training error basically shoots off to infinity (vertical wall). \n",
    "* Next, the best perfomance usually comes from a large model that is regularized well, for example, by using dropout. \n",
    "* Table showing typical hyperparameter relationships with model capacity. Remember that you can basically brute force your way to good performance by jacking up the model capacity and training set size. \n",
    "\n",
    "| Hyperparameter | Increases capacity when... | \n",
    "| -------------- | -------------------------- |\n",
    "| Num hidden units | increased | \n",
    "| Learning rate | tuned optimally |\n",
    "| Convolution kernal width | increased | \n",
    "| Implicit zero padding | increased | \n",
    "| Weight decay coefficient | decreased | \n",
    "| Dropout rate | decreased | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic HyperParameter Optimization\n",
    "\n",
    "__Grid Search__: This is what I'm doing right now. User selects a small finite set of values to explore. Grid search trains a model for every joint specification of hyperparameter values in the Cartesian product of possible values. The experiment with the best _validation error_ is chosen as the best. \n",
    "\n",
    "__Random Search (Better)__: \n",
    "1. Define a marginal distribution for each hyperparameter, e.g. multinoulli for discrete hparams or uniform (log-scale) for positive real-valued hyparams. For example, if we were interested in the range $[10^{-5}, 0.1]$ for the learning rate:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\texttt{logLearningRate} &\\sim Unif[-1, -5] \\\\\n",
    "\\texttt{learningRate} &= 10^{logLearningRate}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
