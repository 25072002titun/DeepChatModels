{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General \n",
    "\n",
    "The vast majority of this notebook is directly from [here](https://keras.io/getting-started/sequential-model-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image dimensions conventions:\n",
    "\n",
    "* 2D-Data:\n",
    "     * __tf__: (rows, cols, channels) \n",
    "     * __th__:  (channels, rows, cols). \n",
    "* 3D-Data:\n",
    "    * __tf__: (conv_dim1, conv_dim2, conv_dim3, channels)\n",
    "    * __th__: (channels, conv_dim1, conv_dim2, conv_dim3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1st layer in a Sequential model needs [at least] [one of the following](https://keras.io/getting-started/sequential-model-guide/) specified. For all code snippets, assume model has been initialized via ```model = Sequential()```. The snippets show 3 _strictly equivalent_ ways of specifying input shape/size/whatever for Dense & LSTM, respectively.\n",
    "\n",
    "* __input\\_shape__:This is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). In input_shape, the batch dimension is not included.\n",
    "```python\n",
    "model.add(Dense(32, input_shape=(784,))) # Dense example \n",
    "model.add(LSTM(32, input_shape=(10, 64))) # LSTM example\n",
    "```\n",
    "* __batch\\_input\\_shape__: argument, where the batch dimension is included. This is useful for specifying a fixed batch size (e.g. with stateful RNNs).\n",
    "```python\n",
    "model.add(Dense(32, batch_input_shape=(None, 784))) # Dense example\n",
    "model.add(LSTM(32, batch_input_shape=(None, 10, 64))) # LSTM example\n",
    "```\n",
    "* __input\\_dim__: some 2D layers, such as Dense, support the specification of their input shape via the argument input_dim, and some 3D temporal layers support the arguments input_dim and input_length.\n",
    "```python\n",
    "model.add(Dense(32, input_dim=784)) # Dense example\n",
    "model.add(LSTM(32, input_length=10, input_dim=64)) # LSTM example\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Merge Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to build the following:\n",
    "<img src=\"https://s3.amazonaws.com/keras.io/img/two_branches_sequential_model.png\" width=\"250\">\n",
    "\n",
    "This is accomplished in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f0704e5795f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create the top-left and top-right branches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mleft_branch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mleft_branch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mright_branch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "\n",
    "# Create the top-left and top-right branches.\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Dense(32, input_dim=784))\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "# Combine the branches via concatenation.\n",
    "# mode can be ['sum' (default), 'concat', 'mul', 'ave', 'dot', 'cos']\n",
    "merged = Merge([left_branch, right_branch]d, mode='concat')\n",
    "\n",
    "# Feed to a softmax layer for output\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# How to train it.\n",
    "final_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "final_model.fit([input_data_1, input_data_2], targets)  # we pass one data array per model input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Links__:\n",
    "* [documentation link](https://keras.io/layers/embeddings/)\n",
    "* [embeddings.py github source code link](https://github.com/fchollet/keras/blob/master/keras/layers/embeddings.py#L8)\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "__General Information__:\n",
    "* Description: \"Turn positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]. This layer can only be used as the first layer in a model.\"\n",
    "* __Function Signature__: \n",
    "```python\n",
    "Embedding(input_dim, output_dim, \n",
    "          init='uniform', input_length=None, \n",
    "          W_regularizer=None, activity_regularizer=None, \n",
    "          W_constraint=None, mask_zero=False, \n",
    "          weights=None, dropout=0.0)\n",
    "```\n",
    "* __Input shape__: 2D tensor with shape: ```(nb_samples, sequence_length).```\n",
    "    * This reveals a very important assumption being made. The reason there is no 3rd 'input\\_dim' [for single sample at single timestep] is because _we assume it is 1_, i.e. that we are feeding in scalars (indices). Yes, there are somewhat conflicting/confusing name conventions for input dimensions vs. input shapes here.\n",
    "* __Output shape__: 3D tensor with shape: ```(nb_samples, sequence_length, output_dim).```\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "\n",
    "Particularly good parameter definitions to know/understand:\n",
    "* __input\\_length__: Length of input sequences, when it is constant. This argument is required if you are going to connect ```Flatten --> Dense``` layers upstream (without it, the shape of the dense outputs cannot be computed).\n",
    "* __dropout__:float between 0 and 1. Fraction of the embeddings to drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code cell:\n",
    "*  __model input__: an integer matrix of size (batch, input_length). Recall that the following are equivalent:\n",
    "```python\n",
    "# version used in Embedding() signature.\n",
    "input_dim=VOCAB_SIZE, input_length=SEQ_LEN \n",
    "# Both of the following can be used instead in, e.g., LSTM(...)\n",
    "batch_input_shape = (None, SEQ_LEN, VOCAB_SIZE) \n",
    "input_shape = (SEQ_LEN, VOCAB_SIZE) \n",
    "```\n",
    "* __model output__: has shape (None, 10, 64), where None is the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 10, 64)        64000       embedding_input_3[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 64,000\n",
      "Trainable params: 64,000\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN    = 10\n",
    "\n",
    "# Random numbers are sampled from Unif[0, VOCAB_SIZE). \n",
    "input_array = np.random.randint(VOCAB_SIZE, size=(BATCH_SIZE, SEQ_LEN))\n",
    "\n",
    "model = Sequential()\n",
    "# input_dim and output_dim must come first, in this order (they don't have default values). \n",
    "model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=64, input_length=SEQ_LEN))\n",
    "model.compile('rmsprop', 'mse')\n",
    "model.summary()\n",
    "\n",
    "embed_output_array = model.predict(input_array)\n",
    "assert(output_array.shape == (BATCH_SIZE, SEQ_LEN, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is assumed to have been run before any of the following examples (i.e. they all start with this in common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked LSTM for sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this model, we stack 3 LSTM layers on top of each other, making the model capable of learning higher-level temporal representations.\n",
    "\n",
    "The first two LSTMs return their full output sequences, but the last one only returns the last step in its output sequence, thus dropping the temporal dimension (i.e. converting the input sequence into a single vector).\n",
    "<img src=\"https://keras.io/img/regular_stacked_lstm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
