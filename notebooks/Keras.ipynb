{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General \n",
    "\n",
    "The vast majority of this notebook is directly from [here](https://keras.io/getting-started/sequential-model-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image dimensions conventions:\n",
    "\n",
    "* 2D-Data:\n",
    "     * __tf__: (rows, cols, channels) \n",
    "     * __th__:  (channels, rows, cols). \n",
    "* 3D-Data:\n",
    "    * __tf__: (conv_dim1, conv_dim2, conv_dim3, channels)\n",
    "    * __th__: (channels, conv_dim1, conv_dim2, conv_dim3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, one_hot, Tokenizer\n",
    "from keras.utils.data_utils import get_file\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from pprint import pprint\n",
    "\n",
    "# Get sample string of words to place in text.\n",
    "text = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(text).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK 5 reasonable-length tokenized sentences:\n",
      "[   'Let us acknowledge\\nunprejudicedly how every higher civilization hitherto has ORIGINATED!',\n",
      "    'A \"free spirit\"--this refreshing term is grateful in any mood,\\nit almost sets one aglow.',\n",
      "    'Each surrenders to the other what the other wants and\\nreceives in return its own desire.',\n",
      "    'Mutual manifestations of pleasure inspire mutual\\nsympathy, the sentiment of homogeneity.',\n",
      "    'The whole\\ncircle of his judgment and feeling is clouded and draped in religious\\nshadows.']\n"
     ]
    }
   ],
   "source": [
    "# Store what nltk does for comparison.\n",
    "nltk_sentences = sent_tokenize(text)\n",
    "# Grab reasonably-sized sentences....\n",
    "nltk_sentences = sorted(nltk_sentences, key=len)\n",
    "print(\"NLTK 5 reasonable-length tokenized sentences:\")\n",
    "start = len(nltk_sentences)//2 - 500\n",
    "stop  = start + 5\n",
    "pprint(nltk_sentences[start:stop], indent=4, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word sequence output by text_to_word_sequence (first 10 entries):\n",
      "['preface', 'supposing', 'that', 'truth', 'is', 'a', 'woman', 'what', 'then', 'is']\n",
      "\n",
      "Example outputs of Tokenizer attributes/methods:\n",
      "Document count: 101358\n",
      "\n",
      "Top 10 most common words:\n",
      " ['ghost', 'falsity', 'professes', 'petit', 'flew', 'eine', 'footing', 'brutes', 'dialogues', 'panorama']\n",
      "\n",
      "Some entries in word_index:\n",
      " [('ghost', 5282), ('intercalary', 10203), ('dregs', 9938), ('falsity', 5283), ('professes', 5284), ('unbend', 9435), ('flew', 5286), ('persuaded', 2519), ('Ã¦sthetical', 8897), ('feastful', 5862)]\n",
      "\n",
      "Some entries in another_word_index:\n",
      " [327, 429, 905, 69, 561, 920, 536, 617, 840, 561]\n",
      "['experience', 'common', 'heavy', 'world', 'satisfaction', 'silence', 'inner', 'since', 'nay', 'satisfaction']\n"
     ]
    }
   ],
   "source": [
    "# Filters out punctuation.\n",
    "word_seq = text_to_word_sequence(text)\n",
    "print(\"Word sequence output by text_to_word_sequence (first 10 entries):\")\n",
    "print(word_seq[:10], end=\"\\n\\n\")\n",
    "\n",
    "# Specify the N most common words we are interested in working with.\n",
    "vocab_size = 1000\n",
    "# nb_words: Maximum number of [most common] words to work with. \n",
    "keras_tokenizer = Tokenizer(nb_words=vocab_size)\n",
    "# Specify which text[s] to train on.\n",
    "keras_tokenizer.fit_on_texts(word_seq)\n",
    "\n",
    "print(\"Example outputs of Tokenizer attributes/methods:\")\n",
    "print(\"Document count:\", keras_tokenizer.document_count)\n",
    "most_common = sorted(keras_tokenizer.word_counts, key=keras_tokenizer.word_counts.get)\n",
    "\n",
    "print(\"\\nTop 10 most common words:\\n\", most_common[:10])\n",
    "word_index = keras_tokenizer.word_index\n",
    "index_word = {i:w for w, i in word_index.items()}\n",
    "\n",
    "print(\"\\nSome entries in word_index:\\n\", [(k, word_index[k]) for k in word_index.keys()][:10])\n",
    "\n",
    "# Get text in the form of integer ids.\n",
    "text_as_idx = one_hot(text, n=vocab_size)\n",
    "print(\"\\nSome entries in another_word_index:\\n\", text_as_idx[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[] [592] [8] ..., [] [2] []]\n"
     ]
    }
   ],
   "source": [
    "idseq = keras_tokenizer.texts_to_sequences(word_seq)\n",
    "idseq = np.array(idseq).reshape((len(idseq),)).flatten()\n",
    "print(idseq)\n",
    "#print([index_word[i] for i in np.array(idseq).flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1st layer in a Sequential model needs [at least] [one of the following](https://keras.io/getting-started/sequential-model-guide/) specified. For all code snippets, assume model has been initialized via ```model = Sequential()```. The snippets show 3 _strictly equivalent_ ways of specifying input shape/size/whatever for Dense & LSTM, respectively.\n",
    "\n",
    "* __input\\_shape__:This is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). In input_shape, the batch dimension is not included.\n",
    "```python\n",
    "model.add(Dense(32, input_shape=(784,))) # Dense example \n",
    "model.add(LSTM(32, input_shape=(10, 64))) # LSTM example\n",
    "```\n",
    "* __batch\\_input\\_shape__: argument, where the batch dimension is included. This is useful for specifying a fixed batch size (e.g. with stateful RNNs).\n",
    "```python\n",
    "model.add(Dense(32, batch_input_shape=(None, 784))) # Dense example\n",
    "model.add(LSTM(32, batch_input_shape=(None, 10, 64))) # LSTM example\n",
    "```\n",
    "* __input\\_dim__: some 2D layers, such as Dense, support the specification of their input shape via the argument input_dim, and some 3D temporal layers support the arguments input_dim and input_length.\n",
    "```python\n",
    "model.add(Dense(32, input_dim=784)) # Dense example\n",
    "model.add(LSTM(32, input_length=10, input_dim=64)) # LSTM example\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Merge Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to build the following:\n",
    "<img src=\"https://s3.amazonaws.com/keras.io/img/two_branches_sequential_model.png\" width=\"250\">\n",
    "\n",
    "This is accomplished in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f0704e5795f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create the top-left and top-right branches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mleft_branch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mleft_branch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mright_branch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "\n",
    "# Create the top-left and top-right branches.\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Dense(32, input_dim=784))\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "# Combine the branches via concatenation.\n",
    "# mode can be ['sum' (default), 'concat', 'mul', 'ave', 'dot', 'cos']\n",
    "merged = Merge([left_branch, right_branch]d, mode='concat')\n",
    "\n",
    "# Feed to a softmax layer for output\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# How to train it.\n",
    "final_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "final_model.fit([input_data_1, input_data_2], targets)  # we pass one data array per model input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Links__:\n",
    "* [documentation link](https://keras.io/layers/embeddings/)\n",
    "* [embeddings.py github source code link](https://github.com/fchollet/keras/blob/master/keras/layers/embeddings.py#L8)\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "__General Information__:\n",
    "* Description: \"Turn positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]. This layer can only be used as the first layer in a model.\"\n",
    "* __Function Signature__: \n",
    "```python\n",
    "Embedding(input_dim, output_dim, \n",
    "          init='uniform', input_length=None, \n",
    "          W_regularizer=None, activity_regularizer=None, \n",
    "          W_constraint=None, mask_zero=False, \n",
    "          weights=None, dropout=0.0)\n",
    "```\n",
    "* __Input shape__: 2D tensor with shape: ```(nb_samples, sequence_length).```\n",
    "    * This reveals a very important assumption being made. The reason there is no 3rd 'input\\_dim' [for single sample at single timestep] is because _we assume it is 1_, i.e. that we are feeding in scalars (indices). Yes, there are somewhat conflicting/confusing name conventions for input dimensions vs. input shapes here.\n",
    "* __Output shape__: 3D tensor with shape: ```(nb_samples, sequence_length, output_dim).```\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "\n",
    "Particularly good parameter definitions to know/understand:\n",
    "* __input\\_length__: Length of input sequences, when it is constant. This argument is required if you are going to connect ```Flatten --> Dense``` layers upstream (without it, the shape of the dense outputs cannot be computed).\n",
    "* __dropout__:float between 0 and 1. Fraction of the embeddings to drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code cell:\n",
    "*  __model input__: an integer matrix of size (batch, input_length). Recall that the following are equivalent:\n",
    "```python\n",
    "# version used in Embedding() signature.\n",
    "input_dim=VOCAB_SIZE, input_length=SEQ_LEN \n",
    "# Both of the following can be used instead in, e.g., LSTM(...)\n",
    "batch_input_shape = (None, SEQ_LEN, VOCAB_SIZE) \n",
    "input_shape = (SEQ_LEN, VOCAB_SIZE) \n",
    "```\n",
    "* __model output__: has shape (None, 10, 64), where None is the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 10, 64)        64000       embedding_input_3[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 64,000\n",
      "Trainable params: 64,000\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN    = 10\n",
    "\n",
    "# Random numbers are sampled from Unif[0, VOCAB_SIZE). \n",
    "input_array = np.random.randint(VOCAB_SIZE, size=(BATCH_SIZE, SEQ_LEN))\n",
    "\n",
    "model = Sequential()\n",
    "# input_dim and output_dim must come first, in this order (they don't have default values). \n",
    "model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=64, input_length=SEQ_LEN))\n",
    "model.compile('rmsprop', 'mse')\n",
    "model.summary()\n",
    "\n",
    "embed_output_array = model.predict(input_array)\n",
    "assert(output_array.shape == (BATCH_SIZE, SEQ_LEN, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## LSTM Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The cell below is assumed to have been run before any of the following examples (i.e. they all start with this in common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Sequence classification with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.add(Embedding(max_features, 256, input_length=maxlen))\n",
    "model.add(LSTM(output_dim=128, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=16, nb_epoch=10)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Stacked LSTM for sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "In this model, we stack 3 LSTM layers on top of each other, making the model capable of learning higher-level temporal representations.\n",
    "\n",
    "The first two LSTMs return their full output sequences, but the last one only returns the last step in its output sequence, thus dropping the temporal dimension (i.e. converting the input sequence into a single vector).\n",
    "<img src=\"https://keras.io/img/regular_stacked_lstm.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_dim = 16\n",
    "timesteps = 8\n",
    "nb_classes = 10\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# generate dummy training data\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, nb_classes))\n",
    "\n",
    "# generate dummy validation data\n",
    "x_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, nb_classes))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64, nb_epoch=5,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Same stacked LSTM model, rendered \"stateful\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_dim = 16\n",
    "timesteps = 8\n",
    "nb_classes = 10\n",
    "batch_size = 32\n",
    "\n",
    "# expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "# note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True,\n",
    "               batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True))\n",
    "model.add(LSTM(32, stateful=True))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# generate dummy training data\n",
    "x_train = np.random.random((batch_size * 10, timesteps, data_dim))\n",
    "y_train = np.random.random((batch_size * 10, nb_classes))\n",
    "\n",
    "# generate dummy validation data\n",
    "x_val = np.random.random((batch_size * 3, timesteps, data_dim))\n",
    "y_val = np.random.random((batch_size * 3, nb_classes))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size, nb_epoch=5,\n",
    "          validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}