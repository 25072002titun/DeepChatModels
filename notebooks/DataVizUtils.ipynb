{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I should probably start standardizing my data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "sys.path.append('..')\n",
    "from imp import reload\n",
    "from data import reddit_preprocessor, DataHelper\n",
    "from data.reddit_preprocessor import *\n",
    "import json\n",
    "from pprint import pprint\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "jtplot.style('onedork', ticks=True, fscale=1.5)\n",
    "jtplot.figsize(x=11., y=8.)\n",
    "DATA_ROOT = '/home/brandon/Datasets/test_data'\n",
    "FROM = os.path.join(DATA_ROOT, 'train_from.txt')\n",
    "TO = os.path.join(DATA_ROOT, 'train_to.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_NAMES = ['inp_sentence', 'resp_sentence']\n",
    "\n",
    "def make_dataframe(data_dir):\n",
    "    \"\"\"\n",
    "    data_dir: contains train_from.txt, train_to.txt\n",
    "    \"\"\"\n",
    "    from_lines = []\n",
    "    to_lines = []\n",
    "    with open(os.path.join(data_dir, 'train_from.txt'), 'r') as from_file:\n",
    "        with open(os.path.join(data_dir, 'train_to.txt'), 'r') as to_file:\n",
    "            from_line = from_file.readline()\n",
    "            to_line = to_file.readline()\n",
    "            while from_line and to_line:\n",
    "                from_lines.append(from_line.strip())\n",
    "                to_lines.append(to_line.strip())\n",
    "                from_line = from_file.readline()\n",
    "                to_line = to_file.readline()\n",
    "            df = pd.DataFrame(np.stack((from_lines, to_lines), 1),\n",
    "                              columns=COL_NAMES)        \n",
    "        return df\n",
    "    \n",
    "def word_tokenize(df):\n",
    "    word_freq = {}\n",
    "    \n",
    "    # I know. I KNOW.\n",
    "    sentences = np.squeeze(list(((map(\n",
    "        DataHelper.word_tokenizer, \n",
    "        list(np.expand_dims(df[COL_NAMES[0]].values, 1)))))))\n",
    "    \n",
    "    word_freq['from'] = Counter(chain.from_iterable(sentences))\n",
    "    # Stop judging me.\n",
    "    sentences = np.squeeze(list(((map(\n",
    "        DataHelper.word_tokenizer, \n",
    "        list(np.expand_dims(df[COL_NAMES[1]].values, 1)))))))\n",
    "    word_freq['to'] = Counter(chain.from_iterable(sentences))\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "def plot_freq_dist(word_freq, n):\n",
    "    words_dict = {}\n",
    "    for dist in word_freq:\n",
    "        most_comm = word_freq[dist].most_common(n)\n",
    "        words, counts = zip(*most_comm)\n",
    "        words_dict[dist] = words\n",
    "        counts_series = pd.Series.from_array(counts)\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        ax = counts_series.plot(kind='bar')\n",
    "\n",
    "        ax.set_title('Frequency Distribution: ' + dist)\n",
    "        ax.set_ylabel('Counts')\n",
    "        ax.set_xlabel('Words')\n",
    "        ax.set_xticklabels(words_dict[dist])\n",
    "\n",
    "    from_words = set(words_dict['from'])\n",
    "    to_words = set(words_dict['to'])\n",
    "    common_words = from_words.intersection(to_words)\n",
    "    common_word_freqs = [\n",
    "        [word_freq['from'][w] for w in common_words],\n",
    "        [word_freq['to'][w] for w in common_words]]\n",
    "    \n",
    "    ind = np.arange(len(common_words))\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    p1 = plt.bar(ind, common_word_freqs[0], width=0.5, color='b')\n",
    "    p2 = plt.bar(ind, common_word_freqs[1], width=0.5, color='r')\n",
    "    plt.xticks(ind, common_words)\n",
    "    plt.legend((p1[0], p2[0]), ('From', 'To'))\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 10000)\n",
    "df = make_dataframe(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 10000)\n",
    "df.head(len(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_freq = word_tokenize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = plot_freq_dist(word_freq, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
