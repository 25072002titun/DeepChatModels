{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tokenizing Words and Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Link to tutorial](https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/)\n",
    "\n",
    "Definitions:\n",
    "* __Corpus__: Body of text, singular. Corpora is the plural of this. \n",
    "* __Lexicon__: words and their meanings. Example: English dictionary.\n",
    "* __Token__: Each \"entity\" that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is \"tokenized\" into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of sent_tokenize(EXAMPLE_TEXT):\n",
      "[   'Hello Mr. Smith, how are you doing today?',\n",
      "    'The weather is great, and Python is awesome.',\n",
      "    'The sky is pinkish-blue.',\n",
      "    \"You shouldn't eat cardboard.\"]\n",
      "\n",
      "Output of word_tokenize(EXAMPLE_TEXT):\n",
      "[   'Hello',\n",
      "    'Mr.',\n",
      "    'Smith',\n",
      "    ',',\n",
      "    'how',\n",
      "    'are',\n",
      "    'you',\n",
      "    'doing',\n",
      "    'today',\n",
      "    '?',\n",
      "    'The',\n",
      "    'weather',\n",
      "    'is',\n",
      "    'great',\n",
      "    ',',\n",
      "    'and',\n",
      "    'Python',\n",
      "    'is',\n",
      "    'awesome',\n",
      "    '.',\n",
      "    'The',\n",
      "    'sky',\n",
      "    'is',\n",
      "    'pinkish-blue',\n",
      "    '.',\n",
      "    'You',\n",
      "    'should',\n",
      "    \"n't\",\n",
      "    'eat',\n",
      "    'cardboard',\n",
      "    '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pprint import pprint\n",
    "\n",
    "EXAMPLE_TEXT = (\"Hello Mr. Smith, how are you doing today? \"\n",
    "                \"The weather is great, and Python is awesome. \"\n",
    "                \"The sky is pinkish-blue. You shouldn't eat cardboard.\")\n",
    "\n",
    "print(\"Output of sent_tokenize(EXAMPLE_TEXT):\")\n",
    "pprint(sent_tokenize(EXAMPLE_TEXT), indent=4)\n",
    "\n",
    "print(\"\\nOutput of word_tokenize(EXAMPLE_TEXT):\")\n",
    "pprint(word_tokenize(EXAMPLE_TEXT), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Stop words are informally defined as useless words(data). Basically a casual term people use for words they don't care about. Seems like most people consider pronouns, articles, prepositions, etc. to be stop words. For example, nltk default outputs:\n",
    "\n",
    "```python\n",
    ">>> set(stopwords.words('english'))\n",
    "{'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'} \n",
    "```\n",
    "\n",
    "Below we show how this can be used on a sentence with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining (word) tokens after filtering out stop words:\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words   = set(stopwords.words('english'))\n",
    "word_tokens  = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "print(\"Remaining (word) tokens after filtering out stop words:\")\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Link to tutorial](https://pythonprogramming.net/stemming-nltk-tutorial/?completed=/stop-words-nltk-tutorial/)\n",
    "\n",
    "The general idea of stemming is removing redundant parts of words (\"affixes\") that don't really provide new meaning. For example, removing 'ing' off the word 'riding'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "for w in example_words: print(ps.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "by\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "new_text = (\"It is important to by very pythonly while you are pythoning with python. \"\n",
    "            \"All pythoners have pythoned poorly at least once.\")\n",
    "for w in word_tokenize(new_text):\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to tutorial](https://pythonprogramming.net/lemmatizing-nltk-tutorial/)\n",
    "\n",
    "TL;DR: same as stemming except actual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats --> cat\n",
      "cacti --> cactus\n",
      "geese --> goose\n",
      "rocks --> rock\n",
      "python --> python\n",
      "run --> run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def print_lematize(s, **kwargs):\n",
    "    print(s, \"-->\", lemmatizer.lemmatize(s, **kwargs))\n",
    "for word in ['cats', 'cacti', 'geese', 'rocks', 'python', 'run']:\n",
    "    print_lematize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better --> better\n",
      "better --> good\n"
     ]
    }
   ],
   "source": [
    "print_lematize('better')\n",
    "# pos means part-of-speech\n",
    "# 'a' means adjective.\n",
    "# default is pos='n'\n",
    "print_lematize('better', pos='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The corpora with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of accessing one of many text documents, the gutenberg bible, from the corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep.\n",
      "And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# sample text\n",
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "\n",
    "tok = sent_tokenize(sample)\n",
    "\n",
    "for x in range(5):\n",
    "    print(tok[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "\n",
    "A lexical database for the English language, which was created by Princeton, and is part of the NLTK corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syns:\n",
      " [Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n",
      "\n",
      "\n",
      "\n",
      "['plan', 'program', 'programme']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# How to find synonyms ('synsets').\n",
    "syns = wordnet.synsets(\"program\")\n",
    "print(\"syns:\\n\", syns)\n",
    "print(\"\\n\\n\")\n",
    "pprint([syns[0].lemmas()[i].name() for i in range(len(syns[0].lemmas()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with NLTK Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717),\n",
      " ('the', 76529),\n",
      " ('.', 65876),\n",
      " ('a', 38106),\n",
      " ('and', 35576),\n",
      " ('of', 34123),\n",
      " ('to', 31937),\n",
      " (\"'\", 30585),\n",
      " ('is', 25195),\n",
      " ('in', 21822),\n",
      " ('s', 18513),\n",
      " ('\"', 17612),\n",
      " ('it', 16107),\n",
      " ('that', 15924),\n",
      " ('-', 15595)]\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category) \n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "# Uncommented below since long output.\n",
    "#print(\"Output of (joined) documents[1][:10][0]\")\n",
    "#pprint(' '.join(documents[1][:10][0]), width=80)\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "pprint(all_words.most_common(15))\n",
    "print(all_words[\"stupid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/brandon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
