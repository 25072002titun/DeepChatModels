{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from progressbar import ProgressBar\n",
    "from multiprocessing import Process, Queue\n",
    "import threading\n",
    "import enchant\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = '/Users/ivan/Documents/sp_17/reddit_data'\n",
    "DATA_YEARS = ['2007']\n",
    "\n",
    "RAW_DATA_FILES = [os.listdir(os.path.join(DATA_ROOT, 'raw_data', year)) for year in DATA_YEARS]\n",
    "\n",
    "RAW_DATA_ABS_FILES = []\n",
    "\n",
    "for i in range(len(DATA_YEARS)):\n",
    "    for j in range(len(RAW_DATA_FILES[i])):\n",
    "        if RAW_DATA_FILES[i][j].startswith('.'):\n",
    "            pass\n",
    "        else:\n",
    "            RAW_DATA_ABS_FILES.append( os.path.join(DATA_ROOT, 'raw_data' , DATA_YEARS[i], RAW_DATA_FILES[i][j]))\n",
    "RAW_DATA_FILES = RAW_DATA_ABS_FILES\n",
    "RAW_DATA_ABS_FILES = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE   = re.compile(r\"\\d\")\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w for w in words if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_to_word_dictionary(split_sentence, dictionary):\n",
    "    for word in split_sentence:\n",
    "        if word.lower() in dictionary:\n",
    "            dictionary[word.lower()] += 1\n",
    "        else:\n",
    "            dictionary[word.lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(file, word_q):\n",
    "    print(\"Current PID: \" + str(os.getpid()))\n",
    "    word_dict = {}\n",
    "    pbar = ProgressBar()\n",
    "    with open(file) as f:\n",
    "        for line in pbar(f):\n",
    "            add_to_word_dictionary(basic_tokenizer(json.loads(line)['body']), word_dict)\n",
    "    word_q.put(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing multiple processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PID: 14700\n",
      "Current PID: 14699\n",
      "Current PID: 14701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 663 Elapsed Time: 0:00:33                                                    \n",
      "| 1307 Elapsed Time: 0:01:06                                                   \n",
      "| 1370 Elapsed Time: 0:01:09                                                   \n"
     ]
    }
   ],
   "source": [
    "word_dicts = []\n",
    "pbar = ProgressBar()\n",
    "for file in RAW_DATA_FILES:\n",
    "    word_dict = {}\n",
    "    p = Process(target=process_file, args=(file, word_dict,))\n",
    "    p.start()\n",
    "    word_dicts.append(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread Count: 5\n",
      "Thread Count: 5\n",
      "Thread Count: 5\n",
      "Current PID: 14856\n",
      "Current PID: 14857\n",
      "Current PID: 14855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\ 630 Elapsed Time: 0:00:31                                                    \n",
      "| 1066 Elapsed Time: 0:00:54                                                   \n",
      "| 1110 Elapsed Time: 0:00:56                                                   \n"
     ]
    }
   ],
   "source": [
    "word_dicts = []\n",
    "pbar = ProgressBar()\n",
    "for file in RAW_DATA_FILES:\n",
    "    print(\"Thread Count: %i\" % threading.active_count())\n",
    "    word_dict = {}\n",
    "    p = Process(target=process_file, args=(file, word_dict,))\n",
    "    p.start()\n",
    "    word_dicts.append(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. All cores running at about 90%\n",
    "2. Takes 0:33, 1:06, 1:09 for the three to be done.\n",
    "3. Brandon is done in 15 seconds! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using just one process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ 4 Elapsed Time: 0:00:00                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PID: 12842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 281 Elapsed Time: 0:00:14                                                    \n",
      "| 847 Elapsed Time: 0:00:42                                                    \n",
      "| 1449 Elapsed Time: 0:01:12                                                   \n"
     ]
    }
   ],
   "source": [
    "print(\"Current PID: \" + str(os.getpid()))\n",
    "word_dict = {}\n",
    "pbar = ProgressBar()\n",
    "for file in RAW_DATA_FILES:\n",
    "    with open(file) as f:\n",
    "        for line in pbar(f):\n",
    "            add_to_dictionary(basic_tokenizer(json.loads(line)['body']), word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cores running at about 35 % average usage.\n",
    "2. Takes 0:12, 0:40, 1:11 for the tree to be done.\n",
    "3. Using the threading package we see that in this one process there are 5 threads running. \n",
    "4. Brandons computer takes 0:06, 0:20, 0:35 secnods to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to generate scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a dictionary that has KEY: name, value: score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comment_score(comment, word_freq):\n",
    "    if type(comment) == str:\n",
    "        comment = basic_tokenizer(comment)\n",
    "    word_count = len(comment)\n",
    "    score = 0\n",
    "    for word in comment:\n",
    "        if not d.check(word):\n",
    "            try: \n",
    "                score = score + 1.0/word_freq[word]\n",
    "            except ZeroDivisionError:\n",
    "                score = score + 1.0\n",
    "    try:\n",
    "        return score / word_count\n",
    "    except ZeroDivisionError:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 3 Elapsed Time: 0:00:00                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 3223 Elapsed Time: 0:02:48                                                   \n",
      "| 3226 Elapsed Time: 0:02:48                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 9127 Elapsed Time: 0:08:05                                                   \n",
      "| 9131 Elapsed Time: 0:08:05                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 16389 Elapsed Time: 0:14:23                                                  \n"
     ]
    }
   ],
   "source": [
    "score_dict = {}\n",
    "pbar = ProgressBar()\n",
    "d = enchant.Dict('en_US')\n",
    "for file in RAW_DATA_FILES:\n",
    "    print(threading.active_count())\n",
    "    with open(file) as f:\n",
    "        for line in pbar(f):\n",
    "            data = json.loads(line)\n",
    "            score_dict[data['name']] = comment_score((data['body']), word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_ROOT, 'compressed_data' , '07_08' , 'score_dict'), 'wb') as pickle_file:\n",
    "    pickle.dump(score_dict, pickle_file)\n",
    "    \n",
    "with open(os.path.join(DATA_ROOT, 'compressed_data' , '07_08' , 'word_dict'), 'wb') as pickle_file:\n",
    "    pickle.dump(word_dict, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using multiple processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file_comments(file, score_dicts):\n",
    "    print(\"Current PID: \" + str(os.getpid()))\n",
    "    score_dict = {}\n",
    "    with open(file) as f:\n",
    "        for line in pbar(f):\n",
    "            data = json.loads(line)\n",
    "            score_dict[data['name']] = comment_score((data['body']), word_dict)\n",
    "    score_dicts.append(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "Current PID: 15694\n",
      "Current PID: 15693\n",
      "Current PID: 15695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 6191 Elapsed Time: 0:05:35                                                   \n",
      "| 10476 Elapsed Time: 0:09:42                                                  \n",
      "| 11381 Elapsed Time: 0:10:07                                                  \n"
     ]
    }
   ],
   "source": [
    "score_dicts = []\n",
    "pbar = ProgressBar()\n",
    "d = enchant.Dict('en_US')\n",
    "for file in RAW_DATA_FILES:\n",
    "    print(threading.active_count())\n",
    "    p = Process(target=process_file_comments, args=(file, score_dicts,))\n",
    "    p.start()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using multiple process in this case is actually noticeable.\n",
    "2. Since there is more work here than just IO, which is probably the bottleneck in the previous test(converting to words).\n",
    "3. 14:23 -> 10:07, so a non-negligible improvement.\n",
    "4. The code is not fully functional since the number score_dicts list is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Queue objects to store return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PID: 17214\n",
      "Current PID: 17213\n",
      "Current PID: 17212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 611 Elapsed Time: 0:00:31                                                    \n",
      "| 988 Elapsed Time: 0:00:50                                                    \n",
      "| 1033 Elapsed Time: 0:00:52                                                   \n"
     ]
    }
   ],
   "source": [
    "word_q = Queue()\n",
    "pbar = ProgressBar()\n",
    "for file in RAW_DATA_FILES:\n",
    "    p = Process(target=process_file, args=(file, word_q,))\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(word_q.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a dictionary of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multiple processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file_sentences(file, q):\n",
    "    print(\"Current PID: \" + str(os.getpid()))\n",
    "    sentence_dict = {}\n",
    "    pbar = ProgressBar()\n",
    "    with open(file) as f:\n",
    "        for line in pbar(f):\n",
    "            data = json.loads(line)\n",
    "            sentence_dict[data['name']] = (data['body'])\n",
    "    q.put(sentence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PID: 17487\n",
      "Current PID: 17488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "/ 0 Elapsed Time: 0:00:00                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PID: 17489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 170 Elapsed Time: 0:00:08                                                    \n",
      "| 307 Elapsed Time: 0:00:15                                                    \n",
      "| 316 Elapsed Time: 0:00:16                                                    \n"
     ]
    }
   ],
   "source": [
    "sentence_q = Queue()\n",
    "pbar = ProgressBar()\n",
    "for file in RAW_DATA_FILES:\n",
    "    p = Process(target=process_file_sentences, args=(file, sentence_q,))\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 67 Elapsed Time: 0:00:03                                                     \n",
      "| 237 Elapsed Time: 0:00:11                                                    \n",
      "| 402 Elapsed Time: 0:00:20                                                    \n"
     ]
    }
   ],
   "source": [
    "sentence_dict = {}\n",
    "pbar = ProgressBar()\n",
    "for file in RAW_DATA_FILES:\n",
    "    with open(file) as f:\n",
    "        for line in pbar(f):\n",
    "            data = json.loads(line)\n",
    "            sentence_dict[data['name']] = (data['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dealing with merging the results from using multiple processes takes longer than just running one process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_ROOT, 'compressed_data' , '07_08' , 'sentence_dict'), 'wb') as pickle_file:\n",
    "    pickle.dump(sentence_dict, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the sentence dictionary to generate the word_dict and sentence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (886802 of 886802) |#################| Elapsed Time: 0:01:07 Time: 0:01:07\n"
     ]
    }
   ],
   "source": [
    "new_word_dict = {}\n",
    "pbar = ProgressBar()\n",
    "for sentence in pbar(sentence_dict.keys()):\n",
    "    add_to_word_dictionary(basic_tokenizer(sentence_dict[sentence]), new_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (886802 of 886802) |#################| Elapsed Time: 0:01:03 Time: 0:01:03\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence_dict = {}\n",
    "pbar = ProgressBar()\n",
    "for sentence in pbar(sentence_dict.keys()):\n",
    "    tokenized_sentence_dict[sentence] = basic_tokenizer(sentence_dict[sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (886802 of 886802) |#################| Elapsed Time: 0:00:43 Time: 0:00:43\n"
     ]
    }
   ],
   "source": [
    "new_word_dict = {}\n",
    "pbar = ProgressBar()\n",
    "for sentence in pbar(tokenized_sentence_dict.keys()):\n",
    "    add_to_word_dictionary(tokenized_sentence_dict[sentence], new_word_dict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
