{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.training import bucket_by_sequence_length\n",
    "\n",
    "tf.reset_default_graph()\n",
    "BASE = '/home/brandon/terabyte/Datasets/test_data/'\n",
    "file = os.path.join(BASE, 'train_from.txt.ids1000')\n",
    "sess = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "omg this works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A **TFRecords** file contains a sequence of strings with CRC hashes. Each record has the format\n",
    "\n",
    "```python\n",
    "uint64 length\n",
    "uint32 masked_crc32_of_length\n",
    "byte   data[length]\n",
    "uint32 masked_crc32_of_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently in dir:  /home/brandon/Documents/seq2seq_projects/notebooks\n"
     ]
    }
   ],
   "source": [
    "tfrecords_fname = 'test.tfrecords'\n",
    "print(\"Currently in dir: \", os.getcwd())\n",
    "train_file = os.path.join(BASE, 'train_from.txt.ids1000')\n",
    "with tf.gfile.GFile(train_file, mode=\"r\") as source_file:\n",
    "    with tf.python_io.TFRecordWriter(tfrecords_fname) as writer:\n",
    "        source = source_file.readline()\n",
    "        while source:\n",
    "            example = tf.train.SequenceExample()\n",
    "            example.context.feature['length'].int64_list.value.append(len([int(x) for x in source.split()]))\n",
    "            for x in source.split():\n",
    "                example.feature_lists.feature_list['encoder_tokens'].feature.add().int64_list.value.append(int(x))\n",
    "            writer.write(example.SerializeToString())\n",
    "            #writer.write(tf.compat.as_bytes(source))\n",
    "            source = source_file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket_by_sequence_length??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = None\n",
    "tf.reset_default_graph()\n",
    "sess = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "capacity = 5\n",
    "LAST_RUNNER = 'batch_dequeue'\n",
    "CONTEXT_FEATURES = {'length': tf.FixedLenFeature([], dtype=tf.int64)}\n",
    "SEQUENCE_FEATURES = {'encoder_tokens': tf.FixedLenSequenceFeature([], dtype=tf.int64)}\n",
    "tfrecords_fname = 'test.tfrecords'\n",
    "\n",
    "filename_queue = tf.train.string_input_producer([tfrecords_fname])\n",
    "reader = tf.TFRecordReader(name='tfrecord_reader')\n",
    "_, next_raw = reader.read(filename_queue, name='read_records')\n",
    "\n",
    "q = tf.RandomShuffleQueue(capacity=capacity,min_after_dequeue=0,dtypes=tf.string,shapes=[()],name='randomize_records')\n",
    "enqueue_op = q.enqueue(next_raw)\n",
    "example_dq = q.dequeue()\n",
    "seq_len_parsed, sequence_parsed = tf.parse_single_sequence_example(serialized=example_dq,\n",
    "                                                     context_features=CONTEXT_FEATURES,\n",
    "                                                     sequence_features=SEQUENCE_FEATURES)\n",
    "seq_len = seq_len_parsed['length'] \n",
    "sequence = sequence_parsed['encoder_tokens']\n",
    "\n",
    "numberOfThreads = 1\n",
    "qr = tf.train.QueueRunner(q, [enqueue_op] * numberOfThreads)\n",
    "# Don't forget to add your \"QueueRunner\" to the QUEUE_RUNNERS collection\n",
    "tf.train.add_queue_runner(qr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_length, outputs = bucket_by_sequence_length(\n",
    "    input_length=tf.to_int32(seq_len),\n",
    "    tensors=[sequence],\n",
    "    batch_size=2, bucket_boundaries=[2, 5], capacity=5,\n",
    "    dynamic_pad=True, name='bucket_by_sequence_length', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ay\n",
      "[9 6]\n",
      "[array([[10,  7,  6, 14, 23,  6, 31, 19,  4],\n",
      "       [17,  8, 22, 43, 28,  4,  0,  0,  0]])]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    fetched = sess.run({'out_len': sequence_length, 'out': outputs})\n",
    "    print('ay')\n",
    "    print(fetched['out_len'])\n",
    "    print(fetched['out'])\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
