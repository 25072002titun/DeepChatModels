{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Grammar and Word Lists\n",
    "\n",
    "It would be nice to clean up a dataset which contains grammatically incorrect language and make it seem more professional. \n",
    "\n",
    "To do this we will experiment with numerous open source libraries to determine their efficacy. \n",
    "\n",
    "We also want to see if we can replace misspelled words and expand contrations into their correct forms.\n",
    "\n",
    "Another eventual task will be to determine the 'quality' of a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/ivan/Documents/sp_17/reddit_data/raw_data/2007/RC_2007-10.json',\n",
      " '/Users/ivan/Documents/sp_17/reddit_data/raw_data/2007/RC_2007-11.json',\n",
      " '/Users/ivan/Documents/sp_17/reddit_data/raw_data/2007/RC_2007-12.json']\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = '/home/brandon/terabyte/Datasets/reddit_data' \n",
    "# Determine if this directory exists, if not use Ivan's directory.\n",
    "if (os.path.isdir(DATA_ROOT)):\n",
    "    pass\n",
    "else:\n",
    "    DATA_ROOT = '/Users/ivan/Documents/sp_17/reddit_data'\n",
    "DATA_YEAR = '2007'\n",
    "# Use os.path.join; it will figure out the '/' in between.\n",
    "RAW_DATA_FILES = os.listdir(os.path.join(DATA_ROOT, 'raw_data', DATA_YEAR))\n",
    "# Always work with full pathnames to be safe.\n",
    "RAW_DATA_FILES = [os.path.join(DATA_ROOT, 'raw_data', DATA_YEAR, file) for file in RAW_DATA_FILES \n",
    "                  if not file.startswith('.')]\n",
    "pprint(RAW_DATA_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_json(RAW_DATA_FILES[0], lines=True)\n",
    "    init_num_rows = len(df)\n",
    "    print(\"Number of lines in raw data file\", init_num_rows)\n",
    "    pprint(\"Column names from raw data file:\")\n",
    "    pprint(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in raw data file 150429\n",
      "'Column names from raw data file:'\n",
      "Index(['archived', 'author', 'author_flair_css_class', 'author_flair_text',\n",
      "       'body', 'controversiality', 'created_utc', 'distinguished', 'downs',\n",
      "       'edited', 'gilded', 'id', 'link_id', 'name', 'parent_id',\n",
      "       'retrieved_on', 'score', 'score_hidden', 'subreddit', 'subreddit_id',\n",
      "       'ups'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_len_update(df):\n",
    "    print(\"Now there are\", len(df), \"rows.\")\n",
    "    \n",
    "def root_comments(df):\n",
    "    '''Build list determining which rows of df are root comments.\n",
    "    \n",
    "    Returns: \n",
    "        list of length equal to the number of rows in our data frame. \n",
    "    '''\n",
    "    root_value = []\n",
    "    # Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple.\n",
    "    for row in df.itertuples():\n",
    "        root_value.append(row.parent_id == row.link_id)\n",
    "    return root_value\n",
    "\n",
    "def random_rows_generator(num_rows_per_print, num_rows_total):\n",
    "    num_iterations = num_rows_total // num_rows_per_print \n",
    "    shuffled_indices = np.arange(num_rows_per_print * num_iterations)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    for batch in shuffled_indices.reshape(num_iterations, num_rows_per_print):\n",
    "        yield batch\n",
    "        \n",
    "#rand_rows = random_rows_generator(4, len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Start by removing comments without a body (deleted).\n",
    "* Remove comments larger than 150 words long.\n",
    "* Remove unneccesary columns. \n",
    "* Add a column determining whether a row is a root comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_clean(df):\n",
    "    df['root'] = root_comments(df)\n",
    "    df = df[['author', 'body', 'link_id', 'parent_id', 'name', 'root', 'subreddit']]\n",
    "    df.style.set_properties(subset=['body'], **{'width': '500px'})\n",
    "    df.style.set_properties(**{'text-align': 'left'})\n",
    "    show_len_update(df)\n",
    "    df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now there are 150429 rows.\n"
     ]
    }
   ],
   "source": [
    "df = initial_clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>name</th>\n",
       "      <th>root</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bostich</td>\n",
       "      <td>test</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299an</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>igiveyoumylife</td>\n",
       "      <td>much smoother.\\r\\n\\r\\nIm just glad reddit is b...</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299ao</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arve</td>\n",
       "      <td>Can we please deprecate the word \"Ajax\" now? \\...</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c02999p</td>\n",
       "      <td>t1_c0299ap</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299aq</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gigaquack</td>\n",
       "      <td>Oh, I see. Fancy schmancy \"submitting....\"</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299ah</td>\n",
       "      <td>t1_c0299ar</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               body  \\\n",
       "0         bostich                                               test   \n",
       "1  igiveyoumylife  much smoother.\\r\\n\\r\\nIm just glad reddit is b...   \n",
       "2            Arve  Can we please deprecate the word \"Ajax\" now? \\...   \n",
       "3       [deleted]                                          [deleted]   \n",
       "4       gigaquack         Oh, I see. Fancy schmancy \"submitting....\"   \n",
       "\n",
       "    link_id   parent_id        name   root   subreddit  \n",
       "0  t3_5yba3    t3_5yba3  t1_c0299an   True  reddit.com  \n",
       "1  t3_5yba3    t3_5yba3  t1_c0299ao   True  reddit.com  \n",
       "2  t3_5yba3  t1_c02999p  t1_c0299ap  False  reddit.com  \n",
       "3  t3_5yba3    t3_5yba3  t1_c0299aq   True  reddit.com  \n",
       "4  t3_5yba3  t1_c0299ah  t1_c0299ar  False  reddit.com  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modify_list = [('\\r\\n', ' '),\n",
    "               ('\\n', ' '),\n",
    "               ('\\r', ' '),\n",
    "               ('&gt;', ' '),\n",
    "               ('&lt;', ' '),\n",
    "               ('/__|\\*|\\#|(?:\\[([^\\]]*)\\]\\([^)]*\\))/gm', '[link]'),\n",
    "               ('https?:\\/\\/(?:www\\.|(?!www))[^\\s\\.]+\\.[^\\s]{2,}|www\\.[^\\s]+\\.[^\\s]{2,}', '[link]'),\n",
    "               ('\\d+', 'NUMBER'),\n",
    "               ('\\[', ''),\n",
    "               ('\\]', ''),\n",
    "               ('\\/\\/', ''),\n",
    "               ('\\.\\.\\.', '. ')\n",
    "              ]\n",
    "\n",
    "modify_value = {'\\r\\n': 1,\n",
    "               '\\n': 1,\n",
    "               '\\r': 1,\n",
    "               '&gt;': 10,\n",
    "               '&lt;': 10,\n",
    "               '/__|\\*|\\#|(?:\\[([^\\]]*)\\]\\([^)]*\\))/gm': 100,\n",
    "               'https?:\\/\\/(?:www\\.|(?!www))[^\\s\\.]+\\.[^\\s]{2,}|www\\.[^\\s]+\\.[^\\s]{2,}': 100,\n",
    "               '\\d+': 1000,\n",
    "               '\\[': 10000,\n",
    "               '\\]': 10000,\n",
    "               '\\/\\/': 10000,\n",
    "               '\\.\\.\\.': 100000\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_with_tracking(df):\n",
    "    df = df.loc[df.body != '[deleted]'].reset_index(drop=True)\n",
    "    df.style.set_properties(subset=['body'], **{'width': '800px'})\n",
    "    df['body'] = df['body'].map(lambda s: s.strip().lower())\n",
    "    \n",
    "    total_mods = {}\n",
    "    if 'mods' not in df: \n",
    "        df['mods'] = np.zeros(len(df['body']), dtype=int)\n",
    "    for patrn in modify_list:\n",
    "        new_df = df['body'].replace({patrn[0]: patrn[1]}, regex=True, inplace=False)\n",
    "        modifications = list((np.where(new_df.values != df['body'].values))[0])\n",
    "        df['body'] = new_df\n",
    "        df['mods'][modifications] += modify_value[patrn[0]]\n",
    "        total_mods[patrn[0]] = len(modifications)\n",
    "    return df, total_mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df,total_mods = clean_with_tracking(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>name</th>\n",
       "      <th>root</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>mods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126315</th>\n",
       "      <td>folderol</td>\n",
       "      <td>muscle memory.   or he never knew how to run a...</td>\n",
       "      <td>t3_5zj73</td>\n",
       "      <td>t1_c02cfl4</td>\n",
       "      <td>t1_c02cheo</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>100001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126316</th>\n",
       "      <td>michaelco</td>\n",
       "      <td>man develops delusions wow thats a headline</td>\n",
       "      <td>t3_5zjx2</td>\n",
       "      <td>t3_5zjx2</td>\n",
       "      <td>t1_c02chep</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126317</th>\n",
       "      <td>aletoledo</td>\n",
       "      <td>i have a list of redditers i wish to have cens...</td>\n",
       "      <td>t3_5zk1h</td>\n",
       "      <td>t1_c02che2</td>\n",
       "      <td>t1_c02cher</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126318</th>\n",
       "      <td>Dark-Star</td>\n",
       "      <td>nope. too lazy and don't care for until they o...</td>\n",
       "      <td>t3_5zimk</td>\n",
       "      <td>t1_c02cebw</td>\n",
       "      <td>t1_c02ches</td>\n",
       "      <td>False</td>\n",
       "      <td>politics</td>\n",
       "      <td>120101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126319</th>\n",
       "      <td>M0b1u5</td>\n",
       "      <td>us: we are fucking idiots.</td>\n",
       "      <td>t3_5zep2</td>\n",
       "      <td>t3_5zep2</td>\n",
       "      <td>t1_c02cheu</td>\n",
       "      <td>True</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               body  \\\n",
       "126315   folderol  muscle memory.   or he never knew how to run a...   \n",
       "126316  michaelco        man develops delusions wow thats a headline   \n",
       "126317  aletoledo  i have a list of redditers i wish to have cens...   \n",
       "126318  Dark-Star  nope. too lazy and don't care for until they o...   \n",
       "126319     M0b1u5                         us: we are fucking idiots.   \n",
       "\n",
       "         link_id   parent_id        name   root   subreddit    mods  \n",
       "126315  t3_5zj73  t1_c02cfl4  t1_c02cheo  False  reddit.com  100001  \n",
       "126316  t3_5zjx2    t3_5zjx2  t1_c02chep   True  reddit.com       0  \n",
       "126317  t3_5zk1h  t1_c02che2  t1_c02cher  False  reddit.com       1  \n",
       "126318  t3_5zimk  t1_c02cebw  t1_c02ches  False    politics  120101  \n",
       "126319  t3_5zep2    t3_5zep2  t1_c02cheu   True    politics       0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyEnchant is used to check if this is a real word.\n",
    "\n",
    "* An issue with this apporach is words that are not english, but are used heavily (e.g. 'reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, 'much'),\n",
       " (True, 'smoother.'),\n",
       " (False, 'im'),\n",
       " (True, 'just'),\n",
       " (True, 'glad'),\n",
       " (False, 'reddit'),\n",
       " (True, 'is'),\n",
       " (False, 'back,'),\n",
       " (False, 'linkreddit'),\n",
       " (True, 'in'),\n",
       " (False, 'mirc'),\n",
       " (True, 'was'),\n",
       " (True, 'entertaining'),\n",
       " (True, 'but'),\n",
       " (True, 'i'),\n",
       " (True, 'had'),\n",
       " (True, 'no'),\n",
       " (True, 'idea'),\n",
       " (True, 'how'),\n",
       " (True, 'addicted'),\n",
       " (True, 'i'),\n",
       " (True, 'had'),\n",
       " (True, 'become.'),\n",
       " (True, 'thanks'),\n",
       " (True, 'for'),\n",
       " (True, 'making'),\n",
       " (True, 'the'),\n",
       " (True, 'detox'),\n",
       " (True, 'somewhat'),\n",
       " (True, 'short.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(d.check(word), word) for word in df.body[1].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'language_check'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-309cc0798104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlanguage_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'language_check'"
     ]
    }
   ],
   "source": [
    "import language_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tool = language_check.LanguageTool('en_US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches = tool.check(df.body[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tool.disabled.add(\"UPPERCASE_SENTENCE_START\")\n",
    "tool.disabled.add('I_LOWERCASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = tool.check(df.body[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove comments with more than n words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_large_comments(n, df):\n",
    "    print(\"Length before:\", df['body'].size)\n",
    "    df = df[df['body'].map(lambda s: len(s.split(' '))) < n].reset_index(drop=True)\n",
    "    show_len_update(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the tokenizer from io_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE   = re.compile(r\"\\d\")\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w for w in words if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "df['body'] = [basic_tokenizer(sentence) for sentence in df['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                    [test]\n",
       "1         [much, smoother, ., im, just, glad, reddit, is...\n",
       "2         [can, we, please, deprecate, the, word, \", aja...\n",
       "3         [oh, ,, i, see, ., fancy, schmancy, \", submitt...\n",
       "4                                              [testing, .]\n",
       "5                      [i, like, it, ., one, more, time, .]\n",
       "6         [try, refreshing, yor, cache, ,, that, worked,...\n",
       "7                    [k, ., i, lied, ., just, one, more, .]\n",
       "8         [i, also, wonder, what, the, differences, are, .]\n",
       "9                                        [so, addictive, .]\n",
       "10        [i, can, ', t, post, a, story, to, proggit, -,...\n",
       "11                              [alright, i, ', m, done, .]\n",
       "12        [is, anyone, else, ', s, \", recommended, \", pa...\n",
       "13        [ok, ,, i, guess, we, need, to, submit, commen...\n",
       "14        [i, can, ', t, submit, any, stories, --, even,...\n",
       "15                [working, fine, with, normal, adblock, .]\n",
       "16        [can, ', t, see, beta, ., reddit, ., com, from...\n",
       "17        [i, had, a, problem, as, well, ,, with, my, co...\n",
       "18                                                  [hm, ?]\n",
       "19                                        [i, think, so, .]\n",
       "20          [well, ., i, read, ron, paul, ', s, website, .]\n",
       "21                                                     [no]\n",
       "22        [cry, me, a, river, ., most, patents, and, cop...\n",
       "23                                                    [jup]\n",
       "24        [as, long, as, we, can, still, say, \", web, NU...\n",
       "25        [what, ', s, new, except, the, asynchronous, s...\n",
       "26                                                    [yep]\n",
       "27        [you, can, stop, anytime, you, want, ., really...\n",
       "28                   [hmm, only, worked, after, refresh, .]\n",
       "29        [okay, ,, dokay, ,, lets, see, how, the, comme...\n",
       "                                ...                        \n",
       "126290    [thanks, ., i, had, almost, forgotten, how, ba...\n",
       "126291    [holy, shit, ., the, second, is, a, good, list...\n",
       "126292    [oil, shale, cannot, be, used, unless, it, is,...\n",
       "126293    [i, find, it, quite, interesting, that, anyone...\n",
       "126294    [uh, oh, ,, must, ', ve, struck, a, nerve, to,...\n",
       "126295    [that, wouldn, ', t, shut, down, a, speed, tra...\n",
       "126296    [i, knew, upon, reading, this, headline, that,...\n",
       "126297    [maybe, i, ', m, going, too, far, here, ,, but...\n",
       "126298    [again, ,, what, school, of, economics, follow...\n",
       "126299    [a, worksafe, goatse, for, the, NUMBERc, :, link]\n",
       "126300    [kind, of, funny, that, the, guy, doing, the, ...\n",
       "126301    [a, bomb, threat, will, shut, one, down, faste...\n",
       "126302                           [yo, soy, el, zorro, !, !]\n",
       "126303    [it, ', s, comin, ', right, at, us, !, aaaaaaa...\n",
       "126304    [not, true, ,, at, the, right, price, ,, it, m...\n",
       "126305    [p, ., s, ., did, you, know, phelps, hates, sw...\n",
       "126306    [eNUMBER, make, linux, powered, smart, phones,...\n",
       "126307    [considering, that, NUMBER%, of, the, us, is, ...\n",
       "126308    [uh, ., i, can, ', t, believe, ,, that, on, re...\n",
       "126309    [arthur, dent, ., although, i, may, have, just...\n",
       "126310    [well, ,, i, think, taxes, should, be, regress...\n",
       "126311                                [ouch, !, my, ego, !]\n",
       "126312    [it, just, wouldn, ', t, be, funny, ., the, ol...\n",
       "126313    [no, compression, ratio, is, theoretically, im...\n",
       "126314    [pale, color, is, always, in, style, ., it, ',...\n",
       "126315    [muscle, memory, ., or, he, never, knew, how, ...\n",
       "126316    [man, develops, delusions, wow, thats, a, head...\n",
       "126317    [i, have, a, list, of, redditers, i, wish, to,...\n",
       "126318    [nope, ., too, lazy, and, don, ', t, care, for...\n",
       "126319                 [us, :, we, are, fucking, idiots, .]\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test',\n",
       " 'much',\n",
       " 'smoother',\n",
       " '.',\n",
       " 'im',\n",
       " 'just',\n",
       " 'glad',\n",
       " 'reddit',\n",
       " 'is',\n",
       " 'back',\n",
       " ',',\n",
       " 'linkreddit',\n",
       " 'in',\n",
       " 'mirc',\n",
       " 'was',\n",
       " 'entertaining',\n",
       " 'but',\n",
       " 'i',\n",
       " 'had',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'how',\n",
       " 'addicted',\n",
       " 'i',\n",
       " 'had',\n",
       " 'become',\n",
       " '.',\n",
       " 'thanks',\n",
       " 'for',\n",
       " 'making',\n",
       " 'the',\n",
       " 'detox',\n",
       " 'somewhat',\n",
       " 'short',\n",
       " '.',\n",
       " 'can',\n",
       " 'we',\n",
       " 'please',\n",
       " 'deprecate',\n",
       " 'the',\n",
       " 'word',\n",
       " '\"',\n",
       " 'ajax',\n",
       " '\"',\n",
       " 'now',\n",
       " '?',\n",
       " '(',\n",
       " 'but',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'this',\n",
       " '_is_',\n",
       " 'much',\n",
       " 'nicer',\n",
       " ')',\n",
       " 'oh',\n",
       " ',',\n",
       " 'i',\n",
       " 'see',\n",
       " '.',\n",
       " 'fancy',\n",
       " 'schmancy',\n",
       " '\"',\n",
       " 'submitting',\n",
       " '.',\n",
       " '.',\n",
       " '\"',\n",
       " 'testing',\n",
       " '.',\n",
       " 'i',\n",
       " 'like',\n",
       " 'it',\n",
       " '.',\n",
       " 'one',\n",
       " 'more',\n",
       " 'time',\n",
       " '.',\n",
       " 'try',\n",
       " 'refreshing',\n",
       " 'yor',\n",
       " 'cache',\n",
       " ',',\n",
       " 'that',\n",
       " 'worked',\n",
       " 'for',\n",
       " 'me',\n",
       " 'edit',\n",
       " ':',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'edit',\n",
       " 'k',\n",
       " '.',\n",
       " 'i',\n",
       " 'lied',\n",
       " '.',\n",
       " 'just',\n",
       " 'one',\n",
       " 'more',\n",
       " '.',\n",
       " 'i',\n",
       " 'also',\n",
       " 'wonder',\n",
       " 'what',\n",
       " 'the',\n",
       " 'differences',\n",
       " 'are',\n",
       " '.',\n",
       " 'so',\n",
       " 'addictive',\n",
       " '.',\n",
       " 'i',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'post',\n",
       " 'a',\n",
       " 'story',\n",
       " 'to',\n",
       " 'proggit',\n",
       " '-',\n",
       " 'i',\n",
       " 'got',\n",
       " '\"',\n",
       " 'you',\n",
       " 'are',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'submit',\n",
       " 'too',\n",
       " 'fast',\n",
       " '\"',\n",
       " 'on',\n",
       " 'my',\n",
       " 'first',\n",
       " 'submission',\n",
       " '.',\n",
       " 'alright',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'done',\n",
       " '.',\n",
       " 'is',\n",
       " 'anyone',\n",
       " 'else',\n",
       " \"'\",\n",
       " 's',\n",
       " '\"',\n",
       " 'recommended',\n",
       " '\"',\n",
       " 'page',\n",
       " 'completely',\n",
       " 'empty',\n",
       " '?',\n",
       " 'mine',\n",
       " 'is',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'jam-packed',\n",
       " '.',\n",
       " 'ok',\n",
       " ',',\n",
       " 'i',\n",
       " 'guess',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'submit',\n",
       " 'comments',\n",
       " 'to',\n",
       " 'test',\n",
       " '?',\n",
       " 'i',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'submit',\n",
       " 'any',\n",
       " 'stories',\n",
       " '--',\n",
       " 'even',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " ',',\n",
       " 'i',\n",
       " 'get',\n",
       " ',',\n",
       " '\"',\n",
       " 'you',\n",
       " 'are',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'submit',\n",
       " 'too',\n",
       " 'fast',\n",
       " '\"',\n",
       " '.',\n",
       " '.',\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'seeing',\n",
       " 'this',\n",
       " '?',\n",
       " 'edit',\n",
       " ':',\n",
       " 'this',\n",
       " 'appears',\n",
       " 'to',\n",
       " 'be',\n",
       " 'fixed',\n",
       " '.',\n",
       " 'working',\n",
       " 'fine',\n",
       " 'with',\n",
       " 'normal',\n",
       " 'adblock',\n",
       " '.',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'see',\n",
       " 'beta',\n",
       " '.',\n",
       " 'reddit',\n",
       " '.',\n",
       " 'com',\n",
       " 'from',\n",
       " 'here',\n",
       " '.',\n",
       " 'i',\n",
       " 'had',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'as',\n",
       " 'well',\n",
       " ',',\n",
       " 'with',\n",
       " 'my',\n",
       " 'comment',\n",
       " 'being',\n",
       " 'lost',\n",
       " '.',\n",
       " 'then',\n",
       " 'i',\n",
       " 'did',\n",
       " 'a',\n",
       " '\"',\n",
       " 'super-refresh',\n",
       " '\"',\n",
       " '(',\n",
       " 'shift-reload',\n",
       " ')',\n",
       " 'and',\n",
       " 'it',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'work',\n",
       " '.',\n",
       " 'hm',\n",
       " '?',\n",
       " 'i',\n",
       " 'think',\n",
       " 'so',\n",
       " '.',\n",
       " 'well',\n",
       " '.',\n",
       " 'i',\n",
       " 'read',\n",
       " 'ron',\n",
       " 'paul',\n",
       " \"'\",\n",
       " 's',\n",
       " 'website',\n",
       " '.',\n",
       " 'no',\n",
       " 'cry',\n",
       " 'me',\n",
       " 'a',\n",
       " 'river',\n",
       " '.',\n",
       " 'most',\n",
       " 'patents',\n",
       " 'and',\n",
       " 'copyrights',\n",
       " 'these',\n",
       " 'days',\n",
       " 'are',\n",
       " 'theft',\n",
       " 'of',\n",
       " 'public',\n",
       " 'domain',\n",
       " 'under',\n",
       " 'the',\n",
       " 'color',\n",
       " 'of',\n",
       " 'law',\n",
       " ',',\n",
       " 'to',\n",
       " 'an',\n",
       " 'absurd',\n",
       " 'degree',\n",
       " '.',\n",
       " 'mickey',\n",
       " 'mouse',\n",
       " 'copyright',\n",
       " 'extended',\n",
       " 'to',\n",
       " 'NUMBER',\n",
       " 'years',\n",
       " 'as',\n",
       " 'a',\n",
       " 'favor',\n",
       " 'to',\n",
       " 'everyone',\n",
       " \"'\",\n",
       " 's',\n",
       " 'favorite',\n",
       " 'political',\n",
       " 'donor',\n",
       " ',',\n",
       " 'will',\n",
       " 'eisner',\n",
       " '.',\n",
       " 'amazon',\n",
       " \"'\",\n",
       " 's',\n",
       " 'one',\n",
       " 'click',\n",
       " 'patent',\n",
       " '.',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'probably',\n",
       " 'been',\n",
       " 'NUMBER',\n",
       " 'years',\n",
       " 'since',\n",
       " 'the',\n",
       " 'constitutional',\n",
       " 'authority',\n",
       " 'to',\n",
       " 'grant',\n",
       " 'patents',\n",
       " 'and',\n",
       " 'copyrights',\n",
       " 'actually',\n",
       " 'served',\n",
       " ',',\n",
       " 'on',\n",
       " 'the',\n",
       " 'whole',\n",
       " ',',\n",
       " 'to',\n",
       " '\"',\n",
       " 'advance',\n",
       " 'the',\n",
       " 'arts',\n",
       " 'and',\n",
       " 'commerce',\n",
       " '\"',\n",
       " ',',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'to',\n",
       " 'stifle',\n",
       " 'them',\n",
       " '.',\n",
       " 'jup',\n",
       " 'as',\n",
       " 'long',\n",
       " 'as',\n",
       " 'we',\n",
       " 'can',\n",
       " 'still',\n",
       " 'say',\n",
       " '\"',\n",
       " 'web',\n",
       " 'NUMBER',\n",
       " '.',\n",
       " 'NUMBER',\n",
       " '\"',\n",
       " '.',\n",
       " 'what',\n",
       " \"'\",\n",
       " 's',\n",
       " 'new',\n",
       " 'except',\n",
       " 'the',\n",
       " 'asynchronous',\n",
       " 'submit',\n",
       " '(',\n",
       " 'which',\n",
       " 'alone',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'really',\n",
       " 'worth',\n",
       " 'being',\n",
       " 'dubbed',\n",
       " \"'\",\n",
       " 'new',\n",
       " 'comment',\n",
       " 'system',\n",
       " \"'\",\n",
       " ')',\n",
       " '?',\n",
       " 'yep',\n",
       " 'you',\n",
       " 'can',\n",
       " 'stop',\n",
       " 'anytime',\n",
       " 'you',\n",
       " 'want',\n",
       " '.',\n",
       " 'really',\n",
       " '.',\n",
       " 'hmm',\n",
       " 'only',\n",
       " 'worked',\n",
       " 'after',\n",
       " 'refresh',\n",
       " '.',\n",
       " 'okay',\n",
       " ',',\n",
       " 'dokay',\n",
       " ',',\n",
       " 'lets',\n",
       " 'see',\n",
       " 'how',\n",
       " 'the',\n",
       " 'comment',\n",
       " 'work',\n",
       " 'now',\n",
       " '.',\n",
       " 'ping',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'testing',\n",
       " ',',\n",
       " 'NUMBER',\n",
       " ',',\n",
       " 'NUMBER',\n",
       " ',',\n",
       " 'NUMBER',\n",
       " ',',\n",
       " 'testing',\n",
       " 'comments',\n",
       " 'posted',\n",
       " 'by',\n",
       " 'friends',\n",
       " 'are',\n",
       " 'colored',\n",
       " 'orange',\n",
       " '.',\n",
       " 'woot',\n",
       " '!',\n",
       " 'nice',\n",
       " '!',\n",
       " 'the',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'jumped',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " '.',\n",
       " 'uh',\n",
       " 'oh',\n",
       " '.',\n",
       " 'what',\n",
       " 'is',\n",
       " 'that',\n",
       " 'about',\n",
       " '?',\n",
       " '(',\n",
       " 'i',\n",
       " 'like',\n",
       " 'your',\n",
       " 'user',\n",
       " 'name',\n",
       " ',',\n",
       " 'mm',\n",
       " 'latin',\n",
       " ')',\n",
       " 'uhm',\n",
       " ',',\n",
       " 'a',\n",
       " 'NUMBER',\n",
       " 'yo',\n",
       " 'girl',\n",
       " 'was',\n",
       " 'sat',\n",
       " 'upon',\n",
       " 'to',\n",
       " 'death',\n",
       " 'by',\n",
       " 'a',\n",
       " 'daycare',\n",
       " 'facility',\n",
       " 'empoloyee',\n",
       " 'and',\n",
       " 'was',\n",
       " 'sentenced',\n",
       " 'to',\n",
       " 'NUMBER',\n",
       " 'days',\n",
       " 'of',\n",
       " 'prison',\n",
       " 'and',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'thousand',\n",
       " 'bucks',\n",
       " '.',\n",
       " 'so',\n",
       " ',',\n",
       " '.',\n",
       " '.',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'dont',\n",
       " 'compare',\n",
       " 'your',\n",
       " 'woes',\n",
       " '.',\n",
       " 'let',\n",
       " 'me',\n",
       " 'see',\n",
       " 'edit',\n",
       " ':',\n",
       " 'oooh',\n",
       " 'thats',\n",
       " 'lovely',\n",
       " ',',\n",
       " 'much',\n",
       " 'better',\n",
       " 'than',\n",
       " 'before',\n",
       " '!',\n",
       " 'i',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'see',\n",
       " 'the',\n",
       " 'difference',\n",
       " 'yet',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'imagine',\n",
       " 'that',\n",
       " 'i',\n",
       " 'will',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'i',\n",
       " 'post',\n",
       " 'this',\n",
       " '.',\n",
       " 'this',\n",
       " 'new',\n",
       " 'comment',\n",
       " 'system',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'do',\n",
       " 'wonders',\n",
       " 'for',\n",
       " 'the',\n",
       " 'fibonacci',\n",
       " 'sequence',\n",
       " 'thread',\n",
       " '(',\n",
       " 'link',\n",
       " 'NUMBER',\n",
       " 'NUMBER',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'testing',\n",
       " '^_^',\n",
       " 'no',\n",
       " ',',\n",
       " 'let',\n",
       " 'me',\n",
       " 'try',\n",
       " 'edit',\n",
       " ',',\n",
       " 'nice',\n",
       " '!',\n",
       " 'NUMBER',\n",
       " 'let',\n",
       " 'me',\n",
       " 'try',\n",
       " 'this',\n",
       " '.',\n",
       " 'i',\n",
       " 'likes',\n",
       " ',',\n",
       " 'i',\n",
       " 'likes',\n",
       " ',',\n",
       " 'i',\n",
       " 'likes',\n",
       " 'juicy',\n",
       " 'ajaxiliscious',\n",
       " 'goodness',\n",
       " '.',\n",
       " 'now',\n",
       " 'if',\n",
       " 'could',\n",
       " 'only',\n",
       " 'find',\n",
       " 'my',\n",
       " 'precious',\n",
       " '.',\n",
       " 'now',\n",
       " 'what',\n",
       " 'has',\n",
       " 'it',\n",
       " 'got',\n",
       " 'in',\n",
       " 'its',\n",
       " 'pocketses',\n",
       " '?',\n",
       " 'is',\n",
       " 'anything',\n",
       " 'else',\n",
       " 'different',\n",
       " '?',\n",
       " 'surely',\n",
       " 'there',\n",
       " \"'\",\n",
       " 's',\n",
       " 'more',\n",
       " 'to',\n",
       " '\"',\n",
       " 'the',\n",
       " 'new',\n",
       " 'reddit',\n",
       " '\"',\n",
       " 'than',\n",
       " 'this',\n",
       " '.',\n",
       " 'edit',\n",
       " ':',\n",
       " 'i',\n",
       " 'guess',\n",
       " 'that',\n",
       " 'sounded',\n",
       " 'kinda',\n",
       " 'snotty',\n",
       " '.',\n",
       " 'i',\n",
       " 'really',\n",
       " 'like',\n",
       " 'the',\n",
       " 'new',\n",
       " 'submitting-in-place',\n",
       " '(',\n",
       " 'it',\n",
       " 'was',\n",
       " 'my',\n",
       " 'linkNUMBER',\n",
       " 'desired',\n",
       " 'fix/upgrade',\n",
       " ')',\n",
       " '.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " '.',\n",
       " 'hey',\n",
       " 'i',\n",
       " 'want',\n",
       " 'in',\n",
       " 'on',\n",
       " 'the',\n",
       " 'comment',\n",
       " 'testing',\n",
       " 'fun',\n",
       " '.',\n",
       " 'edit',\n",
       " ':',\n",
       " 'oooh',\n",
       " 'slick',\n",
       " '.',\n",
       " 'NUMBER',\n",
       " 'you',\n",
       " 'think',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'problem',\n",
       " '?',\n",
       " 'i',\n",
       " 'wasn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'even',\n",
       " 'able',\n",
       " 'to',\n",
       " 'log',\n",
       " 'in',\n",
       " 'when',\n",
       " 'i',\n",
       " 'was',\n",
       " 'using',\n",
       " 'firefox',\n",
       " '.',\n",
       " 'i',\n",
       " 'had',\n",
       " 'to',\n",
       " 'switch',\n",
       " 'to',\n",
       " 'opera',\n",
       " 'to',\n",
       " 'log',\n",
       " 'in',\n",
       " '.',\n",
       " 'now',\n",
       " 'to',\n",
       " 'test',\n",
       " 'that',\n",
       " 'new',\n",
       " 'submit',\n",
       " '.',\n",
       " 'i',\n",
       " 'think',\n",
       " 'the',\n",
       " '\"',\n",
       " 'parent',\n",
       " '\"',\n",
       " 'link',\n",
       " 'is',\n",
       " 'broken',\n",
       " '.',\n",
       " 'whats',\n",
       " 'new',\n",
       " '?',\n",
       " 'same',\n",
       " 'here',\n",
       " '.',\n",
       " 'certainly',\n",
       " 'a',\n",
       " 'bug',\n",
       " ',',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'take',\n",
       " 'a',\n",
       " 'screenshot',\n",
       " ',',\n",
       " 'one',\n",
       " 'sec',\n",
       " '.',\n",
       " 'NUMBER',\n",
       " 'mass',\n",
       " 'torture',\n",
       " 'at',\n",
       " 'guantanamo',\n",
       " '?',\n",
       " 'i',\n",
       " 'would',\n",
       " 'love',\n",
       " 'to',\n",
       " 'see',\n",
       " 'a',\n",
       " 'source',\n",
       " 'on',\n",
       " 'that',\n",
       " 'one',\n",
       " '.',\n",
       " 'the',\n",
       " 'only',\n",
       " 'i',\n",
       " 'have',\n",
       " 'heard',\n",
       " 'there',\n",
       " 'is',\n",
       " 'sleep',\n",
       " 'deprivation',\n",
       " 'and',\n",
       " 'loud',\n",
       " 'music',\n",
       " '.',\n",
       " 'abu',\n",
       " 'gharib',\n",
       " '?',\n",
       " 'that',\n",
       " 'was',\n",
       " 'an',\n",
       " 'isolated',\n",
       " 'incident',\n",
       " 'and',\n",
       " 'only',\n",
       " 'the',\n",
       " '\"',\n",
       " 'bush',\n",
       " 'is',\n",
       " 'hitler',\n",
       " '\"',\n",
       " 'nutjobs',\n",
       " 'think',\n",
       " 'the',\n",
       " 'administration',\n",
       " 'had',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'hand',\n",
       " 'in',\n",
       " 'it',\n",
       " '.',\n",
       " 'renditioning',\n",
       " '?',\n",
       " 'i',\n",
       " 'hate',\n",
       " 'to',\n",
       " 'break',\n",
       " 'it',\n",
       " 'to',\n",
       " 'you',\n",
       " 'but',\n",
       " 'enemy',\n",
       " 'forces',\n",
       " 'fighting',\n",
       " 'on',\n",
       " 'a',\n",
       " 'field',\n",
       " 'of',\n",
       " 'battle',\n",
       " 'linkout',\n",
       " 'of',\n",
       " 'uniformlink',\n",
       " 'have',\n",
       " 'no',\n",
       " 'rights',\n",
       " 'under',\n",
       " 'geneva',\n",
       " '.',\n",
       " 'none',\n",
       " ',',\n",
       " 'zip',\n",
       " ',',\n",
       " 'nada',\n",
       " '.',\n",
       " 'call',\n",
       " 'it',\n",
       " 'wrong',\n",
       " 'all',\n",
       " 'you',\n",
       " 'like',\n",
       " ',',\n",
       " 'but',\n",
       " 'illegal',\n",
       " 'its',\n",
       " 'not',\n",
       " '.',\n",
       " 'fisa',\n",
       " '?',\n",
       " 'this',\n",
       " 'has',\n",
       " 'been',\n",
       " 'done',\n",
       " 'in',\n",
       " 'drug',\n",
       " 'enforcement',\n",
       " 'long',\n",
       " 'before',\n",
       " 'wot',\n",
       " '.',\n",
       " 'and',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'spying',\n",
       " 'on',\n",
       " \"'\",\n",
       " 'millions',\n",
       " \"'\",\n",
       " 'of',\n",
       " 'citizens',\n",
       " 'either',\n",
       " '.',\n",
       " 'the',\n",
       " 'roving',\n",
       " 'wiretaps',\n",
       " 'are',\n",
       " 'on',\n",
       " 'linkindividualslink',\n",
       " 'which',\n",
       " 'we',\n",
       " 'are',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'monitor',\n",
       " ',',\n",
       " 'therefore',\n",
       " 'law',\n",
       " 'enforcement',\n",
       " 'should',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'track',\n",
       " 'them',\n",
       " 'on',\n",
       " 'new',\n",
       " 'phones',\n",
       " '.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'think',\n",
       " 'terrorists',\n",
       " 'under',\n",
       " 'surveillance',\n",
       " 'should',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'just',\n",
       " 'grab',\n",
       " 'a',\n",
       " 'new',\n",
       " 'cell',\n",
       " 'phone',\n",
       " 'at',\n",
       " 'wal-mart',\n",
       " 'and',\n",
       " 'start',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'judicial',\n",
       " 'review',\n",
       " 'process',\n",
       " 'all',\n",
       " 'over',\n",
       " 'again',\n",
       " 'you',\n",
       " 'are',\n",
       " 'crazy',\n",
       " '.',\n",
       " 'ohhhh',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'that',\n",
       " \"'\",\n",
       " 's',\n",
       " 'nice',\n",
       " '.',\n",
       " 'let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'do',\n",
       " 'it',\n",
       " 'again',\n",
       " '.',\n",
       " 'just',\n",
       " 'another',\n",
       " 'comment',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'd',\n",
       " 'rather',\n",
       " 'have',\n",
       " 'a',\n",
       " 'dead',\n",
       " 'hotdog',\n",
       " 'than',\n",
       " 'a',\n",
       " 'live',\n",
       " 'one',\n",
       " '.',\n",
       " 'NUMBER',\n",
       " 'nope',\n",
       " '.',\n",
       " 'months',\n",
       " 'of',\n",
       " 'development',\n",
       " ',',\n",
       " 'a',\n",
       " 'day',\n",
       " 'of',\n",
       " 'downtime',\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= [word for sentence in df['body'].values for word in sentence]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#source : http://stackoverflow.com/questions/33093809/count-the-frequency-of-elements-in-list-of-lists-in-python/33093930\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "a= [word for sentence in df['body'].values for word in sentence]\n",
    "word_freq = Counter(chain(a))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5898318"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([value for key, value in word_freq.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of 'valid' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5020458"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([value for key, value in word_freq.items() if d.check(key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now there are 126320 rows.\n"
     ]
    }
   ],
   "source": [
    "show_len_update(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to know how many sentences we have if we remove all senteces with invalid words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def invalid_word(df):\n",
    "    '''Goes through the content and determines whether an invalid word is \n",
    "    present.\n",
    "    \n",
    "    The data frame should provide a body field which will be inspected.\n",
    "    '''\n",
    "    d = enchant.Dict(\"en_US\") \n",
    "    valid_sentences = [True] * len(df)\n",
    "    misspelled_words = {}\n",
    "     \n",
    "    for idx, sentence in enumerate(df['body'].values):\n",
    "        for word in sentence:\n",
    "            if not d.check(word):\n",
    "                if word in misspelled_words:\n",
    "                    misspelled_words[word] += 1\n",
    "                else:\n",
    "                    misspelled_words[word] = 1\n",
    "                valid_sentences[idx] = False\n",
    "    print(\"There are %i valid sentences out of %i.\" % (sum(valid_sentences), len(valid_sentences)))\n",
    "    print(\"There are %i misspelled words.\" % len(misspelled_words))\n",
    "    return valid_sentences, misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11249 valid sentences out of 126320.\n",
      "There are 63912 misspelled words.\n"
     ]
    }
   ],
   "source": [
    "valid_sent, misspelled = invalid_word(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of valid sentences using basic english dictionary:\n",
      "11249\n"
     ]
    }
   ],
   "source": [
    "print('Total number of valid sentences using basic english dictionary:')\n",
    "print(sum(valid_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['spell'] = valid_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use this list to append some words to our dictionary.\n",
    "sorted_mispelled_words = sorted(misspelled.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We write to a text-file the 1000 most common misspelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MISSPELLED_WORDS = '/Users/ivan/Desktop/mywords.txt'\n",
    "f = open(MISSPELLED_WORDS, 'w')\n",
    "for word in sorted_mispelled_words[:1000]:\n",
    "    f.write(word[0] + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 207292),\n",
       " (\"'\", 146616),\n",
       " ('\"', 59729),\n",
       " ('?', 45454),\n",
       " ('(', 31024),\n",
       " (')', 28141),\n",
       " (':', 22230),\n",
       " ('!', 22036),\n",
       " ('-', 9009),\n",
       " (';', 6892),\n",
       " ('ve', 6387),\n",
       " ('doesn', 6003),\n",
       " ('isn', 4228),\n",
       " ('didn', 4103),\n",
       " ('reddit', 4077),\n",
       " ('paul', 3858),\n",
       " ('NUMBER%', 2897),\n",
       " ('ron', 2669),\n",
       " ('$NUMBER', 2139),\n",
       " ('wouldn', 2132),\n",
       " ('american', 1976),\n",
       " ('--', 1975),\n",
       " ('aren', 1961),\n",
       " ('america', 1762),\n",
       " ('iran', 1735),\n",
       " ('=', 1652),\n",
       " ('wasn', 1614),\n",
       " ('iraq', 1536),\n",
       " ('internet', 1409),\n",
       " ('ok', 1269),\n",
       " ('NUMBER/NUMBER', 1217),\n",
       " ('google', 1090),\n",
       " ('americans', 1080),\n",
       " ('israel', 1063),\n",
       " ('java', 1028),\n",
       " ('linux', 962),\n",
       " ('shouldn', 909),\n",
       " ('couldn', 850),\n",
       " ('english', 833),\n",
       " ('&amp', 822),\n",
       " ('c++', 819),\n",
       " ('NUMBER-NUMBER', 789),\n",
       " ('online', 770),\n",
       " ('clinton', 707),\n",
       " ('kucinich', 700),\n",
       " ('christian', 688),\n",
       " ('jews', 678),\n",
       " ('NUMBERs', 673),\n",
       " ('tv', 667),\n",
       " ('usa', 650),\n",
       " ('hillary', 634),\n",
       " ('canada', 630),\n",
       " ('dont', 604),\n",
       " ('spam', 604),\n",
       " ('NUMBERth', 602),\n",
       " ('lol', 574),\n",
       " ('europe', 570),\n",
       " ('/', 569),\n",
       " ('wikipedia', 563),\n",
       " ('downmodded', 549),\n",
       " ('jesus', 531),\n",
       " ('blog', 527),\n",
       " ('al', 521),\n",
       " ('weren', 519),\n",
       " ('haskell', 516),\n",
       " ('microsoft', 506),\n",
       " ('+', 505),\n",
       " ('os', 495),\n",
       " ('russia', 488),\n",
       " ('hasn', 470),\n",
       " ('|', 459),\n",
       " ('california', 458),\n",
       " ('im', 454),\n",
       " ('christians', 448),\n",
       " ('cheney', 444),\n",
       " ('obama', 439),\n",
       " ('chinese', 427),\n",
       " ('linklink', 426),\n",
       " ('wtf', 423),\n",
       " ('mr', 409),\n",
       " ('linkNUMBER', 385),\n",
       " ('btw', 383),\n",
       " ('africa', 381),\n",
       " ('ubuntu', 380),\n",
       " ('french', 380),\n",
       " ('jewish', 371),\n",
       " ('uk', 370),\n",
       " ('digg', 369),\n",
       " ('firefox', 366),\n",
       " ('iq', 365),\n",
       " ('george', 354),\n",
       " ('subreddit', 348),\n",
       " ('india', 347),\n",
       " ('apps', 336),\n",
       " ('hitler', 328),\n",
       " ('”', 320),\n",
       " ('pc', 318),\n",
       " ('url', 313),\n",
       " ('redditors', 312),\n",
       " ('NUMBERk', 308),\n",
       " ('british', 306),\n",
       " ('germany', 305),\n",
       " ('giuliani', 305),\n",
       " ('san', 304),\n",
       " ('hmm', 303),\n",
       " ('canadian', 299),\n",
       " ('un', 296),\n",
       " ('dr', 296),\n",
       " ('muslims', 293),\n",
       " ('}', 281),\n",
       " ('cia', 280),\n",
       " ('rp', 277),\n",
       " ('perl', 273),\n",
       " ('heh', 269),\n",
       " ('ie', 268),\n",
       " ('healthcare', 265),\n",
       " ('upmodded', 260),\n",
       " ('japanese', 259),\n",
       " ('france', 258),\n",
       " ('muslim', 257),\n",
       " ('and/or', 257),\n",
       " ('youtube', 255),\n",
       " ('iaea', 253),\n",
       " ('york', 251),\n",
       " ('{', 250),\n",
       " ('iraqi', 250),\n",
       " ('colbert', 247),\n",
       " ('washington', 245),\n",
       " ('european', 245),\n",
       " ('christianity', 243),\n",
       " ('downmod', 241),\n",
       " ('app', 241),\n",
       " ('linknotlink', 241),\n",
       " ('modded', 235),\n",
       " ('comcast', 235),\n",
       " ('watson', 234),\n",
       " ('haha', 233),\n",
       " ('iphone', 233),\n",
       " ('ain', 231),\n",
       " ('html', 227),\n",
       " ('$NUMBERk', 225),\n",
       " ('`', 223),\n",
       " ('christ', 220),\n",
       " ('edwards', 219),\n",
       " ('african', 217),\n",
       " ('german', 209),\n",
       " ('sarkozy', 209),\n",
       " ('bs', 206),\n",
       " ('hadn', 206),\n",
       " ('pakistan', 206),\n",
       " ('pics', 205),\n",
       " ('javascript', 202),\n",
       " ('xp', 201),\n",
       " ('linkislink', 198),\n",
       " ('NUMBER+', 197),\n",
       " ('nazi', 195),\n",
       " ('texas', 194),\n",
       " ('sheeple', 194),\n",
       " ('php', 194),\n",
       " ('blackwater', 194),\n",
       " ('it’s', 193),\n",
       " ('omg', 192),\n",
       " ('xtian', 192),\n",
       " ('russian', 192),\n",
       " ('israeli', 192),\n",
       " ('jew', 188),\n",
       " ('palestinians', 188),\n",
       " ('gmail', 187),\n",
       " ('upmod', 185),\n",
       " ('NUMBERrd', 182),\n",
       " ('gop', 181),\n",
       " ('ai', 181),\n",
       " ('iraqis', 179),\n",
       " ('cosby', 178),\n",
       " ('halloween', 177),\n",
       " ('xml', 176),\n",
       " ('—', 176),\n",
       " ('dems', 175),\n",
       " ('imo', 174),\n",
       " ('radiohead', 174),\n",
       " ('dodd', 173),\n",
       " ('rudy', 171),\n",
       " ('mexico', 170),\n",
       " ('australia', 168),\n",
       " ('-NUMBER', 166),\n",
       " ('wiki', 166),\n",
       " ('dvorak', 166),\n",
       " ('florida', 165),\n",
       " ('dna', 165),\n",
       " ('doesnt', 165),\n",
       " ('putin', 165),\n",
       " ('turing', 164),\n",
       " ('nah', 163),\n",
       " ('ip', 162),\n",
       " ('saudi', 161),\n",
       " ('NUMBERnd', 154),\n",
       " ('joe', 153),\n",
       " ('reagan', 153),\n",
       " ('downvoted', 153),\n",
       " ('james', 153),\n",
       " ('david', 152),\n",
       " ('org', 151),\n",
       " ('islam', 151),\n",
       " ('dumbledore', 151),\n",
       " ('don’t', 150),\n",
       " ('NUMBERst', 150),\n",
       " ('indian', 150),\n",
       " ('cd', 149),\n",
       " ('gui', 149),\n",
       " ('fbi', 149),\n",
       " ('iranian', 149),\n",
       " ('vietnam', 148),\n",
       " ('dvd', 147),\n",
       " ('alot', 147),\n",
       " ('romney', 146),\n",
       " ('diego', 146),\n",
       " ('oo', 145),\n",
       " ('chomsky', 145),\n",
       " ('arabs', 145),\n",
       " ('_', 144),\n",
       " ('hmmm', 144),\n",
       " ('er', 144),\n",
       " ('sweden', 144),\n",
       " ('usenet', 144),\n",
       " ('jones', 143),\n",
       " ('de', 140),\n",
       " ('nazis', 140),\n",
       " ('NUMBERgb', 140),\n",
       " ('pdf', 140),\n",
       " ('kerry', 139),\n",
       " ('afghanistan', 138),\n",
       " ('riaa', 137),\n",
       " ('flink', 137),\n",
       " ('neocons', 136),\n",
       " ('saddam', 136),\n",
       " ('london', 136),\n",
       " ('nsfw', 136),\n",
       " ('pic', 135),\n",
       " ('didnt', 135),\n",
       " ('christmas', 134),\n",
       " ('michael', 134),\n",
       " ('asian', 134),\n",
       " ('linkthe', 133),\n",
       " ('cnn', 132),\n",
       " ('tlink', 132),\n",
       " ('realise', 131),\n",
       " ('pretards', 131),\n",
       " ('msm', 131),\n",
       " ('october', 130),\n",
       " ('right-wing', 130),\n",
       " ('osx', 129),\n",
       " ('$', 129),\n",
       " ('unix', 128),\n",
       " ('nobel', 128),\n",
       " ('ipod', 126),\n",
       " ('asians', 126),\n",
       " ('ocaml', 126),\n",
       " ('goddamned', 126),\n",
       " ('–', 125),\n",
       " ('xkcd', 124),\n",
       " ('css', 123),\n",
       " ('NUMBERm', 122),\n",
       " ('qaeda', 122),\n",
       " ('dumbass', 121),\n",
       " ('november', 119),\n",
       " ('urls', 118),\n",
       " ('islamic', 118),\n",
       " ('britain', 117),\n",
       " ('downmodding', 117),\n",
       " ('arab', 117),\n",
       " ('coulter', 117),\n",
       " ('cuba', 117),\n",
       " ('syria', 117),\n",
       " ('NUMBERx', 116),\n",
       " ('korea', 116),\n",
       " ('imap', 116),\n",
       " ('ps', 115),\n",
       " ('behaviour', 115),\n",
       " ('neocon', 115),\n",
       " ('facebook', 114),\n",
       " ('gaza', 114),\n",
       " ('steve', 113),\n",
       " ('downvote', 112),\n",
       " ('http', 112),\n",
       " ('imho', 112),\n",
       " ('+NUMBER', 111),\n",
       " ('==', 111),\n",
       " ('richard', 110),\n",
       " ('wwii', 110),\n",
       " ('africans', 110),\n",
       " ('nixon', 109),\n",
       " ('ny', 108),\n",
       " ('anti-war', 108),\n",
       " ('xNUMBER', 108),\n",
       " ('arabia', 108),\n",
       " ('linkyoulink', 106),\n",
       " ('so-called', 105),\n",
       " ('palestinian', 105),\n",
       " ('dennis', 105),\n",
       " ('umm', 105),\n",
       " ('fyi', 105),\n",
       " ('mmm', 105),\n",
       " ('NUMBER-year-old', 104),\n",
       " ('e-mail', 103),\n",
       " ('api', 103),\n",
       " ('---', 103),\n",
       " ('isnt', 102),\n",
       " ('godwin', 102),\n",
       " ('myspace', 101),\n",
       " ('dawkins', 101),\n",
       " ('psNUMBER', 101),\n",
       " ('photoshop', 101),\n",
       " ('bbc', 100),\n",
       " ('at&amp', 100),\n",
       " ('wwNUMBER', 100),\n",
       " ('medicare', 100),\n",
       " ('santa', 99),\n",
       " ('irs', 99),\n",
       " ('europeans', 99),\n",
       " ('linkreallylink', 98),\n",
       " ('cpu', 98),\n",
       " ('long-term', 98),\n",
       " ('larry', 98),\n",
       " ('latin', 97),\n",
       " ('upvote', 97),\n",
       " ('mpNUMBER', 97),\n",
       " ('sql', 97),\n",
       " ('redditor', 97),\n",
       " ('macbook', 97),\n",
       " ('admin', 96),\n",
       " ('palestine', 96),\n",
       " ('rumsfeld', 96),\n",
       " ('fred', 95),\n",
       " ('thompson', 95),\n",
       " ('amish', 95),\n",
       " ('npt', 95),\n",
       " ('NUMBER-NUMBER%', 95),\n",
       " ('asia', 94),\n",
       " ('erlang', 94),\n",
       " ('indians', 94),\n",
       " ('wtc', 94),\n",
       " ('pretard', 93),\n",
       " ('hollywood', 93),\n",
       " ('pNUMBERp', 93),\n",
       " ('egypt', 93),\n",
       " ('sunday', 92),\n",
       " ('truthers', 92),\n",
       " ('blogs', 92),\n",
       " ('%', 92),\n",
       " ('spanish', 92),\n",
       " ('dc', 91),\n",
       " ('reilly', 91),\n",
       " ('robert', 91),\n",
       " ('schip', 91),\n",
       " ('NUMBERmb', 91),\n",
       " ('pelosi', 90),\n",
       " ('canadians', 90),\n",
       " ('bittorrent', 90),\n",
       " ('phd', 90),\n",
       " ('ann', 90),\n",
       " ('meth', 90),\n",
       " ('username', 89),\n",
       " ('nader', 89),\n",
       " ('ui', 89),\n",
       " ('NUMBERd', 89),\n",
       " ('upvoted', 89),\n",
       " ('subreddits', 88),\n",
       " ('linkarelink', 88),\n",
       " ('isp', 88),\n",
       " ('iirc', 88),\n",
       " ('rome', 87),\n",
       " ('linkthatlink', 87),\n",
       " ('yay', 87),\n",
       " ('greek', 87),\n",
       " ('linkandlink', 87),\n",
       " ('rss', 86),\n",
       " ('israelis', 86),\n",
       " ('dilbert', 86),\n",
       " ('nasa', 85),\n",
       " ('jim', 85),\n",
       " ('se', 85),\n",
       " ('npr', 85),\n",
       " ('mon', 84),\n",
       " ('da', 83),\n",
       " ('stephen', 83),\n",
       " ('tibet', 82),\n",
       " ('internets', 81),\n",
       " ('francisco', 81),\n",
       " ('hiv', 81),\n",
       " ('ftw', 80),\n",
       " ('hominem', 80),\n",
       " ('gdp', 80),\n",
       " ('stalin', 80),\n",
       " ('rowling', 80),\n",
       " ('boston', 79),\n",
       " ('walmart', 79),\n",
       " ('nyt', 79),\n",
       " ('vp', 79),\n",
       " ('eu', 79),\n",
       " ('orleans', 79),\n",
       " ('chicago', 79),\n",
       " ('nyc', 79),\n",
       " ('nethack', 79),\n",
       " ('teh', 78),\n",
       " ('foo', 78),\n",
       " ('cuz', 77),\n",
       " ('ahh', 77),\n",
       " ('intel', 77),\n",
       " ('distro', 77),\n",
       " ('al-qaeda', 76),\n",
       " ('meme', 76),\n",
       " ('jackson', 76),\n",
       " ('/sarcasm', 76),\n",
       " ('oct', 76),\n",
       " ('paris', 76),\n",
       " ('drm', 75),\n",
       " ('limbaugh', 75),\n",
       " ('jefferson', 75),\n",
       " ('eg', 75),\n",
       " ('seo', 75),\n",
       " ('dsl', 75),\n",
       " ('waterboarding', 75),\n",
       " ('friday', 74),\n",
       " ('thomas', 74),\n",
       " ('hahaha', 74),\n",
       " ('ceo', 74),\n",
       " ('linkanylink', 74),\n",
       " ('meh', 74),\n",
       " ('nevermind', 74),\n",
       " ('germans', 74),\n",
       " ('alex', 73),\n",
       " ('slashdot', 73),\n",
       " ('einstein', 73),\n",
       " ('taser', 73),\n",
       " ('w/', 73),\n",
       " ('ajax', 72),\n",
       " ('hm', 72),\n",
       " ('wal-mart', 72),\n",
       " ('reid', 72),\n",
       " ('katrina', 72),\n",
       " ('irish', 72),\n",
       " ('fema', 72),\n",
       " ('mexican', 72),\n",
       " ('sheesh', 71),\n",
       " ('i’m', 71),\n",
       " ('ohio', 71),\n",
       " ('linkalllink', 71),\n",
       " ('tm', 71),\n",
       " ('mccain', 71),\n",
       " ('usd', 71),\n",
       " ('hah', 71),\n",
       " ('mexicans', 71),\n",
       " ('ebonics', 71),\n",
       " ('~', 70),\n",
       " ('kennedy', 70),\n",
       " ('pro-life', 70),\n",
       " ('inbox', 69),\n",
       " ('rtfa', 69),\n",
       " ('gc', 69),\n",
       " ('afaik', 69),\n",
       " ('mb', 69),\n",
       " ('jvm', 69),\n",
       " ('tsa', 69),\n",
       " ('coNUMBER', 68),\n",
       " ('linki', 68),\n",
       " ('russians', 68),\n",
       " ('linkdolink', 68),\n",
       " ('craigslist', 68),\n",
       " ('douchebag', 68),\n",
       " ('blair', 68),\n",
       " ('spamming', 68),\n",
       " ('chris', 68),\n",
       " ('los', 68),\n",
       " ('hfcs', 68),\n",
       " ('judgement', 67),\n",
       " ('ur', 67),\n",
       " ('fda', 67),\n",
       " ('craig', 66),\n",
       " ('zealand', 66),\n",
       " ('euro', 66),\n",
       " ('verizon', 66),\n",
       " ('swedish', 66),\n",
       " ('poland', 66),\n",
       " ('fcc', 65),\n",
       " ('that’s', 65),\n",
       " ('spammer', 65),\n",
       " ('zionist', 65),\n",
       " ('goatse', 65),\n",
       " ('iranians', 65),\n",
       " ('NUMBERxNUMBER', 64),\n",
       " ('ol', 64),\n",
       " ('burma', 64),\n",
       " ('he/she', 64),\n",
       " ('boomers', 64),\n",
       " ('hamas', 64),\n",
       " ('you’re', 63),\n",
       " ('spammers', 63),\n",
       " ('ummm', 63),\n",
       " ('ireland', 63),\n",
       " ('ebay', 63),\n",
       " ('adams', 63),\n",
       " ('ibm', 63),\n",
       " ('kasparov', 63),\n",
       " ('sam', 62),\n",
       " ('@', 62),\n",
       " ('carolina', 62),\n",
       " ('anti-semitic', 62),\n",
       " ('/a', 62),\n",
       " ('plato', 61),\n",
       " ('huckabee', 61),\n",
       " ('nfl', 61),\n",
       " ('arent', 61),\n",
       " ('itunes', 61),\n",
       " ('december', 61),\n",
       " ('hitchens', 61),\n",
       " ('linkilink', 60),\n",
       " ('stewart', 60),\n",
       " ('ive', 60),\n",
       " ('richardson', 60),\n",
       " ('del', 59),\n",
       " ('william', 59),\n",
       " ('july', 59),\n",
       " ('colour', 59),\n",
       " ('gitmo', 59),\n",
       " ('guantanamo', 58),\n",
       " ('dem', 58),\n",
       " ('xbox', 58),\n",
       " ('modding', 58),\n",
       " ('sub-reddit', 58),\n",
       " ('downmods', 58),\n",
       " ('lua', 58),\n",
       " ('high-level', 58),\n",
       " ('satan', 58),\n",
       " ('madison', 58),\n",
       " ('angeles', 58),\n",
       " ('alaska', 58),\n",
       " ('wwiii', 58),\n",
       " ('doesn’t', 57),\n",
       " ('hannity', 57),\n",
       " ('gcc', 57),\n",
       " ('georgia', 57),\n",
       " ('svn', 57),\n",
       " ('january', 57),\n",
       " ('NUMBER-bit', 57),\n",
       " ('september', 57),\n",
       " ('fp', 57),\n",
       " ('linklinklink', 57),\n",
       " ('jfk', 57),\n",
       " ('hussein', 56),\n",
       " ('disney', 56),\n",
       " ('favour', 56),\n",
       " ('NUMBER$', 56),\n",
       " ('guiliani', 56),\n",
       " ('alan', 56),\n",
       " ('theatre', 56),\n",
       " ('marx', 56),\n",
       " ('aclu', 56),\n",
       " ('photoshopped', 56),\n",
       " ('occam', 56),\n",
       " ('maher', 56),\n",
       " ('toolbar', 55),\n",
       " ('rockefeller', 55),\n",
       " ('monday', 55),\n",
       " ('stfu', 55),\n",
       " ('tinyurl', 55),\n",
       " ('wmd', 55),\n",
       " ('voip', 55),\n",
       " ('aave', 55),\n",
       " ('moore', 54),\n",
       " ('manhattan', 54),\n",
       " ('yah', 54),\n",
       " ('nbc', 54),\n",
       " ('~NUMBER', 54),\n",
       " ('captain-obvious', 54),\n",
       " ('bju', 54),\n",
       " ('alabama', 53),\n",
       " ('can’t', 53),\n",
       " ('fark', 53),\n",
       " ('lcd', 53),\n",
       " ('wii', 53),\n",
       " ('murdoch', 53),\n",
       " ('sony', 53),\n",
       " ('strawman', 53),\n",
       " ('mississippi', 53),\n",
       " ('britney', 52),\n",
       " ('wmds', 52),\n",
       " ('linkin', 52),\n",
       " ('upvotes', 52),\n",
       " ('mNUMBER', 52),\n",
       " ('williams', 52),\n",
       " ('bloggers', 52),\n",
       " ('pro-choice', 52),\n",
       " ('gb', 52),\n",
       " ('ayn', 52),\n",
       " ('NUMBERam', 51),\n",
       " ('irc', 51),\n",
       " ('buddhist', 51),\n",
       " ('\\\\', 51),\n",
       " ('osama', 51),\n",
       " ('ide', 51),\n",
       " ('mormon', 51),\n",
       " ('£NUMBER', 51),\n",
       " ('dvcs', 51),\n",
       " ('mcdonalds', 51),\n",
       " ('isps', 51),\n",
       " ('debian', 51),\n",
       " ('zionists', 51),\n",
       " ('usb', 50),\n",
       " ('no-one', 50),\n",
       " ('wouldnt', 50),\n",
       " ('non-profit', 50),\n",
       " ('jr', 50),\n",
       " ('lincoln', 50),\n",
       " ('fuckin', 50),\n",
       " ('hindu', 50),\n",
       " ('his/her', 50),\n",
       " ('psp', 50),\n",
       " ('NUMBER-NUMBER-NUMBER', 50),\n",
       " ('fdr', 50),\n",
       " ('bruce', 50),\n",
       " ('sms', 50),\n",
       " ('anti-semite', 50),\n",
       " ('geneva', 49),\n",
       " ('linkedit', 49),\n",
       " ('scott', 49),\n",
       " ('mit', 49),\n",
       " ('inline', 49),\n",
       " ('jon', 49),\n",
       " ('euros', 49),\n",
       " ('distros', 49),\n",
       " ('finland', 49),\n",
       " ('cds', 49),\n",
       " ('hehe', 49),\n",
       " ('johnson', 49),\n",
       " ('brits', 49),\n",
       " ('seattle', 49),\n",
       " ('viagra', 49),\n",
       " ('anti-abortion', 49),\n",
       " ('saturday', 48),\n",
       " ('lsd', 48),\n",
       " ('well-known', 48),\n",
       " ('colorado', 48),\n",
       " ('denmark', 48),\n",
       " ('linksighlink', 48),\n",
       " ('april', 48),\n",
       " ('randi', 48),\n",
       " ('ap', 48),\n",
       " ('wilson', 48),\n",
       " ('darwin', 47),\n",
       " ('everytime', 47),\n",
       " ('favourite', 47),\n",
       " ('ben', 47),\n",
       " ('linkthat', 47),\n",
       " ('adam', 47),\n",
       " ('italian', 47),\n",
       " ('hampshire', 47),\n",
       " ('australian', 47),\n",
       " ('apis', 47),\n",
       " ('bc', 47),\n",
       " ('yeh', 46),\n",
       " ('offline', 46),\n",
       " ('linkyou', 46),\n",
       " ('low-level', 46),\n",
       " ('ph', 46),\n",
       " ('gps', 46),\n",
       " ('t-shirt', 46),\n",
       " ('linksomelink', 46),\n",
       " ('arabic', 46),\n",
       " ('seperate', 46),\n",
       " ('repl', 46),\n",
       " ('kansas', 46),\n",
       " ('mozilla', 45),\n",
       " ('massachusetts', 45),\n",
       " ('linklinknotlinklink', 45),\n",
       " ('NUMBERg', 45),\n",
       " ('NUMBERpm', 45),\n",
       " ('dannykeithjames', 45),\n",
       " ('linkdoeslink', 45),\n",
       " ('nNUMBER', 45),\n",
       " ('turkish', 45),\n",
       " ('ghc', 45),\n",
       " ('netflix', 45),\n",
       " ('mcdonald', 45),\n",
       " ('starbucks', 45),\n",
       " ('hd', 45),\n",
       " ('built-in', 45),\n",
       " ('neo-cons', 45),\n",
       " ('plugin', 45),\n",
       " ('clojure', 45),\n",
       " ('slava', 45),\n",
       " ('adblock', 44),\n",
       " ('oop', 44),\n",
       " ('ahhh', 44),\n",
       " ('norway', 44),\n",
       " ('har', 44),\n",
       " ('fanboys', 44),\n",
       " ('oregon', 44),\n",
       " ('gpl', 44),\n",
       " ('scala', 44),\n",
       " ('toronto', 44),\n",
       " ('-p', 44),\n",
       " ('“i', 44),\n",
       " ('NUMBERc', 44),\n",
       " ('taliban', 44),\n",
       " ('miami', 44),\n",
       " ('linkat', 44),\n",
       " ('freakin', 43),\n",
       " ('buddhism', 43),\n",
       " ('bt', 43),\n",
       " ('howard', 43),\n",
       " ('NUMBERmph', 43),\n",
       " ('linkdon', 43),\n",
       " ('fanboy', 43),\n",
       " ('labour', 43),\n",
       " ('kkk', 43),\n",
       " ('amex', 43),\n",
       " ('tcl', 43),\n",
       " ('leno', 43),\n",
       " ('freebsd', 43),\n",
       " ('organisation', 42),\n",
       " ('spain', 42),\n",
       " ('ufo', 42),\n",
       " ('damnit', 42),\n",
       " ('prius', 42),\n",
       " ('linka', 42),\n",
       " ('yer', 42),\n",
       " ('eval', 42),\n",
       " ('tase', 42),\n",
       " ('msnbc', 42),\n",
       " ('didn’t', 42),\n",
       " ('scotus', 42),\n",
       " ('blogger', 42),\n",
       " ('amerika', 42),\n",
       " ('ahmadinejad', 42),\n",
       " ('hilary', 42),\n",
       " ('…', 42),\n",
       " ('linkyourlink', 42),\n",
       " ('otoh', 42),\n",
       " ('tim', 42),\n",
       " ('vegas', 42),\n",
       " ('telcos', 42),\n",
       " ('penn', 42),\n",
       " ('michigan', 42),\n",
       " ('redstate', 42),\n",
       " ('screenshot', 41),\n",
       " ('greasemonkey', 41),\n",
       " ('italy', 41),\n",
       " ('linus', 41),\n",
       " ('mysql', 41),\n",
       " ('kurds', 41),\n",
       " ('linknot', 41),\n",
       " ('virginia', 41),\n",
       " ('linkverylink', 41),\n",
       " ('allah', 41),\n",
       " ('wednesday', 41),\n",
       " ('NUMBER-year', 41),\n",
       " ('columbia', 41),\n",
       " ('exe', 41),\n",
       " ('definately', 41),\n",
       " ('quebec', 41),\n",
       " ('hunter-gatherer', 41),\n",
       " ('prefs', 40),\n",
       " ('dan', 40),\n",
       " ('re-read', 40),\n",
       " ('karl', 40),\n",
       " ('vatican', 40),\n",
       " ('carlin', 40),\n",
       " ('tennessee', 40),\n",
       " ('faq', 40),\n",
       " ('gwb', 40),\n",
       " ('gw', 40),\n",
       " ('arkansas', 40),\n",
       " ('iowa', 40),\n",
       " ('dialogue', 40),\n",
       " ('knee-jerk', 40),\n",
       " ('linkreallink', 40),\n",
       " ('brazil', 40),\n",
       " ('tuesday', 40),\n",
       " ('linkwilllink', 40),\n",
       " ('href=', 40),\n",
       " ('vm', 40),\n",
       " ('io', 40),\n",
       " ('dave', 40),\n",
       " ('louisiana', 40),\n",
       " ('charles', 40),\n",
       " ('trojan', 40),\n",
       " ('nevada', 39),\n",
       " ('woah', 39),\n",
       " ('django', 39),\n",
       " ('ieNUMBER', 39),\n",
       " ('harvard', 39),\n",
       " ('couchdb', 39),\n",
       " ('mary', 39),\n",
       " ('thier', 39),\n",
       " ('blogspot', 39),\n",
       " ('whitehouse', 39),\n",
       " ('podcast', 39),\n",
       " ('judaism', 39),\n",
       " ('marxism', 39),\n",
       " ('detroit', 39),\n",
       " ('orwell', 39),\n",
       " ('ecuador', 39),\n",
       " ('nutjobs', 38),\n",
       " ('mins', 38),\n",
       " ('nytimes', 38),\n",
       " ('linklinkislinklink', 38),\n",
       " ('worldview', 38),\n",
       " ('sbcl', 38),\n",
       " ('left-wing', 38),\n",
       " ('mpaa', 38),\n",
       " ('snarky', 38),\n",
       " ('americas', 38),\n",
       " ('fundraising', 38),\n",
       " ('norris', 38),\n",
       " ('linkdidlink', 38),\n",
       " ('greenspan', 38),\n",
       " ('portland', 38),\n",
       " ('NUMBER/NUMBERth', 38),\n",
       " ('linkwhylink', 38),\n",
       " ('dea', 38),\n",
       " ('ss', 38),\n",
       " ('pakistani', 38),\n",
       " ('february', 38),\n",
       " ('bmw', 38),\n",
       " ('hilton', 38),\n",
       " ('kelly', 38),\n",
       " ('lolcats', 38),\n",
       " ('sapiens', 38),\n",
       " ('arar', 38),\n",
       " ('trent', 38),\n",
       " ('mou', 38),\n",
       " ('spez', 37),\n",
       " ('suv', 37),\n",
       " ('sox', 37),\n",
       " ('hmmmm', 37),\n",
       " ('grey', 37),\n",
       " ('i’ve', 37),\n",
       " ('mormons', 37),\n",
       " ('down-modded', 37),\n",
       " ('short-term', 37),\n",
       " ('sayin', 37),\n",
       " ('tcp', 37),\n",
       " ('objective-c', 37),\n",
       " ('devs', 37),\n",
       " ('aipac', 37),\n",
       " ('linkhavelink', 37),\n",
       " ('googling', 37),\n",
       " ('blogging', 37),\n",
       " ('wasnt', 37),\n",
       " ('jared', 37),\n",
       " ('castro', 37),\n",
       " ('anti-gay', 37),\n",
       " ('cross-platform', 37),\n",
       " ('pro-lifers', 37),\n",
       " ('fsm', 37),\n",
       " ('linkonlylink', 36),\n",
       " ('sooo', 36),\n",
       " ('le', 36),\n",
       " ('geez', 36),\n",
       " ('automata', 36),\n",
       " ('warcraft', 36),\n",
       " ('linkonelink', 36),\n",
       " ('becuase', 36),\n",
       " ('admins', 36),\n",
       " ('june', 36),\n",
       " ('ftp', 36),\n",
       " ('dev', 36),\n",
       " ('arizona', 36),\n",
       " ('friedman', 36),\n",
       " ('pcs', 36),\n",
       " ('redditers', 36),\n",
       " ('^', 36),\n",
       " ('neo-con', 36),\n",
       " ('NUMBER/NUMBER/NUMBER', 36),\n",
       " ('barack', 36),\n",
       " ('hotmail', 36),\n",
       " ('il', 35),\n",
       " ('linkalwayslink', 35),\n",
       " ('linkshouldlink', 35),\n",
       " ('switzerland', 35),\n",
       " ('nokia', 35),\n",
       " ('linkwaslink', 35),\n",
       " ('open-source', 35),\n",
       " ('joel', 35),\n",
       " ('uhh', 35),\n",
       " ('danny', 35),\n",
       " ('bsd', 35),\n",
       " ('dipshit', 35),\n",
       " ('linkthislink', 35),\n",
       " ('kosovo', 35),\n",
       " ('plumpynut', 35),\n",
       " ('carl', 34),\n",
       " ('noone', 34),\n",
       " ('defence', 34),\n",
       " ('jordan', 34),\n",
       " ('cuban', 34),\n",
       " ('downvotes', 34),\n",
       " ('linkknowlink', 34),\n",
       " ('humour', 34),\n",
       " ('mohammed', 34),\n",
       " ('commenters', 34),\n",
       " ('dubya', 34),\n",
       " ('kevin', 34),\n",
       " ('costco', 34),\n",
       " ('indie', 34),\n",
       " ('telecom', 34),\n",
       " ('downvoting', 34),\n",
       " ('darfur', 34),\n",
       " ('linkexactlylink', 34),\n",
       " ('blonde', 34),\n",
       " ('dalai', 34),\n",
       " ('vancouver', 34),\n",
       " ('isn’t', 34),\n",
       " ('guliani', 34),\n",
       " ('NUMBERNUMBER', 34),\n",
       " ('fundies', 34),\n",
       " ('ira', 34),\n",
       " ('simpson', 34),\n",
       " ('nsa', 34),\n",
       " ('wtcNUMBER', 34),\n",
       " ('russell', 34),\n",
       " ('typename', 34),\n",
       " ('non-profits', 34),\n",
       " ('unsc', 34),\n",
       " ('gandhi', 33),\n",
       " ('oprah', 33),\n",
       " ('googled', 33),\n",
       " ('linkmorelink', 33),\n",
       " ('faux', 33),\n",
       " ('mrs', 33),\n",
       " ('tfa', 33),\n",
       " ('nancy', 33),\n",
       " ('t-shirts', 33),\n",
       " ('r&amp', 33),\n",
       " ('$NUMBERm', 33),\n",
       " ('netherlands', 33),\n",
       " ('ifpi', 33),\n",
       " ('fud', 33),\n",
       " ('fucktard', 33),\n",
       " ('real-world', 33),\n",
       " ('they’re', 33),\n",
       " ('hebrew', 33),\n",
       " ('tased', 33),\n",
       " ('sw', 33),\n",
       " ('self-defense', 33),\n",
       " ('hawaii', 33),\n",
       " ('saul', 33),\n",
       " ('jpg', 33),\n",
       " ('zsh', 33),\n",
       " ('malware', 33),\n",
       " ('hezbollah', 33),\n",
       " ('biodiesel', 33),\n",
       " ('NUMBERa', 33),\n",
       " ('gonzales', 33),\n",
       " ('glock', 33),\n",
       " ('marshall', 32),\n",
       " ('mmmm', 32),\n",
       " ('batshit', 32),\n",
       " ('frontpage', 32),\n",
       " ('off-topic', 32),\n",
       " ('unicode', 32),\n",
       " ('cNUMBER', 32),\n",
       " ('fwiw', 32),\n",
       " ('linkstilllink', 32),\n",
       " ('honda', 32),\n",
       " ('emo', 32),\n",
       " ('linkcanlink', 32),\n",
       " ('fta', 32),\n",
       " ('steven', 32),\n",
       " ('erm', 32),\n",
       " ('linkneverlink', 32),\n",
       " ('rofl', 32),\n",
       " ('anderson', 32),\n",
       " ('cgi', 32),\n",
       " ('armenian', 32),\n",
       " ('tNUMBER', 32),\n",
       " ('yale', 32),\n",
       " ('touche', 32),\n",
       " ('alice', 32),\n",
       " ('meds', 32),\n",
       " ('sudan', 32),\n",
       " ('cdc', 32),\n",
       " ('hahahaha', 32),\n",
       " ('thc', 32),\n",
       " ('cliche', 32),\n",
       " ('aol', 32),\n",
       " ('maddox', 32),\n",
       " ('truther', 32),\n",
       " ('silverlight', 32),\n",
       " ('sativa', 32),\n",
       " ('amsterdam', 32),\n",
       " ('NUMBERmm', 32),\n",
       " ('robertson', 32),\n",
       " ('rst', 32),\n",
       " ('pageless', 32),\n",
       " ('non-trivial', 31),\n",
       " ('pennsylvania', 31),\n",
       " ('wifi', 31),\n",
       " ('ffs', 31),\n",
       " ('badass', 31),\n",
       " ...]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_mispelled_words[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try and replace contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset(df):\n",
    "    df = load_data()\n",
    "    df = initial_clean(df)\n",
    "    df,total_mods = clean_with_tracking(df)\n",
    "    df = remove_large_comments(60, df)\n",
    "    df = contraction_replacer(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "        \"ain't\": \"am not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he had\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he shall\",\n",
    "        \"he'll've\": \"he shall have\",\n",
    "        \"he's\": \"he has\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how has\",\n",
    "        \"I'd\": \"I had\",\n",
    "        \"I'd've\": \"I would have\",\n",
    "        \"I'll\": \"I shall\",\n",
    "        \"I'll've\": \"I shall have\",\n",
    "        \"I'm\": \"I am\",\n",
    "        \"I've\": \"I have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it had\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it shall\",\n",
    "        \"it'll've\": \"it shall have\",\n",
    "        \"it's\": \"it has\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she had\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she shall\",\n",
    "        \"she'll've\": \"she shall have\",\n",
    "        \"she's\": \"she has\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so as\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that has\",\n",
    "        \"there'd\": \"there had\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there has\",\n",
    "        \"they'd\": \"they had\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they shall\",\n",
    "        \"they'll've\": \"they shall have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we had\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what shall\",\n",
    "        \"what'll've\": \"what shall have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what has\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when has\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where has\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who shall\",\n",
    "        \"who'll've\": \"who shall have\",\n",
    "        \"who's\": \"who has\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why has\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you had\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you shall\",\n",
    "        \"you'll've\": \"you shall have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contraction_replacer(df):\n",
    "    for patrn in contractions.items():\n",
    "        df['body'].replace({patrn[0]: patrn[1]}, regex=True, inplace=True)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = contraction_replacer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-962200dcb3fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-962200dcb3fb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-96d801e9a6ef>\u001b[0m in \u001b[0;36mbasic_tokenizer\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mspace_separated_fragment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_WORD_SPLIT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace_separated_fragment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "df['body'] = [basic_tokenizer(sentence) for sentence in df['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_len_update(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_sentences, misspelled_words = invalid_word(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_misspelled_words = sorted(misspelled_words.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_misspelled_words[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MISSPELLED_WORDS = '/Users/ivan/Desktop/mywords.txt'\n",
    "f = open(MISSPELLED_WORDS, 'w')\n",
    "for word in sorted_mispelled_words[:10000]:\n",
    "    f.write(word[0] + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def invalid_word_modified(df):\n",
    "    '''Goes through the content and determines whether an invalid word is \n",
    "    present.\n",
    "    \n",
    "    The data frame should provide a body field which will be inspected.\n",
    "    '''\n",
    "    d = enchant.DictWithPWL(\"en_US\", MISSPELLED_WORDS)\n",
    "    valid_sentences = [True] * len(df)\n",
    "    misspelled_words = {}\n",
    "     \n",
    "    for idx, sentence in enumerate(df['body'].values):\n",
    "        for word in sentence:\n",
    "            if not d.check(word):\n",
    "                if word in misspelled_words:\n",
    "                    misspelled_words[word] += 1\n",
    "                else:\n",
    "                    misspelled_words[word] = 1\n",
    "                valid_sentences[idx] = False\n",
    "    print(\"There are %i valid sentences out of %i.\" % (sum(valid_sentences), len(valid_sentences)))\n",
    "    print(\"There are %i misspelled words.\" % len(misspelled_words))\n",
    "    return valid_sentences, misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_sentences, misspelled_words = invalid_word_modified(df)\n",
    "sorted_misspelled_words = sorted(misspelled_words.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By adding 10000 extra words (not in the original dictionary) we only see 5000 more valid sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a threshold which keeps sentences with many of the misspelled words\n",
    "* For words that are in the original dictionary add 0 points. \n",
    "* Words that are not in the original dictionary add the inverse of the number of occurences in the corpus\n",
    "* We then normalize to the length of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in raw data file 150429\n",
      "'Column names from raw data file:'\n",
      "Index(['archived', 'author', 'author_flair_css_class', 'author_flair_text',\n",
      "       'body', 'controversiality', 'created_utc', 'distinguished', 'downs',\n",
      "       'edited', 'gilded', 'id', 'link_id', 'name', 'parent_id',\n",
      "       'retrieved_on', 'score', 'score_hidden', 'subreddit', 'subreddit_id',\n",
      "       'ups'],\n",
      "      dtype='object')\n",
      "Now there are 150429 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before: 126320\n",
      "Now there are 102363 rows.\n"
     ]
    }
   ],
   "source": [
    "df = reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [basic_tokenizer(sentence) for sentence in df['body']]\n",
    "words= [word for sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_freq = Counter(chain(words))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_score(sentence):\n",
    "    d = enchant.Dict('en_US')\n",
    "    word_count = len(sentence)\n",
    "    score = 0\n",
    "    for word in sentence:\n",
    "        if not d.check(word):\n",
    "            try: \n",
    "                score = score + 1.0/word_freq[word]\n",
    "            except ZeroDivisionError:\n",
    "                score = score + 1.0\n",
    "    try:\n",
    "        return score / word_count\n",
    "    except ZeroDivisionError:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_sentence_scores(df):\n",
    "    scores = []\n",
    "    pbar = ProgressBar()\n",
    "    for sentence in pbar(df.body):\n",
    "        scores.append(sentence_score(basic_tokenizer(sentence)))\n",
    "    df['score'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49% (285253 of 580784) |########          | Elapsed Time: 0:20:42 ETA: 0:21:07"
     ]
    }
   ],
   "source": [
    "add_sentence_scores(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A plot which displays the distribution of \"penalty score\" of a sentence. \n",
    "plt.hist(df.score.values, bins=500)\n",
    "plt.xlim(0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df.loc[df.score < 0.005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_json(RAW_DATA_FILES[0], lines=True)\n",
    "    df2 = pd.read_json(RAW_DATA_FILES[1], lines=True)\n",
    "    df3 = pd.read_json(RAW_DATA_FILES[2], lines=True)\n",
    "    df = df.append(df2, ignore_index=True)\n",
    "    df = df.append(df3, ignore_index=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    init_num_rows = len(df)\n",
    "    print(\"Number of lines in raw data file\", init_num_rows)\n",
    "    pprint(\"Column names from raw data file:\")\n",
    "    pprint(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in raw data file 886802\n",
      "'Column names from raw data file:'\n",
      "Index(['archived', 'author', 'author_flair_css_class', 'author_flair_text',\n",
      "       'body', 'controversiality', 'created_utc', 'distinguished', 'downs',\n",
      "       'edited', 'gilded', 'id', 'link_id', 'name', 'parent_id',\n",
      "       'retrieved_on', 'score', 'score_hidden', 'subreddit', 'subreddit_id',\n",
      "       'ups'],\n",
      "      dtype='object')\n",
      "Now there are 886802"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rows.\n",
      "Length before: 707894\n",
      "Now there are 580784 rows.\n"
     ]
    }
   ],
   "source": [
    "df = reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580784"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [basic_tokenizer(sentence) for sentence in df['body']]\n",
    "words= [word for sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freq = Counter(chain(words))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 839911),\n",
       " ('the', 465182),\n",
       " (',', 431223),\n",
       " ('to', 273903),\n",
       " ('i', 261061),\n",
       " ('a', 259704),\n",
       " ('it', 200564),\n",
       " ('of', 199422),\n",
       " ('is', 189873),\n",
       " ('that', 187720),\n",
       " ('you', 185582),\n",
       " ('not', 185270),\n",
       " ('and', 183542),\n",
       " ('?', 156249),\n",
       " ('\"', 140273),\n",
       " ('in', 134641),\n",
       " ('has', 123191),\n",
       " (\"'\", 115135),\n",
       " ('are', 108203),\n",
       " ('for', 104891),\n",
       " ('!', 100336),\n",
       " ('this', 84086),\n",
       " ('do', 80716),\n",
       " ('have', 75754),\n",
       " ('on', 73410),\n",
       " ('be', 73168),\n",
       " ('they', 72579),\n",
       " ('NUMBER', 70331),\n",
       " ('but', 63086),\n",
       " ('(', 62003),\n",
       " ('with', 61780),\n",
       " ('was', 61727),\n",
       " ('link', 55899),\n",
       " ('if', 55680),\n",
       " ('what', 53426),\n",
       " ('as', 52669),\n",
       " ('he', 50438),\n",
       " (')', 49593),\n",
       " ('just', 48642),\n",
       " (':', 48137),\n",
       " ('like', 46909),\n",
       " ('would', 44452),\n",
       " ('or', 43766),\n",
       " ('your', 43547),\n",
       " ('so', 43248),\n",
       " ('about', 42833),\n",
       " ('all', 42601),\n",
       " ('my', 40120),\n",
       " ('people', 39823),\n",
       " ('we', 38380),\n",
       " ('no', 38339),\n",
       " ('at', 38061),\n",
       " ('there', 37931),\n",
       " ('an', 36721),\n",
       " ('can', 34837),\n",
       " ('s', 34312),\n",
       " ('one', 34204),\n",
       " ('from', 32842),\n",
       " ('more', 32836),\n",
       " ('think', 31906),\n",
       " ('me', 31568),\n",
       " ('will', 31006),\n",
       " ('get', 30020),\n",
       " ('m', 29993),\n",
       " ('how', 29722),\n",
       " ('who', 29631),\n",
       " ('up', 28975),\n",
       " ('does', 28551),\n",
       " ('by', 28501),\n",
       " ('out', 27368),\n",
       " ('did', 26319),\n",
       " ('when', 25108),\n",
       " ('their', 24923),\n",
       " ('some', 23929),\n",
       " ('them', 23743),\n",
       " ('know', 23646),\n",
       " ('because', 23311),\n",
       " ('why', 23050),\n",
       " ('than', 22568),\n",
       " ('his', 21945),\n",
       " ('should', 21432),\n",
       " ('good', 21262),\n",
       " ('only', 20864),\n",
       " ('really', 19803),\n",
       " ('had', 19702),\n",
       " ('then', 19412),\n",
       " ('-', 19070),\n",
       " ('see', 18803),\n",
       " ('well', 18797),\n",
       " ('time', 18776),\n",
       " ('now', 18665),\n",
       " ('us', 17962),\n",
       " ('could', 17753),\n",
       " ('too', 17616),\n",
       " ('any', 17185),\n",
       " ('right', 17146),\n",
       " ('much', 17085),\n",
       " ('make', 16929),\n",
       " ('even', 16859),\n",
       " ('other', 16619)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_word_freq[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-f049c2ba1cf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madd_sentence_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-032333c2428b>\u001b[0m in \u001b[0;36madd_sentence_scores\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-8a7f5de2be9c>\u001b[0m in \u001b[0;36msentence_score\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mword_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/lib/python3.6/site-packages/enchant/__init__.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can't check spelling of empty string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_this\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/lib/python3.6/site-packages/enchant/_enchant.py\u001b[0m in \u001b[0;36mdict_check\u001b[0;34m(dict, word)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0mdict_check1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdict_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict_check1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0mdict_suggest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menchant_dict_suggest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "add_sentence_scores(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df.score.values, bins=500)\n",
    "plt.xlim(0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df.loc[df.score < 0.005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns a dictionary with keys being the root comments and values being their immediate children.\n",
    "## Assumes to have a 'root' column already\n",
    "\n",
    "## Go through all comments, if it is a root skip it since they wont have a parent_id corresponding\n",
    "## to a comment.\n",
    "## \n",
    "def children_dict(df):\n",
    "    children = {}\n",
    "    for row in df.itertuples():\n",
    "        if row.root == False:\n",
    "            if row.parent_id in children.keys():\n",
    "                children[row.parent_id].append(row.name)\n",
    "            else:\n",
    "                children[row.parent_id] = [row.name]\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Return a dictionary with name being the key and body being the value. \n",
    "values_dict = pd.Series(df.body.values, index=df.name).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "children = children_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Generates two files, [from_file_path] and [to_file_path] of one-to-one comments. \n",
    "def generate_files(from_file_path, to_file_path):\n",
    "    ## Open the files and clear them. \n",
    "    from_file = open(from_file_path, 'w')\n",
    "    to_file = open(to_file_path, 'w')\n",
    "    from_file.write(\"\")\n",
    "    to_file.write(\"\")\n",
    "    from_file.close()\n",
    "    to_file.close()\n",
    "\n",
    "    for key in children.keys():\n",
    "        from_file = open(from_file_path, 'a')\n",
    "        to_file = open(to_file_path, 'a')\n",
    "\n",
    "        ## Since we have deleted comments, some comments parents might not exist anymore so we must catch that error.\n",
    "        for child in children[key]:\n",
    "            try: \n",
    "                from_file.write(values_dict[key].replace('\\n', '').replace('\\r', ' ').replace('&gt', '') + \"\\n\")\n",
    "                to_file.write(values_dict[child].replace('\\n', '').replace('\\r', ' ').replace('&gt', '') + \"\\n\")\n",
    "            except KeyError:    \n",
    "                pass\n",
    "    from_file.close()\n",
    "    to_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_files(\"from_file.txt\", \"to_file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
