{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Grammar and Word Lists\n",
    "\n",
    "It would be nice to clean up a dataset which contains grammatically incorrect language and make it seem more professional. \n",
    "\n",
    "To do this we will experiment with numerous open source libraries to determine their efficacy. \n",
    "\n",
    "We also want to see if we can replace misspelled words and expand contrations into their correct forms.\n",
    "\n",
    "Another eventual task will be to determine the 'quality' of a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/brandon/terabyte/Datasets/reddit/raw_data/2007/RC_2007-10',\n",
      " '/home/brandon/terabyte/Datasets/reddit/raw_data/2007/RC_2007-11',\n",
      " '/home/brandon/terabyte/Datasets/reddit/raw_data/2007/RC_2007-12']\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = '/home/brandon/terabyte/Datasets/reddit'\n",
    "# Determine if this directory exists, if not use Ivan's directory.\n",
    "if (os.path.isdir(DATA_ROOT)):\n",
    "    pass\n",
    "else:\n",
    "    DATA_ROOT = '/Users/ivan/Documents/sp_17/reddit_data'\n",
    "DATA_YEARS = ['2007'] #$, '2008']\n",
    "# Use os.path.join; it will figure out the '/' in between.\n",
    "RAW_DATA_FILES = [os.listdir(os.path.join(DATA_ROOT, 'raw_data', year)) for year in DATA_YEARS]\n",
    "\n",
    "RAW_DATA_ABS_FILES = []\n",
    "# Always work with full pathnames to be safe.\n",
    "for i in range(len(DATA_YEARS)):\n",
    "    for j in range(len(RAW_DATA_FILES[i])):\n",
    "        if RAW_DATA_FILES[i][j].startswith('.'):\n",
    "            pass\n",
    "        else:\n",
    "            RAW_DATA_ABS_FILES.append( os.path.join(DATA_ROOT, 'raw_data' , DATA_YEARS[i], RAW_DATA_FILES[i][j]))\n",
    "RAW_DATA_FILES = RAW_DATA_ABS_FILES\n",
    "pprint(RAW_DATA_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_json(RAW_DATA_FILES[0], lines=True)\n",
    "    init_num_rows = len(df)\n",
    "    print(\"Number of lines in raw data file\", init_num_rows)\n",
    "    pprint(\"Column names from raw data file:\")\n",
    "    pprint(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(RAW_DATA_FILES[0])\n",
    "    df = pd.read_json(RAW_DATA_FILES[0], lines=True)\n",
    "    for i in range(len(RAW_DATA_FILES) - 1):\n",
    "        df = df.append(pd.read_json(RAW_DATA_FILES[i+1], lines=True), ignore_index=True)\n",
    "    init_num_rows = len(df)\n",
    "    print(\"Number of lines in raw data file\", init_num_rows)\n",
    "    pprint(\"Column names from raw data file:\")\n",
    "    pprint(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/brandon/terabyte/Datasets/reddit/raw_data/2007/RC_2007-10\n",
      "Number of lines in raw data file 886802\n",
      "'Column names from raw data file:'\n",
      "Index(['archived', 'author', 'author_flair_css_class', 'author_flair_text',\n",
      "       'body', 'controversiality', 'created_utc', 'distinguished', 'downs',\n",
      "       'edited', 'gilded', 'id', 'link_id', 'name', 'parent_id',\n",
      "       'retrieved_on', 'score', 'score_hidden', 'subreddit', 'subreddit_id',\n",
      "       'ups'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_len_update(df):\n",
    "    print(\"Now there are\", len(df), \"rows.\")\n",
    "    \n",
    "def root_comments(df):\n",
    "    '''Build list determining which rows of df are root comments.\n",
    "    \n",
    "    Returns: \n",
    "        list of length equal to the number of rows in our data frame. \n",
    "    '''\n",
    "    root_value = []\n",
    "    # Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple.\n",
    "    for row in df.itertuples():\n",
    "        root_value.append(row.parent_id == row.link_id)\n",
    "    return root_value\n",
    "\n",
    "def random_rows_generator(num_rows_per_print, num_rows_total):\n",
    "    num_iterations = num_rows_total // num_rows_per_print \n",
    "    shuffled_indices = np.arange(num_rows_per_print * num_iterations)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    for batch in shuffled_indices.reshape(num_iterations, num_rows_per_print):\n",
    "        yield batch\n",
    "        \n",
    "#rand_rows = random_rows_generator(4, len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Start by removing comments without a body (deleted).\n",
    "* Remove comments larger than 150 words long.\n",
    "* Remove unneccesary columns. \n",
    "* Add a column determining whether a row is a root comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_clean(df):\n",
    "    df['root'] = root_comments(df)\n",
    "    df = df[['author', 'body', 'link_id', 'parent_id', 'name', 'root', 'subreddit']]\n",
    "    df.style.set_properties(subset=['body'], **{'width': '500px'})\n",
    "    df.style.set_properties(**{'text-align': 'left'})\n",
    "    show_len_update(df)\n",
    "    df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now there are 886802 rows.\n"
     ]
    }
   ],
   "source": [
    "df = initial_clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>name</th>\n",
       "      <th>root</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bostich</td>\n",
       "      <td>test</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299an</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>igiveyoumylife</td>\n",
       "      <td>much smoother.\\r\\n\\r\\nIm just glad reddit is b...</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299ao</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arve</td>\n",
       "      <td>Can we please deprecate the word \"Ajax\" now? \\...</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c02999p</td>\n",
       "      <td>t1_c0299ap</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299aq</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gigaquack</td>\n",
       "      <td>Oh, I see. Fancy schmancy \"submitting....\"</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299ah</td>\n",
       "      <td>t1_c0299ar</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               body  \\\n",
       "0         bostich                                               test   \n",
       "1  igiveyoumylife  much smoother.\\r\\n\\r\\nIm just glad reddit is b...   \n",
       "2            Arve  Can we please deprecate the word \"Ajax\" now? \\...   \n",
       "3       [deleted]                                          [deleted]   \n",
       "4       gigaquack         Oh, I see. Fancy schmancy \"submitting....\"   \n",
       "\n",
       "    link_id   parent_id        name   root   subreddit  \n",
       "0  t3_5yba3    t3_5yba3  t1_c0299an   True  reddit.com  \n",
       "1  t3_5yba3    t3_5yba3  t1_c0299ao   True  reddit.com  \n",
       "2  t3_5yba3  t1_c02999p  t1_c0299ap  False  reddit.com  \n",
       "3  t3_5yba3    t3_5yba3  t1_c0299aq   True  reddit.com  \n",
       "4  t3_5yba3  t1_c0299ah  t1_c0299ar  False  reddit.com  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modify_list = [('\\r\\n', ' '),\n",
    "               ('\\n', ' '),\n",
    "               ('\\r', ' '),\n",
    "               ('&gt;', ' '),\n",
    "               ('&lt;', ' '),\n",
    "               ('/__|\\*|\\#|(?:\\[([^\\]]*)\\]\\([^)]*\\))/gm', '[link]'),\n",
    "               ('https?:\\/\\/(?:www\\.|(?!www))[^\\s\\.]+\\.[^\\s]{2,}|www\\.[^\\s]+\\.[^\\s]{2,}', '[link]'),\n",
    "               ('\\d+', 'NUMBER'),\n",
    "               ('\\[', ''),\n",
    "               ('\\]', ''),\n",
    "               ('\\/\\/', ''),\n",
    "               ('\\.\\.\\.', '. ')\n",
    "              ]\n",
    "\n",
    "modify_value = {'\\r\\n': 1,\n",
    "               '\\n': 1,\n",
    "               '\\r': 1,\n",
    "               '&gt;': 10,\n",
    "               '&lt;': 10,\n",
    "               '/__|\\*|\\#|(?:\\[([^\\]]*)\\]\\([^)]*\\))/gm': 100,\n",
    "               'https?:\\/\\/(?:www\\.|(?!www))[^\\s\\.]+\\.[^\\s]{2,}|www\\.[^\\s]+\\.[^\\s]{2,}': 100,\n",
    "               '\\d+': 1000,\n",
    "               '\\[': 10000,\n",
    "               '\\]': 10000,\n",
    "               '\\/\\/': 10000,\n",
    "               '\\.\\.\\.': 100000\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_with_tracking(df):\n",
    "    df = df.loc[df.body != '[deleted]'].reset_index(drop=True)\n",
    "    df.style.set_properties(subset=['body'], **{'width': '800px'})\n",
    "    df['body'] = df['body'].map(lambda s: s.strip().lower())\n",
    "    \n",
    "    total_mods = {}\n",
    "    if 'mods' not in df: \n",
    "        df['mods'] = np.zeros(len(df['body']), dtype=int)\n",
    "    for patrn in modify_list:\n",
    "        new_df = df['body'].replace({patrn[0]: patrn[1]}, regex=True, inplace=False)\n",
    "        modifications = list((np.where(new_df.values != df['body'].values))[0])\n",
    "        df['body'] = new_df\n",
    "        df['mods'][modifications] += modify_value[patrn[0]]\n",
    "        total_mods[patrn[0]] = len(modifications)\n",
    "    return df, total_mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df,total_mods = clean_with_tracking(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>name</th>\n",
       "      <th>root</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>mods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>707889</th>\n",
       "      <td>rowd149</td>\n",
       "      <td>so.  so dinotopia linkcouldlink exist? :d</td>\n",
       "      <td>t3_6482i</td>\n",
       "      <td>t1_c02s8g5</td>\n",
       "      <td>t1_c02s9s1</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707890</th>\n",
       "      <td>mlietzen</td>\n",
       "      <td>don't you wish americans cared this much about...</td>\n",
       "      <td>t3_648hg</td>\n",
       "      <td>t3_648hg</td>\n",
       "      <td>t1_c02s9s2</td>\n",
       "      <td>True</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707891</th>\n",
       "      <td>bbqribs</td>\n",
       "      <td>the illegals around here said in interviews th...</td>\n",
       "      <td>t3_647mf</td>\n",
       "      <td>t1_c02s8lb</td>\n",
       "      <td>t1_c02s9s3</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707892</th>\n",
       "      <td>joyork</td>\n",
       "      <td>for the first time ever on nye i'm alone and s...</td>\n",
       "      <td>t3_648on</td>\n",
       "      <td>t3_648on</td>\n",
       "      <td>t1_c02s9s4</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707893</th>\n",
       "      <td>boredzo</td>\n",
       "      <td>i recently discovered that this doesn't work...</td>\n",
       "      <td>t3_648cg</td>\n",
       "      <td>t3_648cg</td>\n",
       "      <td>t1_c02s9s5</td>\n",
       "      <td>True</td>\n",
       "      <td>programming</td>\n",
       "      <td>21111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                                               body   link_id  \\\n",
       "707889   rowd149          so.  so dinotopia linkcouldlink exist? :d  t3_6482i   \n",
       "707890  mlietzen  don't you wish americans cared this much about...  t3_648hg   \n",
       "707891   bbqribs  the illegals around here said in interviews th...  t3_647mf   \n",
       "707892    joyork  for the first time ever on nye i'm alone and s...  t3_648on   \n",
       "707893   boredzo    i recently discovered that this doesn't work...  t3_648cg   \n",
       "\n",
       "         parent_id        name   root    subreddit    mods  \n",
       "707889  t1_c02s8g5  t1_c02s9s1  False   reddit.com  120100  \n",
       "707890    t3_648hg  t1_c02s9s2   True     politics       0  \n",
       "707891  t1_c02s8lb  t1_c02s9s3  False   reddit.com    1000  \n",
       "707892    t3_648on  t1_c02s9s4   True   reddit.com       1  \n",
       "707893    t3_648cg  t1_c02s9s5   True  programming   21111  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyEnchant is used to check if this is a real word.\n",
    "\n",
    "* An issue with this apporach is words that are not english, but are used heavily (e.g. 'reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, 'much'),\n",
       " (True, 'smoother.'),\n",
       " (False, 'im'),\n",
       " (True, 'just'),\n",
       " (True, 'glad'),\n",
       " (False, 'reddit'),\n",
       " (True, 'is'),\n",
       " (False, 'back,'),\n",
       " (False, 'linkreddit'),\n",
       " (True, 'in'),\n",
       " (False, 'mirc'),\n",
       " (True, 'was'),\n",
       " (True, 'entertaining'),\n",
       " (True, 'but'),\n",
       " (True, 'i'),\n",
       " (True, 'had'),\n",
       " (True, 'no'),\n",
       " (True, 'idea'),\n",
       " (True, 'how'),\n",
       " (True, 'addicted'),\n",
       " (True, 'i'),\n",
       " (True, 'had'),\n",
       " (True, 'become.'),\n",
       " (True, 'thanks'),\n",
       " (True, 'for'),\n",
       " (True, 'making'),\n",
       " (True, 'the'),\n",
       " (True, 'detox'),\n",
       " (True, 'somewhat'),\n",
       " (True, 'short.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(d.check(word), word) for word in df.body[1].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'language_check'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-309cc0798104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlanguage_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'language_check'"
     ]
    }
   ],
   "source": [
    "import language_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tool = language_check.LanguageTool('en_US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches = tool.check(df.body[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tool.disabled.add(\"UPPERCASE_SENTENCE_START\")\n",
    "tool.disabled.add('I_LOWERCASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = tool.check(df.body[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove comments with more than n words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_large_comments(n, df):\n",
    "    print(\"Length before:\", df['body'].size)\n",
    "    df = df[df['body'].map(lambda s: len(s.split(' '))) < n].reset_index(drop=True)\n",
    "    show_len_update(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the tokenizer from io_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE   = re.compile(r\"\\d\")\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w for w in words if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "df['body'] = [basic_tokenizer(sentence) for sentence in df['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                    [test]\n",
       "1         [much, smoother, ., im, just, glad, reddit, is...\n",
       "2         [can, we, please, deprecate, the, word, \", aja...\n",
       "3         [oh, ,, i, see, ., fancy, schmancy, \", submitt...\n",
       "4                                              [testing, .]\n",
       "5                      [i, like, it, ., one, more, time, .]\n",
       "6         [try, refreshing, yor, cache, ,, that, worked,...\n",
       "7                    [k, ., i, lied, ., just, one, more, .]\n",
       "8         [i, also, wonder, what, the, differences, are, .]\n",
       "9                                        [so, addictive, .]\n",
       "10        [i, can, ', t, post, a, story, to, proggit, -,...\n",
       "11                              [alright, i, ', m, done, .]\n",
       "12        [is, anyone, else, ', s, \", recommended, \", pa...\n",
       "13        [ok, ,, i, guess, we, need, to, submit, commen...\n",
       "14        [i, can, ', t, submit, any, stories, --, even,...\n",
       "15                [working, fine, with, normal, adblock, .]\n",
       "16        [can, ', t, see, beta, ., reddit, ., com, from...\n",
       "17        [i, had, a, problem, as, well, ,, with, my, co...\n",
       "18                                                  [hm, ?]\n",
       "19                                        [i, think, so, .]\n",
       "20          [well, ., i, read, ron, paul, ', s, website, .]\n",
       "21                                                     [no]\n",
       "22        [cry, me, a, river, ., most, patents, and, cop...\n",
       "23                                                    [jup]\n",
       "24        [as, long, as, we, can, still, say, \", web, NU...\n",
       "25        [what, ', s, new, except, the, asynchronous, s...\n",
       "26                                                    [yep]\n",
       "27        [you, can, stop, anytime, you, want, ., really...\n",
       "28                   [hmm, only, worked, after, refresh, .]\n",
       "29        [okay, ,, dokay, ,, lets, see, how, the, comme...\n",
       "                                ...                        \n",
       "707864    [with, carnivorous, trees, growing, on, them, ...\n",
       "707865    [its, a, ploy, dude, ,, they, have, no, integr...\n",
       "707866    [the, whole, fact, it, is, risky, is, why, peo...\n",
       "707867    [while, ultimately, all, civil, legal, action,...\n",
       "707868    [i, ', m, an, atheist, and, i, have, no, probl...\n",
       "707869    [i, actually, would, do, this, except, for, th...\n",
       "707870    [and, vote, down, if, you, are, sick, of, stup...\n",
       "707871    [the, neocons, and, zionists, do, this, well, ...\n",
       "707872    [i, know, that, ', s, the, problem, !, i, ', m...\n",
       "707873    [call, a, doctor, ,, my, jaw, won, ', t, close...\n",
       "707874    [\", our, state, has, over, NUMBER, traffic, la...\n",
       "707875    [you, put, white, or, not, ,, and, then, you, ...\n",
       "707876    [i, don, ', t, know, about, my, bf, ,, but, is...\n",
       "707877                  [bro, my, spelling, is, fucked, up]\n",
       "707878         [he, did, say, he, went, to, texas, tech, .]\n",
       "707879    [you, ', re, right, ., it, was, a, hasty, comm...\n",
       "707880    [don, ', t, you, wish, americans, cared, as, m...\n",
       "707881    [hello, ., checks, and, balances, ring, any, b...\n",
       "707882    [i, agree, whole, heartedly, ., just, one, cla...\n",
       "707883    [tonight, is, the, perfect, night, to, march, ...\n",
       "707884    [um, ,, sure, if, you, want, to, ., not, sure,...\n",
       "707885    [if, the, ravens, get, marty, schottenheimer, ...\n",
       "707886    [the, chaos, and, mayhem, decreased, when, the...\n",
       "707887                                      [sociopaths, .]\n",
       "707888    [i, live, in, this, neighborhood, ,, f-NUMBER,...\n",
       "707889    [so, ., so, dinotopia, linkcouldlink, exist, ?...\n",
       "707890    [don, ', t, you, wish, americans, cared, this,...\n",
       "707891    [the, illegals, around, here, said, in, interv...\n",
       "707892    [for, the, first, time, ever, on, nye, i, ', m...\n",
       "707893    [i, recently, discovered, that, this, doesn, '...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test',\n",
       " 'much',\n",
       " 'smoother',\n",
       " '.',\n",
       " 'im',\n",
       " 'just',\n",
       " 'glad',\n",
       " 'reddit',\n",
       " 'is',\n",
       " 'back',\n",
       " ',',\n",
       " 'linkreddit',\n",
       " 'in',\n",
       " 'mirc',\n",
       " 'was',\n",
       " 'entertaining',\n",
       " 'but',\n",
       " 'i',\n",
       " 'had',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'how',\n",
       " 'addicted',\n",
       " 'i',\n",
       " 'had',\n",
       " 'become',\n",
       " '.',\n",
       " 'thanks',\n",
       " 'for',\n",
       " 'making',\n",
       " 'the',\n",
       " 'detox',\n",
       " 'somewhat',\n",
       " 'short',\n",
       " '.',\n",
       " 'can',\n",
       " 'we',\n",
       " 'please',\n",
       " 'deprecate',\n",
       " 'the',\n",
       " 'word',\n",
       " '\"',\n",
       " 'ajax',\n",
       " '\"',\n",
       " 'now',\n",
       " '?',\n",
       " '(',\n",
       " 'but',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'this',\n",
       " '_is_',\n",
       " 'much',\n",
       " 'nicer',\n",
       " ')',\n",
       " 'oh',\n",
       " ',',\n",
       " 'i',\n",
       " 'see',\n",
       " '.',\n",
       " 'fancy',\n",
       " 'schmancy',\n",
       " '\"',\n",
       " 'submitting',\n",
       " '.',\n",
       " '.',\n",
       " '\"',\n",
       " 'testing',\n",
       " '.',\n",
       " 'i',\n",
       " 'like',\n",
       " 'it',\n",
       " '.',\n",
       " 'one',\n",
       " 'more',\n",
       " 'time',\n",
       " '.',\n",
       " 'try',\n",
       " 'refreshing',\n",
       " 'yor',\n",
       " 'cache',\n",
       " ',',\n",
       " 'that',\n",
       " 'worked',\n",
       " 'for',\n",
       " 'me',\n",
       " 'edit',\n",
       " ':',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'edit',\n",
       " 'k',\n",
       " '.',\n",
       " 'i',\n",
       " 'lied',\n",
       " '.',\n",
       " 'just',\n",
       " 'one',\n",
       " 'more',\n",
       " '.',\n",
       " 'i',\n",
       " 'also',\n",
       " 'wonder',\n",
       " 'what',\n",
       " 'the',\n",
       " 'differences',\n",
       " 'are',\n",
       " '.',\n",
       " 'so',\n",
       " 'addictive',\n",
       " '.',\n",
       " 'i',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'post',\n",
       " 'a',\n",
       " 'story',\n",
       " 'to',\n",
       " 'proggit',\n",
       " '-',\n",
       " 'i',\n",
       " 'got',\n",
       " '\"',\n",
       " 'you',\n",
       " 'are',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'submit',\n",
       " 'too',\n",
       " 'fast',\n",
       " '\"',\n",
       " 'on',\n",
       " 'my',\n",
       " 'first',\n",
       " 'submission',\n",
       " '.',\n",
       " 'alright',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'done',\n",
       " '.',\n",
       " 'is',\n",
       " 'anyone',\n",
       " 'else',\n",
       " \"'\",\n",
       " 's',\n",
       " '\"',\n",
       " 'recommended',\n",
       " '\"',\n",
       " 'page',\n",
       " 'completely',\n",
       " 'empty',\n",
       " '?',\n",
       " 'mine',\n",
       " 'is',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'jam-packed',\n",
       " '.',\n",
       " 'ok',\n",
       " ',',\n",
       " 'i',\n",
       " 'guess',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'submit',\n",
       " 'comments',\n",
       " 'to',\n",
       " 'test',\n",
       " '?',\n",
       " 'i',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'submit',\n",
       " 'any',\n",
       " 'stories',\n",
       " '--',\n",
       " 'even',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " ',',\n",
       " 'i',\n",
       " 'get',\n",
       " ',',\n",
       " '\"',\n",
       " 'you',\n",
       " 'are',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'submit',\n",
       " 'too',\n",
       " 'fast',\n",
       " '\"',\n",
       " '.',\n",
       " '.',\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'seeing',\n",
       " 'this',\n",
       " '?',\n",
       " 'edit',\n",
       " ':',\n",
       " 'this',\n",
       " 'appears',\n",
       " 'to',\n",
       " 'be',\n",
       " 'fixed',\n",
       " '.',\n",
       " 'working',\n",
       " 'fine',\n",
       " 'with',\n",
       " 'normal',\n",
       " 'adblock',\n",
       " '.',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'see',\n",
       " 'beta',\n",
       " '.',\n",
       " 'reddit',\n",
       " '.',\n",
       " 'com',\n",
       " 'from',\n",
       " 'here',\n",
       " '.',\n",
       " 'i',\n",
       " 'had',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'as',\n",
       " 'well',\n",
       " ',',\n",
       " 'with',\n",
       " 'my',\n",
       " 'comment',\n",
       " 'being',\n",
       " 'lost',\n",
       " '.',\n",
       " 'then',\n",
       " 'i',\n",
       " 'did',\n",
       " 'a',\n",
       " '\"',\n",
       " 'super-refresh',\n",
       " '\"',\n",
       " '(',\n",
       " 'shift-reload',\n",
       " ')',\n",
       " 'and',\n",
       " 'it',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'work',\n",
       " '.',\n",
       " 'hm',\n",
       " '?',\n",
       " 'i',\n",
       " 'think',\n",
       " 'so',\n",
       " '.',\n",
       " 'well',\n",
       " '.',\n",
       " 'i',\n",
       " 'read',\n",
       " 'ron',\n",
       " 'paul',\n",
       " \"'\",\n",
       " 's',\n",
       " 'website',\n",
       " '.',\n",
       " 'no',\n",
       " 'cry',\n",
       " 'me',\n",
       " 'a',\n",
       " 'river',\n",
       " '.',\n",
       " 'most',\n",
       " 'patents',\n",
       " 'and',\n",
       " 'copyrights',\n",
       " 'these',\n",
       " 'days',\n",
       " 'are',\n",
       " 'theft',\n",
       " 'of',\n",
       " 'public',\n",
       " 'domain',\n",
       " 'under',\n",
       " 'the',\n",
       " 'color',\n",
       " 'of',\n",
       " 'law',\n",
       " ',',\n",
       " 'to',\n",
       " 'an',\n",
       " 'absurd',\n",
       " 'degree',\n",
       " '.',\n",
       " 'mickey',\n",
       " 'mouse',\n",
       " 'copyright',\n",
       " 'extended',\n",
       " 'to',\n",
       " 'NUMBER',\n",
       " 'years',\n",
       " 'as',\n",
       " 'a',\n",
       " 'favor',\n",
       " 'to',\n",
       " 'everyone',\n",
       " \"'\",\n",
       " 's',\n",
       " 'favorite',\n",
       " 'political',\n",
       " 'donor',\n",
       " ',',\n",
       " 'will',\n",
       " 'eisner',\n",
       " '.',\n",
       " 'amazon',\n",
       " \"'\",\n",
       " 's',\n",
       " 'one',\n",
       " 'click',\n",
       " 'patent',\n",
       " '.',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'probably',\n",
       " 'been',\n",
       " 'NUMBER',\n",
       " 'years',\n",
       " 'since',\n",
       " 'the',\n",
       " 'constitutional',\n",
       " 'authority',\n",
       " 'to',\n",
       " 'grant',\n",
       " 'patents',\n",
       " 'and',\n",
       " 'copyrights',\n",
       " 'actually',\n",
       " 'served',\n",
       " ',',\n",
       " 'on',\n",
       " 'the',\n",
       " 'whole',\n",
       " ',',\n",
       " 'to',\n",
       " '\"',\n",
       " 'advance',\n",
       " 'the',\n",
       " 'arts',\n",
       " 'and',\n",
       " 'commerce',\n",
       " '\"',\n",
       " ',',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'to',\n",
       " 'stifle',\n",
       " 'them',\n",
       " '.',\n",
       " 'jup',\n",
       " 'as',\n",
       " 'long',\n",
       " 'as',\n",
       " 'we',\n",
       " 'can',\n",
       " 'still',\n",
       " 'say',\n",
       " '\"',\n",
       " 'web',\n",
       " 'NUMBER',\n",
       " '.',\n",
       " 'NUMBER',\n",
       " '\"',\n",
       " '.',\n",
       " 'what',\n",
       " \"'\",\n",
       " 's',\n",
       " 'new',\n",
       " 'except',\n",
       " 'the',\n",
       " 'asynchronous',\n",
       " 'submit',\n",
       " '(',\n",
       " 'which',\n",
       " 'alone',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'really',\n",
       " 'worth',\n",
       " 'being',\n",
       " 'dubbed',\n",
       " \"'\",\n",
       " 'new',\n",
       " 'comment',\n",
       " 'system',\n",
       " \"'\",\n",
       " ')',\n",
       " '?',\n",
       " 'yep',\n",
       " 'you',\n",
       " 'can',\n",
       " 'stop',\n",
       " 'anytime',\n",
       " 'you',\n",
       " 'want',\n",
       " '.',\n",
       " 'really',\n",
       " '.',\n",
       " 'hmm',\n",
       " 'only',\n",
       " 'worked',\n",
       " 'after',\n",
       " 'refresh',\n",
       " '.',\n",
       " 'okay',\n",
       " ',',\n",
       " 'dokay',\n",
       " ',',\n",
       " 'lets',\n",
       " 'see',\n",
       " 'how',\n",
       " 'the',\n",
       " 'comment',\n",
       " 'work',\n",
       " 'now',\n",
       " '.',\n",
       " 'ping',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " 'testing',\n",
       " ',',\n",
       " 'NUMBER',\n",
       " ',',\n",
       " 'NUMBER',\n",
       " ',',\n",
       " 'NUMBER',\n",
       " ',',\n",
       " 'testing',\n",
       " 'comments',\n",
       " 'posted',\n",
       " 'by',\n",
       " 'friends',\n",
       " 'are',\n",
       " 'colored',\n",
       " 'orange',\n",
       " '.',\n",
       " 'woot',\n",
       " '!',\n",
       " 'nice',\n",
       " '!',\n",
       " 'the',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'jumped',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " '.',\n",
       " 'uh',\n",
       " 'oh',\n",
       " '.',\n",
       " 'what',\n",
       " 'is',\n",
       " 'that',\n",
       " 'about',\n",
       " '?',\n",
       " '(',\n",
       " 'i',\n",
       " 'like',\n",
       " 'your',\n",
       " 'user',\n",
       " 'name',\n",
       " ',',\n",
       " 'mm',\n",
       " 'latin',\n",
       " ')',\n",
       " 'uhm',\n",
       " ',',\n",
       " 'a',\n",
       " 'NUMBER',\n",
       " 'yo',\n",
       " 'girl',\n",
       " 'was',\n",
       " 'sat',\n",
       " 'upon',\n",
       " 'to',\n",
       " 'death',\n",
       " 'by',\n",
       " 'a',\n",
       " 'daycare',\n",
       " 'facility',\n",
       " 'empoloyee',\n",
       " 'and',\n",
       " 'was',\n",
       " 'sentenced',\n",
       " 'to',\n",
       " 'NUMBER',\n",
       " 'days',\n",
       " 'of',\n",
       " 'prison',\n",
       " 'and',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'thousand',\n",
       " 'bucks',\n",
       " '.',\n",
       " 'so',\n",
       " ',',\n",
       " '.',\n",
       " '.',\n",
       " 'yeah',\n",
       " ',',\n",
       " 'dont',\n",
       " 'compare',\n",
       " 'your',\n",
       " 'woes',\n",
       " '.',\n",
       " 'let',\n",
       " 'me',\n",
       " 'see',\n",
       " 'edit',\n",
       " ':',\n",
       " 'oooh',\n",
       " 'thats',\n",
       " 'lovely',\n",
       " ',',\n",
       " 'much',\n",
       " 'better',\n",
       " 'than',\n",
       " 'before',\n",
       " '!',\n",
       " 'i',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'see',\n",
       " 'the',\n",
       " 'difference',\n",
       " 'yet',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'imagine',\n",
       " 'that',\n",
       " 'i',\n",
       " 'will',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'i',\n",
       " 'post',\n",
       " 'this',\n",
       " '.',\n",
       " 'this',\n",
       " 'new',\n",
       " 'comment',\n",
       " 'system',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'do',\n",
       " 'wonders',\n",
       " 'for',\n",
       " 'the',\n",
       " 'fibonacci',\n",
       " 'sequence',\n",
       " 'thread',\n",
       " '(',\n",
       " 'link',\n",
       " 'NUMBER',\n",
       " 'NUMBER',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'testing',\n",
       " '^_^',\n",
       " 'no',\n",
       " ',',\n",
       " 'let',\n",
       " 'me',\n",
       " 'try',\n",
       " 'edit',\n",
       " ',',\n",
       " 'nice',\n",
       " '!',\n",
       " 'NUMBER',\n",
       " 'let',\n",
       " 'me',\n",
       " 'try',\n",
       " 'this',\n",
       " '.',\n",
       " 'i',\n",
       " 'likes',\n",
       " ',',\n",
       " 'i',\n",
       " 'likes',\n",
       " ',',\n",
       " 'i',\n",
       " 'likes',\n",
       " 'juicy',\n",
       " 'ajaxiliscious',\n",
       " 'goodness',\n",
       " '.',\n",
       " 'now',\n",
       " 'if',\n",
       " 'could',\n",
       " 'only',\n",
       " 'find',\n",
       " 'my',\n",
       " 'precious',\n",
       " '.',\n",
       " 'now',\n",
       " 'what',\n",
       " 'has',\n",
       " 'it',\n",
       " 'got',\n",
       " 'in',\n",
       " 'its',\n",
       " 'pocketses',\n",
       " '?',\n",
       " 'is',\n",
       " 'anything',\n",
       " 'else',\n",
       " 'different',\n",
       " '?',\n",
       " 'surely',\n",
       " 'there',\n",
       " \"'\",\n",
       " 's',\n",
       " 'more',\n",
       " 'to',\n",
       " '\"',\n",
       " 'the',\n",
       " 'new',\n",
       " 'reddit',\n",
       " '\"',\n",
       " 'than',\n",
       " 'this',\n",
       " '.',\n",
       " 'edit',\n",
       " ':',\n",
       " 'i',\n",
       " 'guess',\n",
       " 'that',\n",
       " 'sounded',\n",
       " 'kinda',\n",
       " 'snotty',\n",
       " '.',\n",
       " 'i',\n",
       " 'really',\n",
       " 'like',\n",
       " 'the',\n",
       " 'new',\n",
       " 'submitting-in-place',\n",
       " '(',\n",
       " 'it',\n",
       " 'was',\n",
       " 'my',\n",
       " 'linkNUMBER',\n",
       " 'desired',\n",
       " 'fix/upgrade',\n",
       " ')',\n",
       " '.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " '.',\n",
       " 'hey',\n",
       " 'i',\n",
       " 'want',\n",
       " 'in',\n",
       " 'on',\n",
       " 'the',\n",
       " 'comment',\n",
       " 'testing',\n",
       " 'fun',\n",
       " '.',\n",
       " 'edit',\n",
       " ':',\n",
       " 'oooh',\n",
       " 'slick',\n",
       " '.',\n",
       " 'NUMBER',\n",
       " 'you',\n",
       " 'think',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'problem',\n",
       " '?',\n",
       " 'i',\n",
       " 'wasn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'even',\n",
       " 'able',\n",
       " 'to',\n",
       " 'log',\n",
       " 'in',\n",
       " 'when',\n",
       " 'i',\n",
       " 'was',\n",
       " 'using',\n",
       " 'firefox',\n",
       " '.',\n",
       " 'i',\n",
       " 'had',\n",
       " 'to',\n",
       " 'switch',\n",
       " 'to',\n",
       " 'opera',\n",
       " 'to',\n",
       " 'log',\n",
       " 'in',\n",
       " '.',\n",
       " 'now',\n",
       " 'to',\n",
       " 'test',\n",
       " 'that',\n",
       " 'new',\n",
       " 'submit',\n",
       " '.',\n",
       " 'i',\n",
       " 'think',\n",
       " 'the',\n",
       " '\"',\n",
       " 'parent',\n",
       " '\"',\n",
       " 'link',\n",
       " 'is',\n",
       " 'broken',\n",
       " '.',\n",
       " 'whats',\n",
       " 'new',\n",
       " '?',\n",
       " 'same',\n",
       " 'here',\n",
       " '.',\n",
       " 'certainly',\n",
       " 'a',\n",
       " 'bug',\n",
       " ',',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'take',\n",
       " 'a',\n",
       " 'screenshot',\n",
       " ',',\n",
       " 'one',\n",
       " 'sec',\n",
       " '.',\n",
       " 'NUMBER',\n",
       " 'mass',\n",
       " 'torture',\n",
       " 'at',\n",
       " 'guantanamo',\n",
       " '?',\n",
       " 'i',\n",
       " 'would',\n",
       " 'love',\n",
       " 'to',\n",
       " 'see',\n",
       " 'a',\n",
       " 'source',\n",
       " 'on',\n",
       " 'that',\n",
       " 'one',\n",
       " '.',\n",
       " 'the',\n",
       " 'only',\n",
       " 'i',\n",
       " 'have',\n",
       " 'heard',\n",
       " 'there',\n",
       " 'is',\n",
       " 'sleep',\n",
       " 'deprivation',\n",
       " 'and',\n",
       " 'loud',\n",
       " 'music',\n",
       " '.',\n",
       " 'abu',\n",
       " 'gharib',\n",
       " '?',\n",
       " 'that',\n",
       " 'was',\n",
       " 'an',\n",
       " 'isolated',\n",
       " 'incident',\n",
       " 'and',\n",
       " 'only',\n",
       " 'the',\n",
       " '\"',\n",
       " 'bush',\n",
       " 'is',\n",
       " 'hitler',\n",
       " '\"',\n",
       " 'nutjobs',\n",
       " 'think',\n",
       " 'the',\n",
       " 'administration',\n",
       " 'had',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'hand',\n",
       " 'in',\n",
       " 'it',\n",
       " '.',\n",
       " 'renditioning',\n",
       " '?',\n",
       " 'i',\n",
       " 'hate',\n",
       " 'to',\n",
       " 'break',\n",
       " 'it',\n",
       " 'to',\n",
       " 'you',\n",
       " 'but',\n",
       " 'enemy',\n",
       " 'forces',\n",
       " 'fighting',\n",
       " 'on',\n",
       " 'a',\n",
       " 'field',\n",
       " 'of',\n",
       " 'battle',\n",
       " 'linkout',\n",
       " 'of',\n",
       " 'uniformlink',\n",
       " 'have',\n",
       " 'no',\n",
       " 'rights',\n",
       " 'under',\n",
       " 'geneva',\n",
       " '.',\n",
       " 'none',\n",
       " ',',\n",
       " 'zip',\n",
       " ',',\n",
       " 'nada',\n",
       " '.',\n",
       " 'call',\n",
       " 'it',\n",
       " 'wrong',\n",
       " 'all',\n",
       " 'you',\n",
       " 'like',\n",
       " ',',\n",
       " 'but',\n",
       " 'illegal',\n",
       " 'its',\n",
       " 'not',\n",
       " '.',\n",
       " 'fisa',\n",
       " '?',\n",
       " 'this',\n",
       " 'has',\n",
       " 'been',\n",
       " 'done',\n",
       " 'in',\n",
       " 'drug',\n",
       " 'enforcement',\n",
       " 'long',\n",
       " 'before',\n",
       " 'wot',\n",
       " '.',\n",
       " 'and',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'spying',\n",
       " 'on',\n",
       " \"'\",\n",
       " 'millions',\n",
       " \"'\",\n",
       " 'of',\n",
       " 'citizens',\n",
       " 'either',\n",
       " '.',\n",
       " 'the',\n",
       " 'roving',\n",
       " 'wiretaps',\n",
       " 'are',\n",
       " 'on',\n",
       " 'linkindividualslink',\n",
       " 'which',\n",
       " 'we',\n",
       " 'are',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'monitor',\n",
       " ',',\n",
       " 'therefore',\n",
       " 'law',\n",
       " 'enforcement',\n",
       " 'should',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'track',\n",
       " 'them',\n",
       " 'on',\n",
       " 'new',\n",
       " 'phones',\n",
       " '.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'think',\n",
       " 'terrorists',\n",
       " 'under',\n",
       " 'surveillance',\n",
       " 'should',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'just',\n",
       " 'grab',\n",
       " 'a',\n",
       " 'new',\n",
       " 'cell',\n",
       " 'phone',\n",
       " 'at',\n",
       " 'wal-mart',\n",
       " 'and',\n",
       " 'start',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'judicial',\n",
       " 'review',\n",
       " 'process',\n",
       " 'all',\n",
       " 'over',\n",
       " 'again',\n",
       " 'you',\n",
       " 'are',\n",
       " 'crazy',\n",
       " '.',\n",
       " 'ohhhh',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'that',\n",
       " \"'\",\n",
       " 's',\n",
       " 'nice',\n",
       " '.',\n",
       " 'let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'do',\n",
       " 'it',\n",
       " 'again',\n",
       " '.',\n",
       " 'just',\n",
       " 'another',\n",
       " 'comment',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'd',\n",
       " 'rather',\n",
       " 'have',\n",
       " 'a',\n",
       " 'dead',\n",
       " 'hotdog',\n",
       " 'than',\n",
       " 'a',\n",
       " 'live',\n",
       " 'one',\n",
       " '.',\n",
       " 'NUMBER',\n",
       " 'nope',\n",
       " '.',\n",
       " 'months',\n",
       " 'of',\n",
       " 'development',\n",
       " ',',\n",
       " 'a',\n",
       " 'day',\n",
       " 'of',\n",
       " 'downtime',\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= [word for sentence in df['body'].values for word in sentence]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#source : http://stackoverflow.com/questions/33093809/count-the-frequency-of-elements-in-list-of-lists-in-python/33093930\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "a= [word for sentence in df['body'].values for word in sentence]\n",
    "word_freq = Counter(chain(a))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32140520"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([value for key, value in word_freq.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of 'valid' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27477766"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([value for key, value in word_freq.items() if d.check(key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now there are 707894 rows.\n"
     ]
    }
   ],
   "source": [
    "show_len_update(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to know how many sentences we have if we remove all senteces with invalid words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def invalid_word(df):\n",
    "    '''Goes through the content and determines whether an invalid word is \n",
    "    present.\n",
    "    \n",
    "    The data frame should provide a body field which will be inspected.\n",
    "    '''\n",
    "    d = enchant.Dict(\"en_US\") \n",
    "    valid_sentences = [True] * len(df)\n",
    "    misspelled_words = {}\n",
    "     \n",
    "    for idx, sentence in enumerate(df['body'].values):\n",
    "        for word in sentence:\n",
    "            if not d.check(word):\n",
    "                if word in misspelled_words:\n",
    "                    misspelled_words[word] += 1\n",
    "                else:\n",
    "                    misspelled_words[word] = 1\n",
    "                valid_sentences[idx] = False\n",
    "    print(\"There are %i valid sentences out of %i.\" % (sum(valid_sentences), len(valid_sentences)))\n",
    "    print(\"There are %i misspelled words.\" % len(misspelled_words))\n",
    "    return valid_sentences, misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 69873 valid sentences out of 707894.\n",
      "There are 187203 misspelled words.\n"
     ]
    }
   ],
   "source": [
    "valid_sent, misspelled = invalid_word(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of valid sentences using basic english dictionary:\n",
      "69873\n"
     ]
    }
   ],
   "source": [
    "print('Total number of valid sentences using basic english dictionary:')\n",
    "print(sum(valid_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['spell'] = valid_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use this list to append some words to our dictionary.\n",
    "sorted_mispelled_words = sorted(misspelled.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We write to a text-file the 1000 most common misspelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ivan/Desktop/mywords.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d8b6120d2ef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMISSPELLED_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/ivan/Desktop/mywords.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSPELLED_WORDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_mispelled_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ivan/Desktop/mywords.txt'"
     ]
    }
   ],
   "source": [
    "MISSPELLED_WORDS = '/Users/ivan/Desktop/mywords.txt'\n",
    "f = open(MISSPELLED_WORDS, 'w')\n",
    "for word in sorted_mispelled_words[:1000]:\n",
    "    f.write(word[0] + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_mispelled_words[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try and replace contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset(df):\n",
    "    df = load_data()\n",
    "    df = initial_clean(df)\n",
    "    df,total_mods = clean_with_tracking(df)\n",
    "    df = remove_large_comments(60, df)\n",
    "    df = contraction_replacer(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contraction_replacer(df):\n",
    "    for patrn in contractions.items():\n",
    "        df['body'].replace({patrn[0]: patrn[1]}, regex=True, inplace=True)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = contraction_replacer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['body'] = [basic_tokenizer(sentence) for sentence in df['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_len_update(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_sentences, misspelled_words = invalid_word(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_misspelled_words = sorted(misspelled_words.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_misspelled_words[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MISSPELLED_WORDS = '/Users/ivan/Desktop/mywords.txt'\n",
    "f = open(MISSPELLED_WORDS, 'w')\n",
    "for word in sorted_mispelled_words[:10000]:\n",
    "    f.write(word[0] + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def invalid_word_modified(df):\n",
    "    '''Goes through the content and determines whether an invalid word is \n",
    "    present.\n",
    "    \n",
    "    The data frame should provide a body field which will be inspected.\n",
    "    '''\n",
    "    d = enchant.DictWithPWL(\"en_US\", MISSPELLED_WORDS)\n",
    "    valid_sentences = [True] * len(df)\n",
    "    misspelled_words = {}\n",
    "     \n",
    "    for idx, sentence in enumerate(df['body'].values):\n",
    "        for word in sentence:\n",
    "            if not d.check(word):\n",
    "                if word in misspelled_words:\n",
    "                    misspelled_words[word] += 1\n",
    "                else:\n",
    "                    misspelled_words[word] = 1\n",
    "                valid_sentences[idx] = False\n",
    "    print(\"There are %i valid sentences out of %i.\" % (sum(valid_sentences), len(valid_sentences)))\n",
    "    print(\"There are %i misspelled words.\" % len(misspelled_words))\n",
    "    return valid_sentences, misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_sentences, misspelled_words = invalid_word_modified(df)\n",
    "sorted_misspelled_words = sorted(misspelled_words.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By adding 10000 extra words (not in the original dictionary) we only see 5000 more valid sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a threshold which keeps sentences with many of the misspelled words\n",
    "* For words that are in the original dictionary add 0 points. \n",
    "* Words that are not in the original dictionary add the inverse of the number of occurences in the corpus\n",
    "* We then normalize to the length of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [basic_tokenizer(sentence) for sentence in df['body']]\n",
    "words= [word for sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_freq = Counter(chain(words))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_score(sentence):\n",
    "    d = enchant.Dict('en_US')\n",
    "    word_count = len(sentence)\n",
    "    score = 0\n",
    "    for word in sentence:\n",
    "        if not d.check(word):\n",
    "            try: \n",
    "                score = score + 1.0/word_freq[word]\n",
    "            except ZeroDivisionError:\n",
    "                score = score + 1.0\n",
    "    try:\n",
    "        return score / word_count\n",
    "    except ZeroDivisionError:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_sentence_scores(df):\n",
    "    scores = []\n",
    "    pbar = ProgressBar()\n",
    "    for sentence in pbar(df.body):\n",
    "        scores.append(sentence_score(basic_tokenizer(sentence)))\n",
    "    df['score'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_sentence_scores(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A plot which displays the distribution of \"penalty score\" of a sentence. \n",
    "plt.hist(df.score.values, bins=500)\n",
    "plt.xlim(0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df.loc[df.score < 0.005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_json(RAW_DATA_FILES[0], lines=True)\n",
    "    df2 = pd.read_json(RAW_DATA_FILES[1], lines=True)\n",
    "    df3 = pd.read_json(RAW_DATA_FILES[2], lines=True)\n",
    "    df = df.append(df2, ignore_index=True)\n",
    "    df = df.append(df3, ignore_index=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    init_num_rows = len(df)\n",
    "    print(\"Number of lines in raw data file\", init_num_rows)\n",
    "    pprint(\"Column names from raw data file:\")\n",
    "    pprint(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [basic_tokenizer(sentence) for sentence in df['body']]\n",
    "words= [word for sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freq = Counter(chain(words))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_word_freq[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_sentence_scores(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df.score.values, bins=500)\n",
    "plt.xlim(0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.loc[df.score < 0.005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns a dictionary with keys being the root comments and values being their immediate children.\n",
    "## Assumes to have a 'root' column already\n",
    "\n",
    "## Go through all comments, if it is a root skip it since they wont have a parent_id corresponding\n",
    "## to a comment.\n",
    "## \n",
    "def children_dict(df):\n",
    "    children = {}\n",
    "    for row in df.itertuples():\n",
    "        if row.root == False:\n",
    "            if row.parent_id in children.keys():\n",
    "                children[row.parent_id].append(row.name)\n",
    "            else:\n",
    "                children[row.parent_id] = [row.name]\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Return a dictionary with name being the key and body being the value. \n",
    "values_dict = pd.Series(df.body.values, index=df.name).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "children = children_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Generates two files, [from_file_path] and [to_file_path] of one-to-one comments. \n",
    "def generate_files(from_file_path, to_file_path):\n",
    "    ## Open the files and clear them. \n",
    "    from_file = open(from_file_path, 'w')\n",
    "    to_file = open(to_file_path, 'w')\n",
    "    from_file.write(\"\")\n",
    "    to_file.write(\"\")\n",
    "    from_file.close()\n",
    "    to_file.close()\n",
    "\n",
    "    for key in children.keys():\n",
    "        from_file = open(from_file_path, 'a')\n",
    "        to_file = open(to_file_path, 'a')\n",
    "\n",
    "        ## Since we have deleted comments, some comments parents might not exist anymore so we must catch that error.\n",
    "        for child in children[key]:\n",
    "            try: \n",
    "                from_file.write(values_dict[key].replace('\\n', '').replace('\\r', ' ').replace('&gt', '') + \"\\n\")\n",
    "                to_file.write(values_dict[child].replace('\\n', '').replace('\\r', ' ').replace('&gt', '') + \"\\n\")\n",
    "            except KeyError:    \n",
    "                pass\n",
    "    from_file.close()\n",
    "    to_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_files(\"from_file.txt\", \"to_file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'dickbutt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
