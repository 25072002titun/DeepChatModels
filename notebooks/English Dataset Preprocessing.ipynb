{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Grammar and Word Lists\n",
    "\n",
    "It would be nice to clean up a dataset which contains grammatically incorrect language and make it seem more professional. \n",
    "\n",
    "To do this we will experiment with numerous open source libraries to determine their efficacy. \n",
    "\n",
    "We also want to see if we can replace misspelled words and expand contrations into their correct forms.\n",
    "\n",
    "Another eventual task will be to determine the 'quality' of a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cornell\t\t\t      imdb_full.pkl\t  ubuntu\r\n",
      "cornell_movie_dialogs_corpus  kaggle\t\t  ubuntu_data_wildml.tar.gz\r\n",
      "dogscats\t\t      pre_trained_models  vocab10000.to\r\n",
      "dogscats.zip\t\t      reddit\t\t  wmt\r\n",
      "fucked_up_data\t\t      simple-examples\r\n",
      "glove\t\t\t      test_data\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/brandon/terabyte/Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/brandon/terabyte/Datasets/reddit/raw_data/2007/RC_2007-10',\n",
      " '/home/brandon/terabyte/Datasets/reddit/raw_data/2007/RC_2007-11',\n",
      " '/home/brandon/terabyte/Datasets/reddit/raw_data/2007/RC_2007-12']\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = '/home/brandon/terabyte/Datasets/reddit/'\n",
    "# Determine if this directory exists, if not use Ivan's directory.\n",
    "if (os.path.isdir(DATA_ROOT)):\n",
    "    pass\n",
    "else:\n",
    "    DATA_ROOT = '/Users/ivan/Documents/sp_17/reddit_data'\n",
    "DATA_YEAR = '2007'\n",
    "# Use os.path.join; it will figure out the '/' in between.\n",
    "RAW_DATA_FILES = os.listdir(os.path.join(DATA_ROOT, 'raw_data', DATA_YEAR))\n",
    "# Always work with full pathnames to be safe.\n",
    "RAW_DATA_FILES = [os.path.join(DATA_ROOT, 'raw_data', DATA_YEAR, file) for file in RAW_DATA_FILES \n",
    "                  if not file.startswith('.')]\n",
    "pprint(RAW_DATA_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_json(RAW_DATA_FILES[0], lines=True)\n",
    "    init_num_rows = len(df)\n",
    "    print(\"Number of lines in raw data file\", init_num_rows)\n",
    "    pprint(\"Column names from raw data file:\")\n",
    "    pprint(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in raw data file 150429\n",
      "'Column names from raw data file:'\n",
      "Index(['archived', 'author', 'author_flair_css_class', 'author_flair_text',\n",
      "       'body', 'controversiality', 'created_utc', 'distinguished', 'downs',\n",
      "       'edited', 'gilded', 'id', 'link_id', 'name', 'parent_id',\n",
      "       'retrieved_on', 'score', 'score_hidden', 'subreddit', 'subreddit_id',\n",
      "       'ups'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_len_update(df):\n",
    "    print(\"Now there are\", len(df), \"rows.\")\n",
    "    \n",
    "def root_comments(df):\n",
    "    '''Build list determining which rows of df are root comments.\n",
    "    \n",
    "    Returns: \n",
    "        list of length equal to the number of rows in our data frame. \n",
    "    '''\n",
    "    root_value = []\n",
    "    # Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple.\n",
    "    for row in df.itertuples():\n",
    "        root_value.append(row.parent_id == row.link_id)\n",
    "    return root_value\n",
    "\n",
    "def random_rows_generator(num_rows_per_print, num_rows_total):\n",
    "    num_iterations = num_rows_total // num_rows_per_print \n",
    "    shuffled_indices = np.arange(num_rows_per_print * num_iterations)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    for batch in shuffled_indices.reshape(num_iterations, num_rows_per_print):\n",
    "        yield batch\n",
    "        \n",
    "#rand_rows = random_rows_generator(4, len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Start by removing comments without a body (deleted).\n",
    "* Remove comments larger than 150 words long.\n",
    "* Remove unneccesary columns. \n",
    "* Add a column determining whether a row is a root comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_clean(df):\n",
    "    df['root'] = root_comments(df)\n",
    "    df = df[['author', 'body', 'link_id', 'parent_id', 'name', 'root', 'subreddit']]\n",
    "    df.style.set_properties(subset=['body'], **{'width': '500px'})\n",
    "    df.style.set_properties(**{'text-align': 'left'})\n",
    "    show_len_update(df)\n",
    "    df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now there are 150429 rows.\n"
     ]
    }
   ],
   "source": [
    "df = initial_clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>name</th>\n",
       "      <th>root</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bostich</td>\n",
       "      <td>test</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299an</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>igiveyoumylife</td>\n",
       "      <td>much smoother.\\r\\n\\r\\nIm just glad reddit is b...</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299ao</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arve</td>\n",
       "      <td>Can we please deprecate the word \"Ajax\" now? \\...</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c02999p</td>\n",
       "      <td>t1_c0299ap</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299aq</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gigaquack</td>\n",
       "      <td>Oh, I see. Fancy schmancy \"submitting....\"</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299ah</td>\n",
       "      <td>t1_c0299ar</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               body  \\\n",
       "0         bostich                                               test   \n",
       "1  igiveyoumylife  much smoother.\\r\\n\\r\\nIm just glad reddit is b...   \n",
       "2            Arve  Can we please deprecate the word \"Ajax\" now? \\...   \n",
       "3       [deleted]                                          [deleted]   \n",
       "4       gigaquack         Oh, I see. Fancy schmancy \"submitting....\"   \n",
       "\n",
       "    link_id   parent_id        name   root   subreddit  \n",
       "0  t3_5yba3    t3_5yba3  t1_c0299an   True  reddit.com  \n",
       "1  t3_5yba3    t3_5yba3  t1_c0299ao   True  reddit.com  \n",
       "2  t3_5yba3  t1_c02999p  t1_c0299ap  False  reddit.com  \n",
       "3  t3_5yba3    t3_5yba3  t1_c0299aq   True  reddit.com  \n",
       "4  t3_5yba3  t1_c0299ah  t1_c0299ar  False  reddit.com  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modify_list = [('\\r\\n', ' '),\n",
    "               ('\\n', ' '),\n",
    "               ('\\r', ' '),\n",
    "               ('&gt;', ' '),\n",
    "               ('&lt;', ' '),\n",
    "               ('/__|\\*|\\#|(?:\\[([^\\]]*)\\]\\([^)]*\\))/gm', '[link]'),\n",
    "               ('https?:\\/\\/(?:www\\.|(?!www))[^\\s\\.]+\\.[^\\s]{2,}|www\\.[^\\s]+\\.[^\\s]{2,}', '[link]'),\n",
    "               ('\\d+', 'NUMBER'),\n",
    "               ('\\[', ''),\n",
    "               ('\\]', ''),\n",
    "               ('\\/\\/', ''),\n",
    "               ('\\.\\.\\.', '. ')\n",
    "              ]\n",
    "\n",
    "modify_value = {'\\r\\n': 1,\n",
    "               '\\n': 1,\n",
    "               '\\r': 1,\n",
    "               '&gt;': 10,\n",
    "               '&lt;': 10,\n",
    "               '/__|\\*|\\#|(?:\\[([^\\]]*)\\]\\([^)]*\\))/gm': 100,\n",
    "               'https?:\\/\\/(?:www\\.|(?!www))[^\\s\\.]+\\.[^\\s]{2,}|www\\.[^\\s]+\\.[^\\s]{2,}': 100,\n",
    "               '\\d+': 1000,\n",
    "               '\\[': 10000,\n",
    "               '\\]': 10000,\n",
    "               '\\/\\/': 10000,\n",
    "               '\\.\\.\\.': 100000\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_with_tracking(df):\n",
    "    df = df.loc[df.body != '[deleted]'].reset_index(drop=True)\n",
    "    df.style.set_properties(subset=['body'], **{'width': '800px'})\n",
    "    df['body'] = df['body'].map(lambda s: s.strip().lower())\n",
    "    \n",
    "    total_mods = {}\n",
    "    if 'mods' not in df: \n",
    "        df['mods'] = np.zeros(len(df['body']), dtype=int)\n",
    "    for patrn in modify_list:\n",
    "        new_df = df['body'].replace({patrn[0]: patrn[1]}, regex=True, inplace=False)\n",
    "        modifications = list((np.where(new_df.values != df['body'].values))[0])\n",
    "        df['body'] = new_df\n",
    "        df['mods'][modifications] += modify_value[patrn[0]]\n",
    "        total_mods[patrn[0]] = len(modifications)\n",
    "    return df, total_mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df,total_mods = clean_with_tracking(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>name</th>\n",
       "      <th>root</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>mods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126315</th>\n",
       "      <td>folderol</td>\n",
       "      <td>muscle memory.   or he never knew how to run a...</td>\n",
       "      <td>t3_5zj73</td>\n",
       "      <td>t1_c02cfl4</td>\n",
       "      <td>t1_c02cheo</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>100001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126316</th>\n",
       "      <td>michaelco</td>\n",
       "      <td>man develops delusions wow thats a headline</td>\n",
       "      <td>t3_5zjx2</td>\n",
       "      <td>t3_5zjx2</td>\n",
       "      <td>t1_c02chep</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126317</th>\n",
       "      <td>aletoledo</td>\n",
       "      <td>i have a list of redditers i wish to have cens...</td>\n",
       "      <td>t3_5zk1h</td>\n",
       "      <td>t1_c02che2</td>\n",
       "      <td>t1_c02cher</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126318</th>\n",
       "      <td>Dark-Star</td>\n",
       "      <td>nope. too lazy and don't care for until they o...</td>\n",
       "      <td>t3_5zimk</td>\n",
       "      <td>t1_c02cebw</td>\n",
       "      <td>t1_c02ches</td>\n",
       "      <td>False</td>\n",
       "      <td>politics</td>\n",
       "      <td>120101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126319</th>\n",
       "      <td>M0b1u5</td>\n",
       "      <td>us: we are fucking idiots.</td>\n",
       "      <td>t3_5zep2</td>\n",
       "      <td>t3_5zep2</td>\n",
       "      <td>t1_c02cheu</td>\n",
       "      <td>True</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               body  \\\n",
       "126315   folderol  muscle memory.   or he never knew how to run a...   \n",
       "126316  michaelco        man develops delusions wow thats a headline   \n",
       "126317  aletoledo  i have a list of redditers i wish to have cens...   \n",
       "126318  Dark-Star  nope. too lazy and don't care for until they o...   \n",
       "126319     M0b1u5                         us: we are fucking idiots.   \n",
       "\n",
       "         link_id   parent_id        name   root   subreddit    mods  \n",
       "126315  t3_5zj73  t1_c02cfl4  t1_c02cheo  False  reddit.com  100001  \n",
       "126316  t3_5zjx2    t3_5zjx2  t1_c02chep   True  reddit.com       0  \n",
       "126317  t3_5zk1h  t1_c02che2  t1_c02cher  False  reddit.com       1  \n",
       "126318  t3_5zimk  t1_c02cebw  t1_c02ches  False    politics  120101  \n",
       "126319  t3_5zep2    t3_5zep2  t1_c02cheu   True    politics       0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyEnchant is used to check if this is a real word.\n",
    "\n",
    "* An issue with this apporach is words that are not english, but are used heavily (e.g. 'reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, 'much'),\n",
       " (True, 'smoother.'),\n",
       " (False, 'im'),\n",
       " (True, 'just'),\n",
       " (True, 'glad'),\n",
       " (False, 'reddit'),\n",
       " (True, 'is'),\n",
       " (False, 'back,'),\n",
       " (False, 'linkreddit'),\n",
       " (True, 'in'),\n",
       " (False, 'mirc'),\n",
       " (True, 'was'),\n",
       " (True, 'entertaining'),\n",
       " (True, 'but'),\n",
       " (True, 'i'),\n",
       " (True, 'had'),\n",
       " (True, 'no'),\n",
       " (True, 'idea'),\n",
       " (True, 'how'),\n",
       " (True, 'addicted'),\n",
       " (True, 'i'),\n",
       " (True, 'had'),\n",
       " (True, 'become.'),\n",
       " (True, 'thanks'),\n",
       " (True, 'for'),\n",
       " (True, 'making'),\n",
       " (True, 'the'),\n",
       " (True, 'detox'),\n",
       " (True, 'somewhat'),\n",
       " (True, 'short.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(d.check(word), word) for word in df.body[1].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'language_check'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-309cc0798104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlanguage_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'language_check'"
     ]
    }
   ],
   "source": [
    "import language_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tool = language_check.LanguageTool('en_US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches = tool.check(df.body[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tool.disabled.add(\"UPPERCASE_SENTENCE_START\")\n",
    "tool.disabled.add('I_LOWERCASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = tool.check(df.body[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove comments with more than n words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_large_comments(n, df):\n",
    "    print(\"Length before:\", df['body'].size)\n",
    "    df = df[df['body'].map(lambda s: len(s.split(' '))) < n].reset_index(drop=True)\n",
    "    show_len_update(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the tokenizer from io_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE   = re.compile(r\"\\d\")\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w for w in words if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "df['body'] = [basic_tokenizer(sentence) for sentence in df['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a= [word for sentence in df['body'].values for word in sentence]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#source : http://stackoverflow.com/questions/33093809/count-the-frequency-of-elements-in-list-of-lists-in-python/33093930\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "a= [word for sentence in df['body'].values for word in sentence]\n",
    "word_freq = Counter(chain(a))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum([value for key, value in word_freq.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of 'valid' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum([value for key, value in word_freq.items() if d.check(key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_len_update(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to know how many sentences we have if we remove all senteces with invalid words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def invalid_word(df):\n",
    "    '''Goes through the content and determines whether an invalid word is \n",
    "    present.\n",
    "    \n",
    "    The data frame should provide a body field which will be inspected.\n",
    "    '''\n",
    "    d = enchant.Dict(\"en_US\") \n",
    "    valid_sentences = [True] * len(df)\n",
    "    misspelled_words = {}\n",
    "     \n",
    "    for idx, sentence in enumerate(df['body'].values):\n",
    "        for word in sentence:\n",
    "            if not d.check(word):\n",
    "                if word in misspelled_words:\n",
    "                    misspelled_words[word] += 1\n",
    "                else:\n",
    "                    misspelled_words[word] = 1\n",
    "                valid_sentences[idx] = False\n",
    "    print(\"There are %i valid sentences out of %i.\" % (sum(valid_sentences), len(valid_sentences)))\n",
    "    print(\"There are %i misspelled words.\" % len(misspelled_words))\n",
    "    return valid_sentences, misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_sent, misspelled = invalid_word(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Total number of valid sentences using basic english dictionary:')\n",
    "print(sum(valid_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['spell'] = valid_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use this list to append some words to our dictionary.\n",
    "sorted_mispelled_words = sorted(misspelled.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We write to a text-file the 1000 most common misspelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MISSPELLED_WORDS = '/Users/ivan/Desktop/mywords.txt'\n",
    "f = open(MISSPELLED_WORDS, 'w')\n",
    "for word in sorted_mispelled_words[:1000]:\n",
    "    f.write(word[0] + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_mispelled_words[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try and replace contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset(df):\n",
    "    df = load_data()\n",
    "    df = initial_clean(df)\n",
    "    df,total_mods = clean_with_tracking(df)\n",
    "    df = remove_large_comments(60, df)\n",
    "    df = contraction_replacer(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "        \"ain't\": \"am not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he had\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he shall\",\n",
    "        \"he'll've\": \"he shall have\",\n",
    "        \"he's\": \"he has\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how has\",\n",
    "        \"I'd\": \"I had\",\n",
    "        \"I'd've\": \"I would have\",\n",
    "        \"I'll\": \"I shall\",\n",
    "        \"I'll've\": \"I shall have\",\n",
    "        \"I'm\": \"I am\",\n",
    "        \"I've\": \"I have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it had\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it shall\",\n",
    "        \"it'll've\": \"it shall have\",\n",
    "        \"it's\": \"it has\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she had\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she shall\",\n",
    "        \"she'll've\": \"she shall have\",\n",
    "        \"she's\": \"she has\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so as\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that has\",\n",
    "        \"there'd\": \"there had\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there has\",\n",
    "        \"they'd\": \"they had\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they shall\",\n",
    "        \"they'll've\": \"they shall have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we had\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what shall\",\n",
    "        \"what'll've\": \"what shall have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what has\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when has\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where has\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who shall\",\n",
    "        \"who'll've\": \"who shall have\",\n",
    "        \"who's\": \"who has\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why has\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you had\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you shall\",\n",
    "        \"you'll've\": \"you shall have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contraction_replacer(df):\n",
    "    for patrn in contractions.items():\n",
    "        df['body'].replace({patrn[0]: patrn[1]}, regex=True, inplace=True)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = contraction_replacer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['body'] = [basic_tokenizer(sentence) for sentence in df['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_len_update(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_sentences, misspelled_words = invalid_word(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_misspelled_words = sorted(misspelled_words.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_misspelled_words[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MISSPELLED_WORDS = '/Users/ivan/Desktop/mywords.txt'\n",
    "f = open(MISSPELLED_WORDS, 'w')\n",
    "for word in sorted_mispelled_words[:10000]:\n",
    "    f.write(word[0] + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def invalid_word_modified(df):\n",
    "    '''Goes through the content and determines whether an invalid word is \n",
    "    present.\n",
    "    \n",
    "    The data frame should provide a body field which will be inspected.\n",
    "    '''\n",
    "    d = enchant.DictWithPWL(\"en_US\", MISSPELLED_WORDS)\n",
    "    valid_sentences = [True] * len(df)\n",
    "    misspelled_words = {}\n",
    "     \n",
    "    for idx, sentence in enumerate(df['body'].values):\n",
    "        for word in sentence:\n",
    "            if not d.check(word):\n",
    "                if word in misspelled_words:\n",
    "                    misspelled_words[word] += 1\n",
    "                else:\n",
    "                    misspelled_words[word] = 1\n",
    "                valid_sentences[idx] = False\n",
    "    print(\"There are %i valid sentences out of %i.\" % (sum(valid_sentences), len(valid_sentences)))\n",
    "    print(\"There are %i misspelled words.\" % len(misspelled_words))\n",
    "    return valid_sentences, misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_sentences, misspelled_words = invalid_word_modified(df)\n",
    "sorted_misspelled_words = sorted(misspelled_words.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By adding 10000 extra words (not in the original dictionary) we only see 5000 more valid sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a threshold which keeps sentences with many of the misspelled words\n",
    "* For words that are in the original dictionary add 0 points. \n",
    "* Words that are not in the original dictionary add the inverse of the number of occurences in the corpus\n",
    "* We then normalize to the length of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in raw data file 150429\n",
      "'Column names from raw data file:'\n",
      "Index(['archived', 'author', 'author_flair_css_class', 'author_flair_text',\n",
      "       'body', 'controversiality', 'created_utc', 'distinguished', 'downs',\n",
      "       'edited', 'gilded', 'id', 'link_id', 'name', 'parent_id',\n",
      "       'retrieved_on', 'score', 'score_hidden', 'subreddit', 'subreddit_id',\n",
      "       'ups'],\n",
      "      dtype='object')\n",
      "Now there are 150429 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before: 126320\n",
      "Now there are 102363 rows.\n"
     ]
    }
   ],
   "source": [
    "df = reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [basic_tokenizer(sentence) for sentence in df['body']]\n",
    "words= [word for sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_freq = Counter(chain(words))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_score(sentence):\n",
    "    d = enchant.Dict('en_US')\n",
    "    word_count = len(sentence)\n",
    "    score = 0\n",
    "    for word in sentence:\n",
    "        if not d.check(word):\n",
    "            try: \n",
    "                score = score + 1.0/word_freq[word]\n",
    "            except ZeroDivisionError:\n",
    "                score = score + 1.0\n",
    "    try:\n",
    "        return score / word_count\n",
    "    except ZeroDivisionError:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_sentence_scores(df):\n",
    "    scores = []\n",
    "    for sentence in df.body:\n",
    "        scores.append(sentence_score(basic_tokenizer(sentence)))\n",
    "    df['score'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_sentence_scores(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD8CAYAAAC2PJlnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFbJJREFUeJzt3H/wZXV93/Hny11BMIFdRJkNS7PYbOIgVtQNbJomE6XC\ngk2WaRiCNbJjqdsG7CSTzjRY22Gq6QxOZyoyMTqMrCw2Bomtw06EbLdo2ulMV9kVBIGY/YI67Aal\ncXFN3BYDvvvH/aw5rN/vfi+fvd+9d+X5mLlzz3mfzznf9z174PU9P+43VYUkST1eNO0GJEnHL0NE\nktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVK35dNuoNfpp59ea9asmXYbknTc2L17\n919W1csnuc3jNkTWrFnDrl27pt2GJB03knx90tv0cpYkqZshIknqZohIkroZIpKkboaIJKmbISJJ\n6maISJK6GSKSpG6GiCSp23EbIg/uO8Ca6z7Dmus+M+1WJOkF67gNEUnS9BkikqRuhogkqZshIknq\nZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSp21ghkmRFkk8l+bMkjyT5uSSnJdmRZE97\nX9nGJslNSeaSPJDk9YPtbGrj9yTZNKi/IcmDbZ2bkmTyH1WSNGnjnol8EPiTqnoV8FrgEeA64J6q\nWgvc0+YBLgHWttdm4MMASU4DrgcuAM4Hrj8UPG3MOwfrbTi6jyVJOhYWDZEkpwK/CNwCUFXfq6pv\nAxuBrW3YVuCyNr0RuK1GdgIrkqwCLgZ2VNX+qnoK2AFsaMtOqaqdVVXAbYNtSZJm2DhnImcD/wf4\nWJL7knw0yUuBM6rqiTbmG8AZbfpM4PHB+ntb7Uj1vfPUJUkzbpwQWQ68HvhwVb0O+C5/e+kKgHYG\nUZNv77mSbE6yK8muZw8eWOofJ0laxDghshfYW1Wfb/OfYhQq32yXomjvT7bl+4CzBuuvbrUj1VfP\nU/8hVXVzVa2rqnXLTj51jNYlSUtp0RCpqm8Ajyf5mVa6EHgY2AYcesJqE3Bnm94GXNWe0loPHGiX\nvbYDFyVZ2W6oXwRsb8u+k2R9eyrrqsG2JEkzbPmY4/4l8AdJTgAeA97BKIDuSHI18HXgijb2LuBS\nYA442MZSVfuTvA+4t417b1Xtb9PXALcCJwF3t5ckacaNFSJVdT+wbp5FF84ztoBrF9jOFmDLPPVd\nwLnj9CJJmh1+Y12S1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUz\nRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUz\nRCRJ3cYKkSRfS/JgkvuT7Gq105LsSLKnva9s9SS5KclckgeSvH6wnU1t/J4kmwb1N7Ttz7V1M+kP\nKkmavOdzJvLGqjqvqta1+euAe6pqLXBPmwe4BFjbXpuBD8ModIDrgQuA84HrDwVPG/POwXobuj+R\nJOmYOZrLWRuBrW16K3DZoH5bjewEViRZBVwM7Kiq/VX1FLAD2NCWnVJVO6uqgNsG25IkzbBxQ6SA\n/5Zkd5LNrXZGVT3Rpr8BnNGmzwQeH6y7t9WOVN87T12SNOOWjznuH1TVviSvAHYk+bPhwqqqJDX5\n9p6rBdhmgGWnvHypf5wkaRFjnYlU1b72/iTwaUb3NL7ZLkXR3p9sw/cBZw1WX91qR6qvnqc+Xx83\nV9W6qlq37ORTx2ldkrSEFg2RJC9N8uOHpoGLgC8D24BDT1htAu5s09uAq9pTWuuBA+2y13bgoiQr\n2w31i4Dtbdl3kqxvT2VdNdiWJGmGjXM56wzg0+2p2+XAJ6rqT5LcC9yR5Grg68AVbfxdwKXAHHAQ\neAdAVe1P8j7g3jbuvVW1v01fA9wKnATc3V6SpBm3aIhU1WPAa+epfwu4cJ56AdcusK0twJZ56ruA\nc8foV5I0Q/zGuiSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maI\nSJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maI\nSJK6jR0iSZYluS/JH7f5s5N8Pslckk8mOaHVT2zzc235msE23t3qX0ly8aC+odXmklw3uY8nSVpK\nz+dM5DeBRwbz7wc+UFU/BTwFXN3qVwNPtfoH2jiSnANcCbwa2AD8fgumZcCHgEuAc4C3trGSpBk3\nVogkWQ28Bfhomw/wJuBTbchW4LI2vbHN05Zf2MZvBG6vqqer6qvAHHB+e81V1WNV9T3g9jZWkjTj\nxj0TuRH418D32/zLgG9X1TNtfi9wZps+E3gcoC0/0Mb/oH7YOgvVf0iSzUl2Jdn17MEDY7YuSVoq\ni4ZIkn8EPFlVu49BP0dUVTdX1bqqWrfs5FOn3Y4kveAtH2PMzwO/kuRS4CXAKcAHgRVJlrezjdXA\nvjZ+H3AWsDfJcuBU4FuD+iHDdRaqS5Jm2KJnIlX17qpaXVVrGN0Y/2xVvQ34HHB5G7YJuLNNb2vz\ntOWfrapq9Svb01tnA2uBLwD3Amvb014ntJ+xbSKfTpK0pMY5E1nI7wC3J/ld4D7glla/Bfh4kjlg\nP6NQoKoeSnIH8DDwDHBtVT0LkORdwHZgGbClqh46ir4kScdIRicJx58TV62tVZtuBOBrN7xlyt1I\n0uxLsruq1k1ym35jXZLUzRCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEk\ndTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEk\ndTNEJEndFg2RJC9J8oUkX0ryUJJ/3+pnJ/l8krkkn0xyQquf2Obn2vI1g229u9W/kuTiQX1Dq80l\nuW7yH1OStBTGORN5GnhTVb0WOA/YkGQ98H7gA1X1U8BTwNVt/NXAU63+gTaOJOcAVwKvBjYAv59k\nWZJlwIeAS4BzgLe2sZKkGbdoiNTIX7fZF7dXAW8CPtXqW4HL2vTGNk9bfmGStPrtVfV0VX0VmAPO\nb6+5qnqsqr4H3N7GSpJm3Fj3RNoZw/3Ak8AO4FHg21X1TBuyFzizTZ8JPA7Qlh8AXjasH7bOQvX5\n+ticZFeSXc8ePDBO65KkJTRWiFTVs1V1HrCa0ZnDq5a0q4X7uLmq1lXVumUnnzqNFiRJA8/r6ayq\n+jbwOeDngBVJlrdFq4F9bXofcBZAW34q8K1h/bB1FqpLkmbcOE9nvTzJijZ9EvBm4BFGYXJ5G7YJ\nuLNNb2vztOWfrapq9Svb01tnA2uBLwD3Amvb014nMLr5vm0SH06StLSWLz6EVcDW9hTVi4A7quqP\nkzwM3J7kd4H7gFva+FuAjyeZA/YzCgWq6qEkdwAPA88A11bVswBJ3gVsB5YBW6rqoYl9QknSksno\nJOH4c+KqtbVq040AfO2Gt0y5G0mafUl2V9W6SW7Tb6xLkroZIpKkboaIJKmbISJJ6maISJK6GSKS\npG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKS\npG6GiCSpmyEiSepmiEiSuhkikqRui4ZIkrOSfC7Jw0keSvKbrX5akh1J9rT3la2eJDclmUvyQJLX\nD7a1qY3fk2TToP6GJA+2dW5KkqX4sJKkyRrnTOQZ4F9V1TnAeuDaJOcA1wH3VNVa4J42D3AJsLa9\nNgMfhlHoANcDFwDnA9cfCp425p2D9TYc/UeTJC21RUOkqp6oqi+26b8CHgHOBDYCW9uwrcBlbXoj\ncFuN7ARWJFkFXAzsqKr9VfUUsAPY0JadUlU7q6qA2wbbkiTNsOd1TyTJGuB1wOeBM6rqibboG8AZ\nbfpM4PHBantb7Uj1vfPUJUkzbuwQSfJjwH8BfquqvjNc1s4gasK9zdfD5iS7kux69uCBpf5xkqRF\njBUiSV7MKED+oKr+ayt/s12Kor0/2er7gLMGq69utSPVV89T/yFVdXNVrauqdctOPnWc1iVJS2ic\np7MC3AI8UlX/abBoG3DoCatNwJ2D+lXtKa31wIF22Ws7cFGSle2G+kXA9rbsO0nWt5911WBbkqQZ\ntnyMMT8PvB14MMn9rfZvgBuAO5JcDXwduKItuwu4FJgDDgLvAKiq/UneB9zbxr23qva36WuAW4GT\ngLvbS5I04xYNkar6X8BC39u4cJ7xBVy7wLa2AFvmqe8Czl2sF0nSbPEb65KkboaIJKmbISJJ6maI\nSJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maI\nSJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqtmiIJNmS5MkkXx7UTkuyI8me9r6y\n1ZPkpiRzSR5I8vrBOpva+D1JNg3qb0jyYFvnpiSZ9IeUJC2Ncc5EbgU2HFa7DrinqtYC97R5gEuA\nte21GfgwjEIHuB64ADgfuP5Q8LQx7xysd/jPkiTNqEVDpKr+J7D/sPJGYGub3gpcNqjfViM7gRVJ\nVgEXAzuqan9VPQXsADa0ZadU1c6qKuC2wbYkSTOu957IGVX1RJv+BnBGmz4TeHwwbm+rHam+d566\nJOk4cNQ31tsZRE2gl0Ul2ZxkV5Jdzx48cCx+pCTpCHpD5JvtUhTt/clW3wecNRi3utWOVF89T31e\nVXVzVa2rqnXLTj61s3VJ0qT0hsg24NATVpuAOwf1q9pTWuuBA+2y13bgoiQr2w31i4Dtbdl3kqxv\nT2VdNdiWJGnGLV9sQJI/BH4JOD3JXkZPWd0A3JHkauDrwBVt+F3ApcAccBB4B0BV7U/yPuDeNu69\nVXXoZv01jJ4AOwm4u70kSceBRUOkqt66wKIL5xlbwLULbGcLsGWe+i7g3MX6kCTNHr+xLknqZohI\nkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqduif4DxeLDm\nus/8YPprN7xlip1I0guLZyKSpG6GiCSpmyEiSepmiEiSuhkikqRuPxJPZw0Nn9QCn9aSpKXkmYgk\nqZshIknq9iN3OetwfhFRkpbOj3yIDB1+v2TIgJGk529mQiTJBuCDwDLgo1V1w7H8+UcKmB6GkqQX\ngpkIkSTLgA8Bbwb2Avcm2VZVD0+3s349oWTwSDrezESIAOcDc1X1GECS24GNwHEbIj0mfTZ0JMPA\nOtJj0T33lGblMWvvh0lLL1U17R5Icjmwoar+WZt/O3BBVb1roXVOXLW2Vm268Vi1qBeonvCZRHgt\nZQBOK1xn5ZeLhcx6f5OQZHdVrZvoNo+nEEmyGdjcZs8FvnxMG33+Tgf+ctpNjME+J8s+J8s+J+dn\nqurHJ7nBWbmctQ84azC/utWeo6puBm4GSLJr0ok6acdDj2Cfk2afk2Wfk5Nk16S3OStfNrwXWJvk\n7CQnAFcC26bckyRpETNxJlJVzyR5F7Cd0SO+W6rqoSm3JUlaxEyECEBV3QXc9TxWuXmpepmg46FH\nsM9Js8/Jss/JmXiPM3FjXZJ0fJqVeyKSpOPQTIRIkg1JvpJkLsl18yw/Mckn2/LPJ1kzWPbuVv9K\nkovH3eax7DPJm5PsTvJge3/TYJ0/bdu8v71eMcU+1yT5v4NePjJY5w2t/7kkNyXJFPt826DH+5N8\nP8l5bdlE9+cYPf5iki8meaY9qj5ctinJnvbaNKhPY1/O22eS85L87yQPJXkgya8Nlt2a5KuDfXne\ntPpsy54d9LJtUD+7HR9z7Xg5YVp9JnnjYcfm/0tyWVs2jf3520kebv+29yT5ycGyyRyfVTXVF6Mb\n6Y8CrwROAL4EnHPYmGuAj7TpK4FPtulz2vgTgbPbdpaNs81j3OfrgJ9o0+cC+wbr/Cmwbkb25xrg\nywts9wvAeiDA3cAl0+rzsDGvAR5div05Zo9rgL8H3AZcPqifBjzW3le26ZVT3JcL9fnTwNo2/RPA\nE8CKNn/rcOw092db9tcLbPcO4Mo2/RHgN6bZ52HHwH7g5CnuzzcOfv5v8Lf/rU/s+JyFM5Ef/MmT\nqvoecOhPngxtBLa26U8BF7Z03AjcXlVPV9VXgbm2vXG2ecz6rKr7quovWv0h4KQkJx5lPxPvc6EN\nJlkFnFJVO2t0lN0GXDYjfb61rbsUFu2xqr5WVQ8A3z9s3YuBHVW1v6qeAnYAG6a1Lxfqs6r+vKr2\ntOm/AJ4EXn6U/Uy8z4W04+FNjI4PGB0vU9ufh7kcuLuqDh5lP0fT5+cGP38no+/gwQSPz1kIkTOB\nxwfze1tt3jFV9QxwAHjZEdYdZ5vHss+hXwW+WFVPD2ofa6e3/24ClzaOts+zk9yX5H8k+YXB+L2L\nbPNY93nIrwF/eFhtUvvzaI6jIx2b09iXi0pyPqPfaB8dlP9DuxTygQn84nO0fb4kya4kOw9dImJ0\nPHy7HR8921yKPg+5kh8+Nqe5P69mdGZxpHWf9/E5CyHygpHk1cD7gX8+KL+tql4D/EJ7vX0avTVP\nAH+nql4H/DbwiSSnTLGfI0pyAXCwqoZ//maW9udxo/0G+nHgHVV16LfrdwOvAn6W0WWP35lSe4f8\nZI2+Ef5PgBuT/N0p97Ogtj9fw+i7b4dMbX8m+XVgHfAfJ73tWQiRcf7kyQ/GJFkOnAp86wjrjvVn\nVI5hnyRZDXwauKqqfvCbXlXta+9/BXyC0SnqVPpslwW/1frZzeg30p9u41cP1p/6/mx+6De9Ce/P\nozmOjnRsTmNfLqj9ovAZ4D1VtfNQvaqeqJGngY9xbI7NBQ3+bR9jdO/rdYyOhxXt+Hje21yKPpsr\ngE9X1d8cKkxrfyb5h8B7gF8ZXAGZ3PE5qZs8vS9GX3h8jNGN8UM3h1592Jhree4N1jva9Kt57o31\nxxjdbFp0m8e4zxVt/D+eZ5unt+kXM7qu+y+m2OfLgWVt+pXt4Dmt5r/Zdum0+mzzL2r9vXKp9ufz\nOY447KYpo980v8ropuXKNj21fXmEPk8A7gF+a56xq9p7gBuBG6bY50rgxDZ9OrCHdhMZ+COee2P9\nmmn1OajvBN447f3JKGgfpT08sRTHZ/cHmOQLuBT48/Zh39Nq72WUnAAvaQfKXPuAw/9xvKet9xUG\nTxHMt81p9Qn8W+C7wP2D1yuAlwK7gQcY3XD/IO1/4lPq81dbH/cDXwR+ebDNdYz+avKjwO/Rvqg6\nxX/3XwJ2Hra9ie/PMXr8WUbXjb/L6Lfihwbr/tPW+xyjy0TT3Jfz9gn8OvA3hx2b57VlnwUebL3+\nZ+DHptjn32+9fKm9Xz3Y5ivb8THXjpcTp9VnW7aG0S84Lzpsm9PYn/8d+Obg33bbpI9Pv7EuSeo2\nC/dEJEnHKUNEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3f4/ZIYLaGL+gmQAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47ec77a6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A plot which displays the distribution of \"penalty score\" of a sentence. \n",
    "plt.hist(df.score.values, bins=500)\n",
    "plt.xlim(0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71194"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[df.score < 0.005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_json(RAW_DATA_FILES[0], lines=True)\n",
    "    df2 = pd.read_json(RAW_DATA_FILES[1], lines=True)\n",
    "    df3 = pd.read_json(RAW_DATA_FILES[2], lines=True)\n",
    "    df = df.append(df2, ignore_index=True)\n",
    "    df = df.append(df3, ignore_index=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    init_num_rows = len(df)\n",
    "    print(\"Number of lines in raw data file\", init_num_rows)\n",
    "    pprint(\"Column names from raw data file:\")\n",
    "    pprint(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in raw data file 886802\n",
      "'Column names from raw data file:'\n",
      "Index(['archived', 'author', 'author_flair_css_class', 'author_flair_text',\n",
      "       'body', 'controversiality', 'created_utc', 'distinguished', 'downs',\n",
      "       'edited', 'gilded', 'id', 'link_id', 'name', 'parent_id',\n",
      "       'retrieved_on', 'score', 'score_hidden', 'subreddit', 'subreddit_id',\n",
      "       'ups'],\n",
      "      dtype='object')\n",
      "Now there are 886802 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before: 707894\n",
      "Now there are 580784 rows.\n"
     ]
    }
   ],
   "source": [
    "df = reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580784"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [basic_tokenizer(sentence) for sentence in df['body']]\n",
    "words= [word for sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freq = Counter(chain(words))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_sentence_scores(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>name</th>\n",
       "      <th>root</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>mods</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bostich</td>\n",
       "      <td>test</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299an</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>igiveyoumylife</td>\n",
       "      <td>much smoother.  im just glad reddit is back, l...</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299ao</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>20101</td>\n",
       "      <td>0.007132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arve</td>\n",
       "      <td>can we please deprecate the word \"ajax\" now?  ...</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c02999p</td>\n",
       "      <td>t1_c0299ap</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gigaquack</td>\n",
       "      <td>oh, i see. fancy schmancy \"submitting. .\"</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299ah</td>\n",
       "      <td>t1_c0299ar</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.027779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Percept</td>\n",
       "      <td>testing .</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>t1_c0299as</td>\n",
       "      <td>True</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               body  \\\n",
       "0         bostich                                               test   \n",
       "1  igiveyoumylife  much smoother.  im just glad reddit is back, l...   \n",
       "2            Arve  can we please deprecate the word \"ajax\" now?  ...   \n",
       "3       gigaquack          oh, i see. fancy schmancy \"submitting. .\"   \n",
       "4         Percept                                         testing .    \n",
       "\n",
       "    link_id   parent_id        name   root   subreddit    mods     score  \n",
       "0  t3_5yba3    t3_5yba3  t1_c0299an   True  reddit.com       0  0.000000  \n",
       "1  t3_5yba3    t3_5yba3  t1_c0299ao   True  reddit.com   20101  0.007132  \n",
       "2  t3_5yba3  t1_c02999p  t1_c0299ap  False  reddit.com       1  0.001079  \n",
       "3  t3_5yba3  t1_c0299ah  t1_c0299ar  False  reddit.com  100000  0.027779  \n",
       "4  t3_5yba3    t3_5yba3  t1_c0299as   True  reddit.com  100000  0.000000  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAD8CAYAAABKKbKtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47faa34eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df.score.values, bins=500)\n",
    "plt.xlim(0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457516"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[df.score < 0.005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns a dictionary with keys being the root comments and values being their immediate children.\n",
    "## Assumes to have a 'root' column already\n",
    "\n",
    "## Go through all comments, if it is a root skip it since they wont have a parent_id corresponding\n",
    "## to a comment.\n",
    "## \n",
    "def children_dict(df):\n",
    "    children = {}\n",
    "    for row in df.itertuples():\n",
    "        if row.root == False:\n",
    "            if row.parent_id in children.keys():\n",
    "                children[row.parent_id].append(row.name)\n",
    "            else:\n",
    "                children[row.parent_id] = [row.name]\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Return a dictionary with name being the key and body being the value. \n",
    "values_dict = pd.Series(df.body.values, index=df.name).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "children = children_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
