{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Grammar and Word Lists\n",
    "\n",
    "It would be nice to clean up a dataset which contains grammatically incorrect language and make it seem more professional. \n",
    "\n",
    "To do this we will experiment with numerous open source libraries to determine their efficacy. \n",
    "\n",
    "We also want to see if we can replace misspelled words and expand contrations into their correct forms.\n",
    "\n",
    "Another eventual task will be to determine the 'quality' of a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "import enchant\n",
    "import json\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from progressbar import ProgressBar\n",
    "%matplotlib inline\n",
    "\n",
    "with open('dicts.json', 'r') as f:\n",
    "    json_data = [json.loads(l) for l in f]\n",
    "modify_list, modify_value, contractions = json_data\n",
    "modify_list = list(modify_list.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = '/home/brandon/terabyte/Datasets/reddit' \n",
    "# Determine if this directory exists, if not use Ivan's directory.\n",
    "if (os.path.isdir(DATA_ROOT)):\n",
    "    pass\n",
    "else:\n",
    "    DATA_ROOT = '/Users/ivan/Documents/sp_17/reddit_data'\n",
    "DATA_YEARS = ['2007']\n",
    "# Use os.path.join; it will figure out the '/' in between.\n",
    "RAW_DATA_FILES = [os.listdir(os.path.join(DATA_ROOT, 'raw_data', year)) for year in DATA_YEARS]\n",
    "\n",
    "RAW_DATA_ABS_FILES = []\n",
    "# Always work with full pathnames to be safe.\n",
    "for i in range(len(DATA_YEARS)):\n",
    "    for j in range(len(RAW_DATA_FILES[i])):\n",
    "        if RAW_DATA_FILES[i][j].startswith('.'):\n",
    "            pass\n",
    "        else:\n",
    "            RAW_DATA_ABS_FILES.append( os.path.join(DATA_ROOT, 'raw_data' , DATA_YEARS[i], RAW_DATA_FILES[i][j]))\n",
    "RAW_DATA_FILES = RAW_DATA_ABS_FILES\n",
    "pprint(RAW_DATA_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/brandon/terabyte/Datasets/reddit/raw_data/2007/RC_2007-10',\n",
      " '/home/brandon/terabyte/Datasets/reddit/raw_data/2007/RC_2007-11',\n",
      " '/home/brandon/terabyte/Datasets/reddit/raw_data/2007/RC_2007-12']\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    print(RAW_DATA_FILES[0])\n",
    "    df = pd.read_json(RAW_DATA_FILES[0], lines=True)\n",
    "    for i in range(len(RAW_DATA_FILES) - 1):\n",
    "        print(\"Read file\", i)\n",
    "        df = df.append(pd.read_json(RAW_DATA_FILES[i+1], lines=True), ignore_index=True)\n",
    "    init_num_rows = len(df)\n",
    "    print(\"Number of lines in raw data file\", init_num_rows)\n",
    "    pprint(\"Column names from raw data file:\")\n",
    "    pprint(df.columns)\n",
    "    return df\n",
    "\n",
    "def show_len_update(df):\n",
    "    print(\"Now there are\", len(df), \"rows.\")\n",
    "    \n",
    "def root_comments(df):\n",
    "    '''Build list determining which rows of df are root comments.\n",
    "    \n",
    "    Returns: \n",
    "        list of length equal to the number of rows in our data frame. \n",
    "    '''\n",
    "    root_value = []\n",
    "    # Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple.\n",
    "    for row in df.itertuples():\n",
    "        root_value.append(row.parent_id == row.link_id)\n",
    "    return root_value\n",
    "\n",
    "def random_rows_generator(num_rows_per_print, num_rows_total):\n",
    "    num_iterations = num_rows_total // num_rows_per_print \n",
    "    shuffled_indices = np.arange(num_rows_per_print * num_iterations)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    for batch in shuffled_indices.reshape(num_iterations, num_rows_per_print):\n",
    "        yield batch\n",
    "        \n",
    "def initial_clean(df):\n",
    "    df['root'] = root_comments(df)\n",
    "    df = df[['author', 'body', 'link_id', 'parent_id', 'name', 'root', 'subreddit']]\n",
    "    df.style.set_properties(subset=['body'], **{'width': '500px'})\n",
    "    df.style.set_properties(**{'text-align': 'left'})\n",
    "    show_len_update(df)\n",
    "    df.head()\n",
    "    return df\n",
    "\n",
    "def clean_with_tracking(df):\n",
    "    df = df.loc[df.body != '[deleted]'].reset_index(drop=True)\n",
    "    df.style.set_properties(subset=['body'], **{'width': '800px'})\n",
    "    df['body'] = df['body'].map(lambda s: s.strip().lower())\n",
    "    \n",
    "    total_mods = {}\n",
    "    if 'mods' not in df: \n",
    "        df['mods'] = np.zeros(len(df['body']), dtype=int)\n",
    "    for patrn in modify_list:\n",
    "        new_df = df['body'].replace({patrn[0]: patrn[1]}, regex=True, inplace=False)\n",
    "        modifications = list((np.where(new_df.values != df['body'].values))[0])\n",
    "        df['body'] = new_df\n",
    "        df['mods'][modifications] += modify_value[patrn[0]]\n",
    "        total_mods[patrn[0]] = len(modifications)\n",
    "    return df, total_mods\n",
    "\n",
    "def remove_large_comments(n, df):\n",
    "    print(\"Length before:\", df['body'].size)\n",
    "    df = df[df['body'].map(lambda s: len(s.split(' '))) < n].reset_index(drop=True)\n",
    "    #show_len_update(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "_WORD_SPLIT = re.compile(r'([.,!?\\\"\\':;)(])|\\s')\n",
    "_DIGIT_RE   = re.compile(r\"\\d\")\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = _WORD_SPLIT.split(sentence.strip())\n",
    "    return [w for w in words if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contraction_replacer(df):\n",
    "    for patrn in contractions.items():\n",
    "        df['body'].replace({patrn[0]: patrn[1]}, regex=True, inplace=True)    \n",
    "    return df\n",
    "\n",
    "def generate_files(from_file_path, to_file_path):\n",
    "    \"\"\"Generates two files, [from_file_path] and [to_file_path] of one-to-one comments\n",
    "    \"\"\"\n",
    "    from_file_path = DATA_ROOT + '/' +  from_file_path\n",
    "    to_file_path = DATA_ROOT + '/' + to_file_path\n",
    "    ## Open the files and clear them. \n",
    "    from_file = open(from_file_path, 'w')\n",
    "    to_file = open(to_file_path, 'w')\n",
    "    from_file.write(\"\")\n",
    "    to_file.write(\"\")\n",
    "    from_file.close()\n",
    "    to_file.close()\n",
    "    for key in children.keys():\n",
    "        from_file = open(from_file_path, 'a')\n",
    "        to_file = open(to_file_path, 'a')\n",
    "        ## Since we have deleted comments, some comments parents might not exist anymore so we must catch that error.\n",
    "        for child in children[key]:\n",
    "            try: \n",
    "                from_file.write(values_dict[key].replace('\\n', '').replace('\\r', ' ').replace('&gt', '') + \"\\n\")\n",
    "                to_file.write(values_dict[child].replace('\\n', '').replace('\\r', ' ').replace('&gt', '') + \"\\n\")\n",
    "            except KeyError:    \n",
    "                pass\n",
    "    from_file.close()\n",
    "    to_file.close()\n",
    "    \n",
    "def children_dict(df):\n",
    "    \"\"\"Returns a dictionary with keys being the root comments and values being their immediate children.\n",
    "        Assumes to have a 'root' column already.\n",
    "        Go through all comments, if it is a root skip it since they wont have a parent_id corresponding\n",
    "        to a comment.\n",
    "    \"\"\"\n",
    "    children = {}\n",
    "    for row in df.itertuples():\n",
    "        if row.root == False:\n",
    "            if row.parent_id in children.keys():\n",
    "                children[row.parent_id].append(row.name)\n",
    "            else:\n",
    "                children[row.parent_id] = [row.name]\n",
    "    return children\n",
    "    \n",
    "def reset():\n",
    "    print('hi')\n",
    "    df = load_data()\n",
    "    print('hi2')\n",
    "    df = initial_clean(df)\n",
    "    print('hi3')\n",
    "    df,total_mods = clean_with_tracking(df)\n",
    "    df = remove_large_comments(10, df)\n",
    "    print('removed comments with more than', 10, 'words.')\n",
    "    df = contraction_replacer(df)\n",
    "    print('I iz returnin')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startin\n",
      "hi\n",
      "/home/brandon/terabyte/Datasets/reddit/raw_data/2007/RC_2007-10\n",
      "Read file 0\n",
      "Read file 1\n",
      "Number of lines in raw data file 886802\n",
      "'Column names from raw data file:'\n",
      "Index(['archived', 'author', 'author_flair_css_class', 'author_flair_text',\n",
      "       'body', 'controversiality', 'created_utc', 'distinguished', 'downs',\n",
      "       'edited', 'gilded', 'id', 'link_id', 'name', 'parent_id',\n",
      "       'retrieved_on', 'score', 'score_hidden', 'subreddit', 'subreddit_id',\n",
      "       'ups'],\n",
      "      dtype='object')\n",
      "hi2\n",
      "Now there are 886802 rows.\n",
      "hi3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before: 707894\n",
      "removed comments with more than 10 words.\n",
      "I iz returnin\n",
      "yayz\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "pool = multiprocessing.Pool(8)\n",
    "print('startin')\n",
    "df = reset()\n",
    "print('yayz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sentences = [basic_tokenizer(sentence) for sentence in df['body']]\n",
    "sentences = list(pool.map(basic_tokenizer, df['body']))\n",
    "words = [word for sentence in sentences for word in sentence]\n",
    "word_freq = Counter(chain(words))\n",
    "#word_freq = dict(word_freq)\n",
    "words = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_score(sentence):\n",
    "    d = enchant.Dict('en_US')\n",
    "    word_count = len(sentence)+1e-20\n",
    "    sent_score = [1.0/((word_freq[w]+1e-20)*word_count) for w in sentence if not d.check(w)]\n",
    "    return sent_score\n",
    "\n",
    "def add_sentence_scores(df):\n",
    "    scores = []\n",
    "    pbar = ProgressBar()\n",
    "    i = 0\n",
    "    for sentence in pbar(df.body):\n",
    "        scores.append(sentence_score(basic_tokenizer(sentence)))\n",
    "    df['score'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i iz redy\n"
     ]
    }
   ],
   "source": [
    "print('i iz redy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sup\n"
     ]
    }
   ],
   "source": [
    "#scores  = list(pool.map(sentence_score, sentences))\n",
    "print('sup')\n",
    "scores_0  = list(pool.map(sentence_score, sentences))\n",
    "print('done again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['score'] = [sum(s) for s in scores]\n",
    "scores = None\n",
    "print('k actually done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.loc[df.score < 0.008]\n",
    "print('k sup')\n",
    "values_dict = pd.Series(df.body.values, index=df.name).to_dict()\n",
    "children = children_dict(df)\n",
    "generate_files(\"from_file.txt\", \"to_file.txt\")\n",
    "print('writin this shit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = '/home/brandon/terabyte/Datasets/reddit_data' \n",
    "# Determine if this directory exists, if not use Ivan's directory.\n",
    "if (os.path.isdir(DATA_ROOT)):\n",
    "    pass\n",
    "else:\n",
    "    DATA_ROOT = '/Users/ivan/Documents/sp_17/reddit_data'\n",
    "DATA_YEARS = ['2007', '2008']\n",
    "# Use os.path.join; it will figure out the '/' in between.\n",
    "RAW_DATA_FILES = [os.listdir(os.path.join(DATA_ROOT, 'raw_data', year)) for year in DATA_YEARS]\n",
    "\n",
    "RAW_DATA_ABS_FILES = []\n",
    "# Always work with full pathnames to be safe.\n",
    "for i in range(len(DATA_YEARS)):\n",
    "    for j in range(len(RAW_DATA_FILES[i])):\n",
    "        if RAW_DATA_FILES[i][j].startswith('.'):\n",
    "            pass\n",
    "        else:\n",
    "            RAW_DATA_ABS_FILES.append( os.path.join(DATA_ROOT, 'raw_data' , DATA_YEARS[i], RAW_DATA_FILES[i][j]))\n",
    "RAW_DATA_FILES = RAW_DATA_ABS_FILES\n",
    "pprint(RAW_DATA_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_json(RAW_DATA_FILES[0], lines=True)\n",
    "    init_num_rows = len(df)\n",
    "    print(\"Number of lines in raw data file\", init_num_rows)\n",
    "    pprint(\"Column names from raw data file:\")\n",
    "    pprint(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(RAW_DATA_FILES[0])\n",
    "    df = pd.read_json(RAW_DATA_FILES[0], lines=True)\n",
    "    for i in range(len(RAW_DATA_FILES) - 1):\n",
    "        df = df.append(pd.read_json(RAW_DATA_FILES[i+1], lines=True), ignore_index=True)\n",
    "    init_num_rows = len(df)\n",
    "    print(\"Number of lines in raw data file\", init_num_rows)\n",
    "    pprint(\"Column names from raw data file:\")\n",
    "    pprint(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_len_update(df):\n",
    "    print(\"Now there are\", len(df), \"rows.\")\n",
    "    \n",
    "def root_comments(df):\n",
    "    '''Build list determining which rows of df are root comments.\n",
    "    \n",
    "    Returns: \n",
    "        list of length equal to the number of rows in our data frame. \n",
    "    '''\n",
    "    root_value = []\n",
    "    # Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple.\n",
    "    for row in df.itertuples():\n",
    "        root_value.append(row.parent_id == row.link_id)\n",
    "    return root_value\n",
    "\n",
    "def random_rows_generator(num_rows_per_print, num_rows_total):\n",
    "    num_iterations = num_rows_total // num_rows_per_print \n",
    "    shuffled_indices = np.arange(num_rows_per_print * num_iterations)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    for batch in shuffled_indices.reshape(num_iterations, num_rows_per_print):\n",
    "        yield batch\n",
    "        \n",
    "#rand_rows = random_rows_generator(4, len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Start by removing comments without a body (deleted).\n",
    "* Remove comments larger than 150 words long.\n",
    "* Remove unneccesary columns. \n",
    "* Add a column determining whether a row is a root comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_clean(df):\n",
    "    df['root'] = root_comments(df)\n",
    "    df = df[['author', 'body', 'link_id', 'parent_id', 'name', 'root', 'subreddit']]\n",
    "    df.style.set_properties(subset=['body'], **{'width': '500px'})\n",
    "    df.style.set_properties(**{'text-align': 'left'})\n",
    "    show_len_update(df)\n",
    "    df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = initial_clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modify_list = [('\\r\\n', ' '),\n",
    "               ('\\n', ' '),\n",
    "               ('\\r', ' '),\n",
    "               ('&gt;', ' '),\n",
    "               ('&lt;', ' '),\n",
    "               ('/__|\\*|\\#|(?:\\[([^\\]]*)\\]\\([^)]*\\))/gm', '[link]'),\n",
    "               ('https?:\\/\\/(?:www\\.|(?!www))[^\\s\\.]+\\.[^\\s]{2,}|www\\.[^\\s]+\\.[^\\s]{2,}', '[link]'),\n",
    "               ('\\d+', 'NUMBER'),\n",
    "               ('\\[', ''),\n",
    "               ('\\]', ''),\n",
    "               ('\\/\\/', ''),\n",
    "               ('\\.\\.\\.', '. ')\n",
    "              ]\n",
    "\n",
    "modify_value = {'\\r\\n': 1,\n",
    "               '\\n': 1,\n",
    "               '\\r': 1,\n",
    "               '&gt;': 10,\n",
    "               '&lt;': 10,\n",
    "               '/__|\\*|\\#|(?:\\[([^\\]]*)\\]\\([^)]*\\))/gm': 100,\n",
    "               'https?:\\/\\/(?:www\\.|(?!www))[^\\s\\.]+\\.[^\\s]{2,}|www\\.[^\\s]+\\.[^\\s]{2,}': 100,\n",
    "               '\\d+': 1000,\n",
    "               '\\[': 10000,\n",
    "               '\\]': 10000,\n",
    "               '\\/\\/': 10000,\n",
    "               '\\.\\.\\.': 100000\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_with_tracking(df):\n",
    "    df = df.loc[df.body != '[deleted]'].reset_index(drop=True)\n",
    "    df.style.set_properties(subset=['body'], **{'width': '800px'})\n",
    "    df['body'] = df['body'].map(lambda s: s.strip().lower())\n",
    "    \n",
    "    total_mods = {}\n",
    "    if 'mods' not in df: \n",
    "        df['mods'] = np.zeros(len(df['body']), dtype=int)\n",
    "    for patrn in modify_list:\n",
    "        new_df = df['body'].replace({patrn[0]: patrn[1]}, regex=True, inplace=False)\n",
    "        modifications = list((np.where(new_df.values != df['body'].values))[0])\n",
    "        df['body'] = new_df\n",
    "        df['mods'][modifications] += modify_value[patrn[0]]\n",
    "        total_mods[patrn[0]] = len(modifications)\n",
    "    return df, total_mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df,total_mods = clean_with_tracking(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyEnchant is used to check if this is a real word.\n",
    "\n",
    "* An issue with this apporach is words that are not english, but are used heavily (e.g. 'reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[(d.check(word), word) for word in df.body[1].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import language_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tool = language_check.LanguageTool('en_US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches = tool.check(df.body[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tool.disabled.add(\"UPPERCASE_SENTENCE_START\")\n",
    "tool.disabled.add('I_LOWERCASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = tool.check(df.body[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove comments with more than n words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_large_comments(n, df):\n",
    "    print(\"Length before:\", df['body'].size)\n",
    "    df = df[df['body'].map(lambda s: len(s.split(' '))) < n].reset_index(drop=True)\n",
    "    show_len_update(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the tokenizer from io_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE   = re.compile(r\"\\d\")\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w for w in words if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "df['body'] = [basic_tokenizer(sentence) for sentence in df['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a= [word for sentence in df['body'].values for word in sentence]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#source : http://stackoverflow.com/questions/33093809/count-the-frequency-of-elements-in-list-of-lists-in-python/33093930\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "a= [word for sentence in df['body'].values for word in sentence]\n",
    "word_freq = Counter(chain(a))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum([value for key, value in word_freq.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of 'valid' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum([value for key, value in word_freq.items() if d.check(key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_len_update(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to know how many sentences we have if we remove all senteces with invalid words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def invalid_word(df):\n",
    "    '''Goes through the content and determines whether an invalid word is \n",
    "    present.\n",
    "    \n",
    "    The data frame should provide a body field which will be inspected.\n",
    "    '''\n",
    "    d = enchant.Dict(\"en_US\") \n",
    "    valid_sentences = [True] * len(df)\n",
    "    misspelled_words = {}\n",
    "     \n",
    "    for idx, sentence in enumerate(df['body'].values):\n",
    "        for word in sentence:\n",
    "            if not d.check(word):\n",
    "                if word in misspelled_words:\n",
    "                    misspelled_words[word] += 1\n",
    "                else:\n",
    "                    misspelled_words[word] = 1\n",
    "                valid_sentences[idx] = False\n",
    "    print(\"There are %i valid sentences out of %i.\" % (sum(valid_sentences), len(valid_sentences)))\n",
    "    print(\"There are %i misspelled words.\" % len(misspelled_words))\n",
    "    return valid_sentences, misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_sent, misspelled = invalid_word(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Total number of valid sentences using basic english dictionary:')\n",
    "print(sum(valid_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['spell'] = valid_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use this list to append some words to our dictionary.\n",
    "sorted_mispelled_words = sorted(misspelled.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We write to a text-file the 1000 most common misspelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MISSPELLED_WORDS = '/Users/ivan/Desktop/mywords.txt'\n",
    "f = open(MISSPELLED_WORDS, 'w')\n",
    "for word in sorted_mispelled_words[:1000]:\n",
    "    f.write(word[0] + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_mispelled_words[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try and replace contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset():\n",
    "    df = load_data()\n",
    "    df = initial_clean(df)\n",
    "    df,total_mods = clean_with_tracking(df)\n",
    "    df = remove_large_comments(60, df)\n",
    "    df = contraction_replacer(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contraction_replacer(df):\n",
    "    for patrn in contractions.items():\n",
    "        df['body'].replace({patrn[0]: patrn[1]}, regex=True, inplace=True)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = contraction_replacer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['body'] = [basic_tokenizer(sentence) for sentence in df['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_len_update(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_sentences, misspelled_words = invalid_word(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_misspelled_words = sorted(misspelled_words.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_misspelled_words[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MISSPELLED_WORDS = '/Users/ivan/Desktop/mywords.txt'\n",
    "f = open(MISSPELLED_WORDS, 'w')\n",
    "for word in sorted_mispelled_words[:10000]:\n",
    "    f.write(word[0] + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def invalid_word_modified(df):\n",
    "    '''Goes through the content and determines whether an invalid word is \n",
    "    present.\n",
    "    \n",
    "    The data frame should provide a body field which will be inspected.\n",
    "    '''\n",
    "    d = enchant.DictWithPWL(\"en_US\", MISSPELLED_WORDS)\n",
    "    valid_sentences = [True] * len(df)\n",
    "    misspelled_words = {}\n",
    "     \n",
    "    for idx, sentence in enumerate(df['body'].values):\n",
    "        for word in sentence:\n",
    "            if not d.check(word):\n",
    "                if word in misspelled_words:\n",
    "                    misspelled_words[word] += 1\n",
    "                else:\n",
    "                    misspelled_words[word] = 1\n",
    "                valid_sentences[idx] = False\n",
    "    print(\"There are %i valid sentences out of %i.\" % (sum(valid_sentences), len(valid_sentences)))\n",
    "    print(\"There are %i misspelled words.\" % len(misspelled_words))\n",
    "    return valid_sentences, misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_sentences, misspelled_words = invalid_word_modified(df)\n",
    "sorted_misspelled_words = sorted(misspelled_words.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By adding 10000 extra words (not in the original dictionary) we only see 5000 more valid sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a threshold which keeps sentences with many of the misspelled words\n",
    "* For words that are in the original dictionary add 0 points. \n",
    "* Words that are not in the original dictionary add the inverse of the number of occurences in the corpus\n",
    "* We then normalize to the length of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = reset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [basic_tokenizer(sentence) for sentence in df['body']]\n",
    "words= [word for sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_freq = Counter(chain(words))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_score(sentence):\n",
    "    d = enchant.Dict('en_US')\n",
    "    word_count = len(sentence)\n",
    "    score = 0\n",
    "    for word in sentence:\n",
    "        if not d.check(word):\n",
    "            try: \n",
    "                score = score + 1.0/word_freq[word]\n",
    "            except ZeroDivisionError:\n",
    "                score = score + 1.0\n",
    "    try:\n",
    "        return score / word_count\n",
    "    except ZeroDivisionError:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_sentence_scores(df):\n",
    "    scores = []\n",
    "    pbar = ProgressBar()\n",
    "    for sentence in pbar(df.body):\n",
    "        scores.append(sentence_score(basic_tokenizer(sentence)))\n",
    "    df['score'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_sentence_scores(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A plot which displays the distribution of \"penalty score\" of a sentence. \n",
    "plt.hist(df.score.values, bins=500)\n",
    "plt.xlim(0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df.loc[df.score < 0.005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_json(RAW_DATA_FILES[0], lines=True)\n",
    "    df2 = pd.read_json(RAW_DATA_FILES[1], lines=True)\n",
    "    df3 = pd.read_json(RAW_DATA_FILES[2], lines=True)\n",
    "    df = df.append(df2, ignore_index=True)\n",
    "    df = df.append(df3, ignore_index=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    init_num_rows = len(df)\n",
    "    print(\"Number of lines in raw data file\", init_num_rows)\n",
    "    pprint(\"Column names from raw data file:\")\n",
    "    pprint(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = reset(df)\n",
    "sentences = [basic_tokenizer(sentence) for sentence in df['body']]\n",
    "words= [word for sentence in sentences for word in sentence]\n",
    "word_freq = Counter(chain(words))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])\n",
    "add_sentence_scores(df)\n",
    "values_dict = pd.Series(df.body.values, index=df.name).to_dict()\n",
    "children = children_dict(df)\n",
    "generate_files(\"from_file.txt\", \"to_file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [basic_tokenizer(sentence) for sentence in df['body']]\n",
    "words= [word for sentence in sentences for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_freq = Counter(chain(words))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_word_freq[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_sentence_scores(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df.score.values, bins=500)\n",
    "plt.xlim(0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.loc[df.score < 0.005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns a dictionary with keys being the root comments and values being their immediate children.\n",
    "## Assumes to have a 'root' column already\n",
    "\n",
    "## Go through all comments, if it is a root skip it since they wont have a parent_id corresponding\n",
    "## to a comment.\n",
    "## \n",
    "def children_dict(df):\n",
    "    children = {}\n",
    "    for row in df.itertuples():\n",
    "        if row.root == False:\n",
    "            if row.parent_id in children.keys():\n",
    "                children[row.parent_id].append(row.name)\n",
    "            else:\n",
    "                children[row.parent_id] = [row.name]\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Return a dictionary with name being the key and body being the value. \n",
    "values_dict = pd.Series(df.body.values, index=df.name).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "children = children_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Generates two files, [from_file_path] and [to_file_path] of one-to-one comments. \n",
    "def generate_files(from_file_path, to_file_path):\n",
    "    ## Open the files and clear them. \n",
    "    from_file = open(from_file_path, 'w')\n",
    "    to_file = open(to_file_path, 'w')\n",
    "    from_file.write(\"\")\n",
    "    to_file.write(\"\")\n",
    "    from_file.close()\n",
    "    to_file.close()\n",
    "\n",
    "    for key in children.keys():\n",
    "        from_file = open(from_file_path, 'a')\n",
    "        to_file = open(to_file_path, 'a')\n",
    "\n",
    "        ## Since we have deleted comments, some comments parents might not exist anymore so we must catch that error.\n",
    "        for child in children[key]:\n",
    "            try: \n",
    "                from_file.write(values_dict[key].replace('\\n', '').replace('\\r', ' ').replace('&gt', '') + \"\\n\")\n",
    "                to_file.write(values_dict[child].replace('\\n', '').replace('\\r', ' ').replace('&gt', '') + \"\\n\")\n",
    "            except KeyError:    \n",
    "                pass\n",
    "    from_file.close()\n",
    "    to_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_files(\"from_file.txt\", \"to_file.txt\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
