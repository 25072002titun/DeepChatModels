{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put all directory information in this cell.\n",
    "print(\"You are currently in\", os.getcwd(), \"\\n\")\n",
    "\n",
    "DATA_DIR = '/home/brandon/Downloads/2007/'\n",
    "DATA_FILES = os.listdir(DATA_DIR)\n",
    "DATA_FILES = [DATA_DIR + DATA_FILE for DATA_FILE in DATA_FILES]\n",
    "print(\"Contents of {}:\".format(DATA_DIR))\n",
    "print(DATA_FILES, \"\\n\")\n",
    "\n",
    "raw_data = np.array([\n",
    "    [json.loads(json_dict) for json_dict in open(data_file)] \n",
    "    for data_file in DATA_FILES\n",
    "])\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys_to_remove = [k for k in raw_data[0][0] if k not in ['author', 'body']]\n",
    "def remove_items_with_keys(dictionary, unwanted_keys):\n",
    "    for k in unwanted_keys:\n",
    "        dictionary.pop(k, None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entries of reddit comments:\n",
      "test\n",
      "much smoother.\r\n",
      "\r\n",
      "Im just glad reddit is back, #reddit in mIRC was entertaining but I had no idea how addicted I had become. Thanks for making the detox somewhat short.\n",
      "Can we please deprecate the word \"Ajax\" now? \r\n",
      "\r\n",
      "(But yeah, this _is_ much nicer)\n",
      "[deleted]\n"
     ]
    }
   ],
   "source": [
    "# Get comments and remove unwanted information.\n",
    "comments_only = list(raw_data[0])\n",
    "for d in comments_only[:4]: \n",
    "    remove_items_with_keys(d, keys_to_remove)\n",
    "comments_only = [item['body'] for item in comments_only]\n",
    "print(\"Example entries of reddit comments:\")\n",
    "for comment in comments_only[:4]: print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??nltk.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEGINNING] TOKENIZING COMMENTS . . . \n",
      "[DONE] TOKENIZING COMMENTS . . . \n",
      "['SENTENCE_START test SENTENCE_END', 'SENTENCE_START much smoother. SENTENCE_END', 'SENTENCE_START Im just glad reddit is back, #reddit in mIRC was entertaining but I had no idea how addicted I had become. SENTENCE_END', 'SENTENCE_START Thanks for making the detox somewhat short. SENTENCE_END', 'SENTENCE_START Can we please deprecate the word \"Ajax\" now? SENTENCE_END', 'SENTENCE_START (But yeah, this _is_ much nicer) SENTENCE_END', 'SENTENCE_START [deleted] SENTENCE_END', 'SENTENCE_START Oh, I see. SENTENCE_END', 'SENTENCE_START Fancy schmancy \"submitting....\" SENTENCE_END', 'SENTENCE_START testing ... SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "\n",
    "vocabulary_size = 4000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "print(\"[BEGINNING] TOKENIZING COMMENTS . . . \")\n",
    "sentences = itertools.chain(*[nltk.sent_tokenize(comment) for comment in comments_only])\n",
    "print(\"[DONE] TOKENIZING COMMENTS . . . \")\n",
    "\n",
    "sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print( \"Parsed %d sentences.\" % (len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 146867 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    " \n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 4000.\n",
      "Least frequent word in vocab: 'breasts', which appeared 97 times.\n",
      "Most frequent word in vocab: 'SENTENCE_START', which appeared 391452 times.\n",
      "Top 10 freq words and frequencies:\n",
      "SENTENCE_START \t 391452\n",
      "SENTENCE_END \t 391452\n",
      ". \t 279683\n",
      ", \t 206150\n",
      "the \t 198631\n",
      "to \t 132886\n",
      "a \t 110935\n",
      "of \t 99477\n",
      "I \t 94112\n",
      "and \t 89096\n"
     ]
    }
   ],
   "source": [
    "print( \"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print( \"Least frequent word in vocab: '%s', which appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "print( \"Most frequent word in vocab: '%s', which appeared %d times.\" % (vocab[0][0], vocab[0][1])) \n",
    "print(\"Top 10 freq words and frequencies:\")\n",
    "for word, freq in vocab[:10]:\n",
    "    print(word, \"\\t\", freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START test SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'test', 'SENTENCE_END']'\n",
      "ayy\n"
     ]
    }
   ],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print(\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print( \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])\n",
    " \n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "print(\"ayy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "data_path = \"/Users/ivan/Documents/sp_17/reddit_data/2007/RC_2007-10.json\"\n",
    "df = pd.read_json(data_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns a list of length equal to the number of rows in our data frame. \n",
    "def root_comments(df):\n",
    "    root_value = []\n",
    "    for row in df.itertuples():\n",
    "        root_value.append(row.parent_id == row.link_id)\n",
    "    return root_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by removing comments without a body (deleted).\n",
    "2. Remove comments larger than 150 characters long.\n",
    "3. Remove unneccesary columns. \n",
    "4. Add a column determining whether a row is a root comment.\n",
    "5. Add a column determining the size of the body in characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.assign(root=pd.Series(root_comments(df)))\n",
    "df = df[['author', 'body', 'link_id', 'parent_id', 'name', 'root', 'subreddit']]\n",
    "df = df.loc[df.body != '[deleted]']\n",
    "sizes = [len(a) for a in df.body.values]\n",
    "df = df.assign(body_len = pd.Series(sizes).values)\n",
    "df = df.loc[df.body_len < 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Returns a dictionary with keys being the root comments and values being their immediate children.\n",
    "## Assumes to have a 'root' column already\n",
    "\n",
    "## Go through all comments, if it is a root skip it since they wont have a parent_id corresponding\n",
    "## to a comment.\n",
    "## \n",
    "def children_dict(df):\n",
    "    children = {}\n",
    "    for row in df.itertuples():\n",
    "        if row.root == False:\n",
    "            if row.parent_id in children.keys():\n",
    "                children[row.parent_id].append(row.name)\n",
    "            else:\n",
    "                children[row.parent_id] = [row.name]\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Return a dictionary with name being the key and body being the value. \n",
    "values_dict = pd.Series(df.body.values, index=df.name).to_dict()\n",
    "children = children_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Generates two files, [from_file_path] and [to_file_path] of one-to-one comments. \n",
    "def generate_files(from_file_path, to_file_path):\n",
    "    ## Open the files and clear them. \n",
    "    from_file = open(from_file_path, 'w')\n",
    "    to_file = open(to_file_path, 'w')\n",
    "    from_file.write(\"\")\n",
    "    to_file.write(\"\")\n",
    "    from_file.close()\n",
    "    to_file.close()\n",
    "\n",
    "    for key in children.keys():\n",
    "        from_file = open(from_file_path, 'a')\n",
    "        to_file = open(to_file_path, 'a')\n",
    "\n",
    "        ## Since we have deleted comments, some comments parents might not exist anymore so we must catch that error.\n",
    "        for child in children[key]:\n",
    "            try: \n",
    "                from_file.write(values_dict[key].replace('\\n', '').replace('\\r', ' ').replace('&gt', '') + \"\\n\")\n",
    "                to_file.write(values_dict[child].replace('\\n', '').replace('\\r', ' ').replace('&gt', '') + \"\\n\")\n",
    "            except KeyError:    \n",
    "                pass\n",
    "    from_file.close()\n",
    "    to_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_files('from_file.txt', 'to_file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "f = open('from_file.txt','r')\n",
    "raw_text = f.read()\n",
    "tokens = nltk.word_tokenize(raw_text)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
