model: DynamicBot
dataset: Cornell
model_params:
    attention_size: 256  # (optional even with AttentionDecoder)
    batch_size: 128
    base_cell: GRUCell
    encoder.class: BidirectionalEncoder
    decoder.class: AttentionDecoder
    num_layers: 1
    ckpt_dir: out/cornell/bi_attn
    steps_per_ckpt: 250
dataset_params:
    data_dir: /home/brandon/Datasets/cornell # The only truly 'mandatory' parameter.
    vocab_size: 52000 # Approximately the true number of unique words in the dataset.
    max_seq_len: 20
    optimize_params: True
