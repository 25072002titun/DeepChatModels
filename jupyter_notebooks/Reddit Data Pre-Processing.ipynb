{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put all directory information in this cell.\n",
    "print(\"You are currently in\", os.getcwd(), \"\\n\")\n",
    "\n",
    "DATA_DIR = '/home/brandon/Downloads/2007/'\n",
    "DATA_FILES = os.listdir(DATA_DIR)\n",
    "DATA_FILES = [DATA_DIR + DATA_FILE for DATA_FILE in DATA_FILES]\n",
    "print(\"Contents of {}:\".format(DATA_DIR))\n",
    "print(DATA_FILES, \"\\n\")\n",
    "\n",
    "raw_data = np.array([\n",
    "    [json.loads(json_dict) for json_dict in open(data_file)] \n",
    "    for data_file in DATA_FILES\n",
    "])\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys_to_remove = [k for k in raw_data[0][0] if k not in ['author', 'body']]\n",
    "def remove_items_with_keys(dictionary, unwanted_keys):\n",
    "    for k in unwanted_keys:\n",
    "        dictionary.pop(k, None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entries of reddit comments:\n",
      "test\n",
      "much smoother.\r\n",
      "\r\n",
      "Im just glad reddit is back, #reddit in mIRC was entertaining but I had no idea how addicted I had become. Thanks for making the detox somewhat short.\n",
      "Can we please deprecate the word \"Ajax\" now? \r\n",
      "\r\n",
      "(But yeah, this _is_ much nicer)\n",
      "[deleted]\n"
     ]
    }
   ],
   "source": [
    "# Get comments and remove unwanted information.\n",
    "comments_only = list(raw_data[0])\n",
    "for d in comments_only[:4]: \n",
    "    remove_items_with_keys(d, keys_to_remove)\n",
    "comments_only = [item['body'] for item in comments_only]\n",
    "print(\"Example entries of reddit comments:\")\n",
    "for comment in comments_only[:4]: print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??nltk.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEGINNING] TOKENIZING COMMENTS . . . \n",
      "[DONE] TOKENIZING COMMENTS . . . \n",
      "['SENTENCE_START test SENTENCE_END', 'SENTENCE_START much smoother. SENTENCE_END', 'SENTENCE_START Im just glad reddit is back, #reddit in mIRC was entertaining but I had no idea how addicted I had become. SENTENCE_END', 'SENTENCE_START Thanks for making the detox somewhat short. SENTENCE_END', 'SENTENCE_START Can we please deprecate the word \"Ajax\" now? SENTENCE_END', 'SENTENCE_START (But yeah, this _is_ much nicer) SENTENCE_END', 'SENTENCE_START [deleted] SENTENCE_END', 'SENTENCE_START Oh, I see. SENTENCE_END', 'SENTENCE_START Fancy schmancy \"submitting....\" SENTENCE_END', 'SENTENCE_START testing ... SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "\n",
    "vocabulary_size = 4000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "print(\"[BEGINNING] TOKENIZING COMMENTS . . . \")\n",
    "sentences = itertools.chain(*[nltk.sent_tokenize(comment) for comment in comments_only])\n",
    "print(\"[DONE] TOKENIZING COMMENTS . . . \")\n",
    "\n",
    "sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print( \"Parsed %d sentences.\" % (len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 146867 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    " \n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 4000.\n",
      "Least frequent word in vocab: 'breasts', which appeared 97 times.\n",
      "Most frequent word in vocab: 'SENTENCE_START', which appeared 391452 times.\n",
      "Top 10 freq words and frequencies:\n",
      "SENTENCE_START \t 391452\n",
      "SENTENCE_END \t 391452\n",
      ". \t 279683\n",
      ", \t 206150\n",
      "the \t 198631\n",
      "to \t 132886\n",
      "a \t 110935\n",
      "of \t 99477\n",
      "I \t 94112\n",
      "and \t 89096\n"
     ]
    }
   ],
   "source": [
    "print( \"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print( \"Least frequent word in vocab: '%s', which appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "print( \"Most frequent word in vocab: '%s', which appeared %d times.\" % (vocab[0][0], vocab[0][1])) \n",
    "print(\"Top 10 freq words and frequencies:\")\n",
    "for word, freq in vocab[:10]:\n",
    "    print(word, \"\\t\", freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START test SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'test', 'SENTENCE_END']'\n",
      "ayy\n"
     ]
    }
   ],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print(\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print( \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])\n",
    " \n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "print(\"ayy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
