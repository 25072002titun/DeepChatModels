{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Github Tutorial (README)\n",
    "\n",
    "Compilation of relevant links:\n",
    "* [Seq2Seq Github](https://github.com/farizrahman4u/seq2seq)\n",
    "    * [Simple Seq2Seq](https://github.com/farizrahman4u/seq2seq/blob/master/seq2seq/models.py#L16)\n",
    "* [RecurrentShop Github](https://github.com/datalogai/recurrentshop)\n",
    "\n",
    "\n",
    "![](https://camo.githubusercontent.com/242210d7d0151cae91107ee63bff364a860db5dd/687474703a2f2f6936342e74696e797069632e636f6d2f333031333674652e706e67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model (I've replaced some code with pseudocode in [brackets] to make it readable) [[link]](https://github.com/farizrahman4u/seq2seq/blob/master/seq2seq/models.py#L64):\n",
    "\n",
    "```python\n",
    "def SimpleSeq2Seq(output_dim, output_length, hidden_dim=None, depth=1, dropout=0., **kwargs):\n",
    "    from recurrentshop import LSTMCell, RecurrentContainer\n",
    "    encoder = RecurrentContainer([...])\n",
    "\tencoder.add(LSTMCell(hidden_dim, [...]))\n",
    "\tfor _ in range([num LSTMS on encoding side]):\n",
    "\t\tencoder.add(Dropout(dropout))\n",
    "\t\tencoder.add(LSTMCell(hidden_dim, **kwargs))\n",
    "        \n",
    "\tdecoder = RecurrentContainer(decode=True, [...])\n",
    "\tdecoder.add(Dropout([...]))\n",
    "\n",
    "\tif [want more than 1 decoding LSTM]: [add them to model with hidden_dim]\n",
    "    decoder.add(LSTMCell(output_dim, **kwargs))\n",
    "\n",
    "\treturn Sequential([encoder, decoder])\n",
    "```\n",
    "\n",
    "Notes:\n",
    "* One allowed kwarg is 'input_length', which would specify the sequence length of the input. If not provided, appears to support arbitrary sequence lengths (i.e. it will figure it out). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seq2seq\n",
    "from seq2seq.models import SimpleSeq2Seq\n",
    "\n",
    "model = SimpleSeq2Seq(input_dim=5, hidden_dim=10, output_length=8, output_dim=8)\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a 'deep' seq2seq model, just specify the depth argument as seen in the snippet above (in the markdown cell). Accepts either integers or tuples. Examples:\n",
    "* depth=3 would just get changed to depth=(3, 3), meaning 3 encoder LSTMs and 3 decoder LSTMs (total depth 6). \n",
    "* depth=(4, 5) would be 4 encoder LSTMs, 5 decoder LSTMs (total depth 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Advanced\" Seq2Seq Models\n",
    "\n",
    "According to the Github author, the code below differs from the simple case because:\n",
    "1. The hidden state of the encoder is \"transferred\" to decoder. \n",
    "    * No it isn't. \n",
    "2. Output of decoder at each timestep becomes input to decoder at next time step (isn't this like the definition of LSTMs and RNNs in general?)\n",
    "3. The hidden state is \"propagated throughout the LSTM stack\". \n",
    "    * Again, isn't this like the definition of an LSTM??\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seq2seq\n",
    "from seq2seq.models import Seq2Seq\n",
    "\n",
    "model = Seq2Seq(batch_input_shape=(16, 7, 5), hidden_dim=10, output_length=8, output_dim=20, depth=4)\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
